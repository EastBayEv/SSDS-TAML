{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15e32f7-13f2-44e7-8a89-90430a4e3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/evanmuzzall/Desktop/SSDS-TAML'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3944520e-7ba9-45cc-9cd5-f6622464d98c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'music' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 lemmatized\u001b[38;5;241m.\u001b[39mappend(lemma)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized)\n\u001b[0;32m---> 19\u001b[0m music[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmusic\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(normalize, lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'music' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #, disable=['parser', 'tagger', 'ner'])\n",
    "# stopwords = en_core_web_sm.Defaults.stop_words\n",
    "# stops = stopwords.words(\"english\")\n",
    "stops = nlp.Defaults.stop_words\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "music['two'] = music['body'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "# Data['Text_After_Clean'] = Data['Text'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092bfa7c-ea02-420d-895b-31474bb02f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [^A-Za-z0-9 ]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309d32a-f5be-4e00-a8f0-9bf3cd42ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51367976-9dd9-482f-b9a8-42c07b3ec7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01fe15-1846-4771-8b91-e2948cee664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "music[\"clean_text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e841ccd-4208-4cfc-8853-27d407a90c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec06c90-6f75-443f-8f8f-928dfff4c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the vocabulary with CountVectorizer()\n",
    "\n",
    "Check out this post to see how min_df = and max_df = can be used to change the size of the vocabulary.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english',\n",
    "\n",
    "                     analyzer = 'word',\n",
    "\n",
    "                     min_df = 0.001,\n",
    "\n",
    "                     max_df = 0.50)\n",
    "\n",
    "cv_vec = cv.fit_transform(music['clean_text'])\n",
    "\n",
    "cv_vec.shape\n",
    "\n",
    "print(cv_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad22f5e-7941-4deb-9700-2125030fae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenize the \"clean_text\" variable\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# human_rights['clean_text'] = human_rights['clean_text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# print(human_rights['clean_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014e23c-ce42-4c1b-8772-5d860a9960c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1366f-b8f7-41c8-a511-4e881d4de3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04841463-8078-445e-b2bb-0e422c6e2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove unicode characters such as Ð and ð\n",
    "\n",
    "# First, save the \"clean_text\" variable as a single sring\n",
    "\n",
    "long_string = ','.join(list(human_rights[\"clean_text\"].values))\n",
    "\n",
    "long_string\n",
    "\n",
    "# encode as ascii\n",
    "\n",
    "strencode = long_string.encode(\"ascii\", \"ignore\")\n",
    "\n",
    "# decode\n",
    "\n",
    "strdecode = strencode.decode()\n",
    "\n",
    "print(long_string)\n",
    "\n",
    "output = ''.join([i if ord(i) < 128 else ' ' for i in long_string])\n",
    "\n",
    "print(output)\n",
    "\n",
    "import regex as re\n",
    "\n",
    "o2 = re.sub(r'\\s+',' ', output)\n",
    "\n",
    "o2\n",
    "\n",
    "# human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'[\\W\\_]', ' ', regex = True)\n",
    "\n",
    "print(human_rights['clean_text'][0])\n",
    "\n",
    "# re.sub(ur'[\\W_]+', u'', s, flags=re.UNICODE)\n",
    "\n",
    "​\n",
    "\n",
    "human_rights['Text_processed'][0]\n",
    "\n",
    "# print(human_rights['Text_processed'][0])\n",
    "\n",
    "# Save the \"Text_processed\" column as one long string\n",
    "\n",
    "long_string = ','.join(list(human_rights[\"Text_processed\"].values))\n",
    "\n",
    "long_string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "​\n",
    "\n",
    "# Tokenize long_string\n",
    "\n",
    "hr_tokens = long_string.split()\n",
    "\n",
    "​\n",
    "\n",
    "# Remove stopwords\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "no_stops = [word for word in hr_tokens if word not in stopwords.words('english')]\n",
    "\n",
    "freq_hr = Counter(no_stops)\n",
    "\n",
    "​\n",
    "\n",
    "# Print the 20 most common words\n",
    "\n",
    "hr_df = pd.DataFrame(freq_hr.most_common(20), columns = [\"Word\", \"Frequency\"])\n",
    "\n",
    "hr_df\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Encode the documents\n",
    "\n",
    "vector = vectorizer.transform(human_rights[\"Text_processed\"])\n",
    "\n",
    "print(vector) #\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "print(vector.shape)\n",
    "\n",
    "print(type(vector))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}