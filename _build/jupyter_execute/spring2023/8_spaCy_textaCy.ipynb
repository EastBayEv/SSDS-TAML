{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae50206",
   "metadata": {},
   "source": [
    "# Chapter 8 - spaCy and textaCy\n",
    "\n",
    "2023 April 7\n",
    "> These abridged materials are borrowed from the CIDR Workshop [Text Analysis with Python](https://github.com/sul-cidr/Workshops/tree/master/Text_Analysis_with_Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e83ea",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/EastBayEv/SSDS-TAML/blob/main/spring2023/8_spaCy_textaCy.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa244ab6",
   "metadata": {},
   "source": [
    "## Why spaCy and textacy?\n",
    "\n",
    "The language processing features of spaCy and the corpus analysis methods of textacy together offer a wide range of functionality for text analysis in a well-maintained and well-documented software package that incorporates cutting-edge techniques as well as standard approaches.\n",
    "\n",
    "The \"C\" in spaCy (and textacy) stands for Cython, which is Python that is compiled to C code and thus offers some performance advantages over interpreted Python, especially when working with large machine-learning models. The use of machine-learning models, including neural networks, is a key feature of spaCy and textacy. The writers of these libraries also have developed [Prodigy](https://prodi.gy/), a similarly leading-edge but approachable tool for training custom machine-learning models for text analysis, among other uses.\n",
    "\n",
    "Check out the [spaCy 101](https://spacy.io/usage/spacy-101) to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0738c",
   "metadata": {},
   "source": [
    "### Topics\n",
    "\n",
    "- Document Tokenization\n",
    "- Part-of-Speech (POS) Tagging\n",
    "- Named-Entity Recognition (NER)\n",
    "- Corpus Vectorization\n",
    "- Topic Modeling\n",
    "- Document Similarity\n",
    "- Stylistic Analysis\n",
    "\n",
    "**Note:** The examples from this workshop use English texts, but all of the methods are applicable to other languages. The availability of specialized resources (parsing rules, dictionaries, trained models) can vary considerably by language, however.\n",
    "\n",
    "### A brief word about terms\n",
    "\n",
    "**Text analysis** involves extraction of information from significant amounts  of free-form text, e.g., literature (prose, poetry), historical records, long-form survey responses, legal documents. Some of the techniques used also are applicable to short-form text data, including documents that are already in tabular format.\n",
    "\n",
    "Text analysis methods are built upon techniques for **Natural Language Processing** (NLP), which began as rule-based approaches to parsing human language and eventually incorporated statistical machine learning methods as well as, most recently, neural network/deep learning-based approaches.\n",
    "\n",
    "**Text mining** typically refers to the extraction of information from very large corpora of unstructured texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04f3ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 11:49:35.321792: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# !pip install textacy\n",
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a14de0",
   "metadata": {
    "id": "kEFaMNm3VyQf"
   },
   "source": [
    "# Document-level analysis with `spaCy`\n",
    "\n",
    "Let's start by learning how spaCy works and using it to begin analyzing a single text document. We'll work with larger corpora later in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253a09f",
   "metadata": {
    "id": "mRVkmEOvVyQq"
   },
   "source": [
    "For this workshop we will work with a pre-trained statistical and deep-learning model provided by spaCy to process text. spaCy's [models](https://spacy.io/models) are differentiated by language (21 languages are supported at present), capabilities, training text, and size. Smaller models are more efficient; larger models are more accurate. Here we'll download and use a medium-sized English multi-task model, which supports part of speech tagging, entity recognition, and includes a word vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0abbc9e",
   "metadata": {
    "id": "h3lrUP1cVyQs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 11:49:42.448832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting en-core-web-md==3.3.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/33.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/33.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/33.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/33.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/33.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/33.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/33.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/33.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/33.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/33.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/33.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/33.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/33.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/33.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/33.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/33.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m23.7/33.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m25.3/33.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m25.3/33.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m29.4/33.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m32.0/33.5 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m33.4/33.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m33.4/33.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-md==3.3.0) (3.3.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.10)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.4)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.1)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/evanmuzzall/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (66.1.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.28.2)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.10.1)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.10.1)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.7)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.1)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (23.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.23.5)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_md')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df5afa8",
   "metadata": {
    "id": "31NWduWIVyQz"
   },
   "outputs": [],
   "source": [
    "# Once we've installed the model, we can import it like any other Python library\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "215bac05",
   "metadata": {
    "id": "VT2rin_fVyQ6"
   },
   "outputs": [],
   "source": [
    "# This instantiates a spaCy text processor based on the installed model\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e3dd8e",
   "metadata": {
    "id": "gQYz476fVyRA"
   },
   "outputs": [],
   "source": [
    "# From H.G. Wells's A Short History of the World, Project Gutenberg \n",
    "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
    "Sardanapalus, Babylon had been a scene of great intellectual\n",
    "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
    "Babylon-ized.  He made a library, a library not of paper but of\n",
    "the clay tablets that were used for writing in Mesopotamia since\n",
    "early Sumerian days.  His collection has been unearthed and is\n",
    "perhaps the most precious store of historical material in the\n",
    "world.  The last of the Chaldean line of Babylonian monarchs,\n",
    "Nabonidus, had even keener literary tastes.  He patronized\n",
    "antiquarian researches, and when a date was worked out by his\n",
    "investigators for the accession of Sargon I he commemorated the\n",
    "fact by inscriptions.  But there were many signs of disunion in\n",
    "his empire, and he sought to centralize it by bringing a number of\n",
    "the various local gods to Babylon and setting up temples to them\n",
    "there.  This device was to be practised quite successfully by the\n",
    "Romans in later times, but in Babylon it roused the jealousy of\n",
    "the powerful priesthood of Bel Marduk, the dominant god of the\n",
    "Babylonians.  They cast about for a possible alternative to\n",
    "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
    "adjacent Median Empire.  Cyrus had already distinguished himself\n",
    "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
    "Minor.  {112} He came up against Babylon, there was a battle\n",
    "outside the walls, and the gates of the city were opened to him\n",
    "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
    "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
    "Bible relates, when a hand appeared and wrote in letters of fire\n",
    "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
    "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
    "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
    "finished it; thou art weighed in the balance and found wanting and\n",
    "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
    "priests of Bel Marduk knew something about that writing on the\n",
    "wall.  Belshazzar was killed that night, says the Bible.\n",
    "Nabonidus was taken prisoner, and the occupation of the city was\n",
    "so peaceful that the services of Bel Marduk continued without\n",
    "intermission.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb52170",
   "metadata": {
    "id": "K9pYgwcqVyRL"
   },
   "source": [
    "By default, spaCy applies its entire NLP \"pipeline\" to the text as soon as it is provided to the model and outputs a processed \"doc.\"\n",
    "\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb1c1b6",
   "metadata": {
    "id": "LR5v_iE3VyRG"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef191e8",
   "metadata": {
    "id": "HnkTWvuwVyRN"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The doc created by spaCy immediately provides access to the word-level tokens of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887823f8",
   "metadata": {
    "id": "Zg6EB7WeVyRR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even\n",
      "under\n",
      "the\n",
      "Assyrian\n",
      "monarchs\n",
      "and\n",
      "especially\n",
      "under\n",
      "\n",
      "\n",
      "Sardanapalus\n",
      ",\n",
      "Babylon\n",
      "had\n",
      "been\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:15]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6937908",
   "metadata": {
    "id": "yh2z0VkgVyRW"
   },
   "source": [
    "Each of these tokens has a number of properties, and we'll look a bit more closely at them in a minute.\n",
    "\n",
    "spaCy also automatically provides sentence-level segmenting (senticization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f6700c",
   "metadata": {
    "id": "c6Rr6LvXVyRY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even under the Assyrian monarchs and especially under\n",
      "Sardanapalus, Babylon had been a scene of great intellectual\n",
      "activity.\n",
      "--\n",
      "\n",
      " {111} Sardanapalus, though an Assyrian, had been quite\n",
      "Babylon-ized.\n",
      "--\n",
      "\n",
      " He made a library, a library not of paper but of\n",
      "the clay tablets that were used for writing in Mesopotamia since\n",
      "early Sumerian days.\n",
      "--\n",
      "\n",
      " His collection has been unearthed and is\n",
      "perhaps the most precious store of historical material in the\n",
      "world.\n",
      "--\n",
      "\n",
      " The last of the Chaldean line of Babylonian monarchs,\n",
      "Nabonidus, had even keener literary tastes.\n",
      "--\n",
      "\n",
      " He patronized\n",
      "antiquarian researches, and when a date was worked out by his\n",
      "investigators for the accession of Sargon I he commemorated the\n",
      "fact by inscriptions.\n",
      "--\n",
      "\n",
      " But there were many signs of disunion in\n",
      "his empire, and he sought to centralize it by bringing a number of\n",
      "the various local gods to Babylon and setting up temples to them\n",
      "there.\n",
      "--\n",
      "\n",
      " This device was to be practised quite successfully by the\n",
      "Romans in later times, but in Babylon it roused the jealousy of\n",
      "the powerful priesthood of Bel Marduk, the dominant god of the\n",
      "Babylonians.\n",
      "--\n",
      "\n",
      " They cast about for a possible alternative to\n",
      "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
      "adjacent Median Empire.\n",
      "--\n",
      "\n",
      " Cyrus had already distinguished himself\n",
      "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
      "Minor.\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f73aa",
   "metadata": {
    "id": "K3PhTxwz7cmD"
   },
   "source": [
    "You'll notice that the line breaks in the sample text are making the extracted sentences and also the word-level tokens a bit messy. The simplest way to avoid this is just to replace all single line breaks from the text with spaces before running it throug the spaCy pipeline, i.e., as a **preprocessing** step.\n",
    "\n",
    "There are other ways to handle this within the spaCy pipeline; an important feature of spaCy is that every phase of the built-in pipeline can be replaced by a custom module. One could imagine, for example, writing a replacement sentencizer that takes advantage of the presence of two spaces between all sentences in the sample text. But we will leave that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2edefab",
   "metadata": {
    "id": "XoP90Ys12tqB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even under the Assyrian monarchs and especially under Sardanapalus, Babylon had been a scene of great intellectual activity.\n",
      "--\n",
      "\n",
      " {111} Sardanapalus, though an Assyrian, had been quite Babylon-ized.\n",
      "--\n",
      "\n",
      " He made a library, a library not of paper but of the clay tablets that were used for writing in Mesopotamia since early Sumerian days.\n",
      "--\n",
      "\n",
      " His collection has been unearthed and is perhaps the most precious store of historical material in the world.\n",
      "--\n",
      "\n",
      " The last of the Chaldean line of Babylonian monarchs, Nabonidus, had even keener literary tastes.\n",
      "--\n",
      "\n",
      " He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of Sargon I he commemorated the fact by inscriptions.\n",
      "--\n",
      "\n",
      " But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.\n",
      "--\n",
      "\n",
      " This device was to be practised quite successfully by the Romans in later times, but in Babylon it roused the jealousy of the powerful priesthood of Bel Marduk, the dominant god of the Babylonians.\n",
      "--\n",
      "\n",
      " They cast about for a possible alternative to Nabonidus and found it in Cyrus the Persian, the ruler of the adjacent Median Empire.\n",
      "--\n",
      "\n",
      " Cyrus had already distinguished himself by conquering Croesus, the rich king of Lydia in Eastern Asia Minor.\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_as_line = text.replace(\"\\n\", \" \")\n",
    "\n",
    "doc = nlp(text_as_line)\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb99081",
   "metadata": {
    "id": "rcwG9tH9VyRd"
   },
   "source": [
    "We can collect both words and sentences into standard Python data structures (lists, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c922497",
   "metadata": {
    "id": "ZwnDSjL7VyRe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even under the Assyrian monarchs and especially under Sardanapalus, Babylon had been a scene of great intellectual activity.',\n",
       " ' {111} Sardanapalus, though an Assyrian, had been quite Babylon-ized.',\n",
       " ' He made a library, a library not of paper but of the clay tablets that were used for writing in Mesopotamia since early Sumerian days.',\n",
       " ' His collection has been unearthed and is perhaps the most precious store of historical material in the world.',\n",
       " ' The last of the Chaldean line of Babylonian monarchs, Nabonidus, had even keener literary tastes.',\n",
       " ' He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of Sargon I he commemorated the fact by inscriptions.',\n",
       " ' But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.',\n",
       " ' This device was to be practised quite successfully by the Romans in later times, but in Babylon it roused the jealousy of the powerful priesthood of Bel Marduk, the dominant god of the Babylonians.',\n",
       " ' They cast about for a possible alternative to Nabonidus and found it in Cyrus the Persian, the ruler of the adjacent Median Empire.',\n",
       " ' Cyrus had already distinguished himself by conquering Croesus, the rich king of Lydia in Eastern Asia Minor.',\n",
       " ' {112} He came up against Babylon, there was a battle outside the walls, and the gates of the city were opened to him (538 B.C.).',\n",
       " ' His soldiers entered the city without fighting.',\n",
       " ' The crown prince Belshazzar, the son of Nabonidus, was feasting, the Bible relates, when a hand appeared and wrote in letters of fire upon the wall these mystical words: _\"Mene, Mene, Tekel, Upharsin,\"_ which was interpreted by the prophet Daniel, whom he summoned to read the riddle, as \"God has numbered thy kingdom and finished it; thou art weighed in the balance and found wanting and thy kingdom is given to the Medes and Persians.\"',\n",
       " ' Possibly the priests of Bel Marduk knew something about that writing on the wall.',\n",
       " ' Belshazzar was killed that night, says the Bible.',\n",
       " 'Nabonidus was taken prisoner, and the occupation of the city was so peaceful that the services of Bel Marduk continued without intermission.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459a1e7e",
   "metadata": {
    "id": "NVJF1L5PVyRi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even',\n",
       " 'under',\n",
       " 'the',\n",
       " 'Assyrian',\n",
       " 'monarchs',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'under',\n",
       " 'Sardanapalus',\n",
       " ',',\n",
       " 'Babylon',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'of',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " '.',\n",
       " ' ',\n",
       " '{',\n",
       " '111',\n",
       " '}',\n",
       " 'Sardanapalus',\n",
       " ',',\n",
       " 'though',\n",
       " 'an',\n",
       " 'Assyrian',\n",
       " ',',\n",
       " 'had',\n",
       " 'been',\n",
       " 'quite',\n",
       " 'Babylon',\n",
       " '-',\n",
       " 'ized',\n",
       " '.',\n",
       " ' ',\n",
       " 'He',\n",
       " 'made',\n",
       " 'a',\n",
       " 'library',\n",
       " ',',\n",
       " 'a',\n",
       " 'library',\n",
       " 'not',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'but',\n",
       " 'of',\n",
       " 'the',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'that',\n",
       " 'were',\n",
       " 'used',\n",
       " 'for',\n",
       " 'writing',\n",
       " 'in',\n",
       " 'Mesopotamia',\n",
       " 'since',\n",
       " 'early',\n",
       " 'Sumerian',\n",
       " 'days',\n",
       " '.',\n",
       " ' ',\n",
       " 'His',\n",
       " 'collection',\n",
       " 'has',\n",
       " 'been',\n",
       " 'unearthed',\n",
       " 'and',\n",
       " 'is',\n",
       " 'perhaps',\n",
       " 'the',\n",
       " 'most',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'of',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " ' ',\n",
       " 'The',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Chaldean',\n",
       " 'line',\n",
       " 'of',\n",
       " 'Babylonian',\n",
       " 'monarchs',\n",
       " ',',\n",
       " 'Nabonidus',\n",
       " ',',\n",
       " 'had',\n",
       " 'even',\n",
       " 'keener',\n",
       " 'literary',\n",
       " 'tastes',\n",
       " '.',\n",
       " ' ',\n",
       " 'He',\n",
       " 'patronized',\n",
       " 'antiquarian',\n",
       " 'researches',\n",
       " ',',\n",
       " 'and',\n",
       " 'when',\n",
       " 'a',\n",
       " 'date',\n",
       " 'was',\n",
       " 'worked',\n",
       " 'out',\n",
       " 'by',\n",
       " 'his',\n",
       " 'investigators',\n",
       " 'for',\n",
       " 'the',\n",
       " 'accession',\n",
       " 'of',\n",
       " 'Sargon',\n",
       " 'I',\n",
       " 'he',\n",
       " 'commemorated',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'by',\n",
       " 'inscriptions',\n",
       " '.',\n",
       " ' ',\n",
       " 'But',\n",
       " 'there',\n",
       " 'were',\n",
       " 'many',\n",
       " 'signs',\n",
       " 'of',\n",
       " 'disunion',\n",
       " 'in',\n",
       " 'his',\n",
       " 'empire',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'sought',\n",
       " 'to',\n",
       " 'centralize',\n",
       " 'it',\n",
       " 'by',\n",
       " 'bringing',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'the',\n",
       " 'various',\n",
       " 'local',\n",
       " 'gods',\n",
       " 'to',\n",
       " 'Babylon',\n",
       " 'and',\n",
       " 'setting',\n",
       " 'up',\n",
       " 'temples',\n",
       " 'to',\n",
       " 'them',\n",
       " 'there',\n",
       " '.',\n",
       " ' ',\n",
       " 'This',\n",
       " 'device',\n",
       " 'was',\n",
       " 'to',\n",
       " 'be',\n",
       " 'practised',\n",
       " 'quite',\n",
       " 'successfully',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Romans',\n",
       " 'in',\n",
       " 'later',\n",
       " 'times',\n",
       " ',',\n",
       " 'but',\n",
       " 'in',\n",
       " 'Babylon',\n",
       " 'it',\n",
       " 'roused',\n",
       " 'the',\n",
       " 'jealousy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'powerful',\n",
       " 'priesthood',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " ',',\n",
       " 'the',\n",
       " 'dominant',\n",
       " 'god',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Babylonians',\n",
       " '.',\n",
       " ' ',\n",
       " 'They',\n",
       " 'cast',\n",
       " 'about',\n",
       " 'for',\n",
       " 'a',\n",
       " 'possible',\n",
       " 'alternative',\n",
       " 'to',\n",
       " 'Nabonidus',\n",
       " 'and',\n",
       " 'found',\n",
       " 'it',\n",
       " 'in',\n",
       " 'Cyrus',\n",
       " 'the',\n",
       " 'Persian',\n",
       " ',',\n",
       " 'the',\n",
       " 'ruler',\n",
       " 'of',\n",
       " 'the',\n",
       " 'adjacent',\n",
       " 'Median',\n",
       " 'Empire',\n",
       " '.',\n",
       " ' ',\n",
       " 'Cyrus',\n",
       " 'had',\n",
       " 'already',\n",
       " 'distinguished',\n",
       " 'himself',\n",
       " 'by',\n",
       " 'conquering',\n",
       " 'Croesus',\n",
       " ',',\n",
       " 'the',\n",
       " 'rich',\n",
       " 'king',\n",
       " 'of',\n",
       " 'Lydia',\n",
       " 'in',\n",
       " 'Eastern',\n",
       " 'Asia',\n",
       " 'Minor',\n",
       " '.',\n",
       " ' ',\n",
       " '{',\n",
       " '112',\n",
       " '}',\n",
       " 'He',\n",
       " 'came',\n",
       " 'up',\n",
       " 'against',\n",
       " 'Babylon',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'battle',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'walls',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gates',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'were',\n",
       " 'opened',\n",
       " 'to',\n",
       " 'him',\n",
       " '(',\n",
       " '538',\n",
       " 'B.C.',\n",
       " ')',\n",
       " '.',\n",
       " ' ',\n",
       " 'His',\n",
       " 'soldiers',\n",
       " 'entered',\n",
       " 'the',\n",
       " 'city',\n",
       " 'without',\n",
       " 'fighting',\n",
       " '.',\n",
       " ' ',\n",
       " 'The',\n",
       " 'crown',\n",
       " 'prince',\n",
       " 'Belshazzar',\n",
       " ',',\n",
       " 'the',\n",
       " 'son',\n",
       " 'of',\n",
       " 'Nabonidus',\n",
       " ',',\n",
       " 'was',\n",
       " 'feasting',\n",
       " ',',\n",
       " 'the',\n",
       " 'Bible',\n",
       " 'relates',\n",
       " ',',\n",
       " 'when',\n",
       " 'a',\n",
       " 'hand',\n",
       " 'appeared',\n",
       " 'and',\n",
       " 'wrote',\n",
       " 'in',\n",
       " 'letters',\n",
       " 'of',\n",
       " 'fire',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'wall',\n",
       " 'these',\n",
       " 'mystical',\n",
       " 'words',\n",
       " ':',\n",
       " '_',\n",
       " '\"',\n",
       " 'Mene',\n",
       " ',',\n",
       " 'Mene',\n",
       " ',',\n",
       " 'Tekel',\n",
       " ',',\n",
       " 'Upharsin',\n",
       " ',',\n",
       " '\"',\n",
       " '_',\n",
       " 'which',\n",
       " 'was',\n",
       " 'interpreted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'prophet',\n",
       " 'Daniel',\n",
       " ',',\n",
       " 'whom',\n",
       " 'he',\n",
       " 'summoned',\n",
       " 'to',\n",
       " 'read',\n",
       " 'the',\n",
       " 'riddle',\n",
       " ',',\n",
       " 'as',\n",
       " '\"',\n",
       " 'God',\n",
       " 'has',\n",
       " 'numbered',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'and',\n",
       " 'finished',\n",
       " 'it',\n",
       " ';',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'weighed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'and',\n",
       " 'found',\n",
       " 'wanting',\n",
       " 'and',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'is',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Medes',\n",
       " 'and',\n",
       " 'Persians',\n",
       " '.',\n",
       " '\"',\n",
       " ' ',\n",
       " 'Possibly',\n",
       " 'the',\n",
       " 'priests',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " 'knew',\n",
       " 'something',\n",
       " 'about',\n",
       " 'that',\n",
       " 'writing',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wall',\n",
       " '.',\n",
       " ' ',\n",
       " 'Belshazzar',\n",
       " 'was',\n",
       " 'killed',\n",
       " 'that',\n",
       " 'night',\n",
       " ',',\n",
       " 'says',\n",
       " 'the',\n",
       " 'Bible',\n",
       " '.',\n",
       " 'Nabonidus',\n",
       " 'was',\n",
       " 'taken',\n",
       " 'prisoner',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'occupation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'was',\n",
       " 'so',\n",
       " 'peaceful',\n",
       " 'that',\n",
       " 'the',\n",
       " 'services',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " 'continued',\n",
       " 'without',\n",
       " 'intermission',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [token.text for token in doc]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674e608",
   "metadata": {
    "id": "xndApEFuVyRn"
   },
   "source": [
    "### Filtering tokens\n",
    "\n",
    "After extracting the tokens, we can use some attributes and methods provided by spaCy, along with some vanilla Python methods, to filter the tokens to just the types we're interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48c24ea",
   "metadata": {
    "id": "ZSHaSQWqVyRo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: Even            IS_PUNCTUATION: False\n",
      "TOKEN: under           IS_PUNCTUATION: False\n",
      "TOKEN: the             IS_PUNCTUATION: False\n",
      "TOKEN: Assyrian        IS_PUNCTUATION: False\n",
      "TOKEN: monarchs        IS_PUNCTUATION: False\n",
      "TOKEN: and             IS_PUNCTUATION: False\n",
      "TOKEN: especially      IS_PUNCTUATION: False\n",
      "TOKEN: under           IS_PUNCTUATION: False\n",
      "TOKEN: Sardanapalus    IS_PUNCTUATION: False\n",
      "TOKEN: ,               IS_PUNCTUATION: True\n",
      "TOKEN: Babylon         IS_PUNCTUATION: False\n",
      "TOKEN: had             IS_PUNCTUATION: False\n",
      "TOKEN: been            IS_PUNCTUATION: False\n",
      "TOKEN: a               IS_PUNCTUATION: False\n",
      "TOKEN: scene           IS_PUNCTUATION: False\n",
      "TOKEN: of              IS_PUNCTUATION: False\n",
      "TOKEN: great           IS_PUNCTUATION: False\n",
      "TOKEN: intellectual    IS_PUNCTUATION: False\n",
      "TOKEN: activity        IS_PUNCTUATION: False\n",
      "TOKEN: .               IS_PUNCTUATION: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Even,\n",
       " under,\n",
       " the,\n",
       " Assyrian,\n",
       " monarchs,\n",
       " and,\n",
       " especially,\n",
       " under,\n",
       " Sardanapalus,\n",
       " Babylon,\n",
       " had,\n",
       " been,\n",
       " a,\n",
       " scene,\n",
       " of,\n",
       " great,\n",
       " intellectual,\n",
       " activity,\n",
       "  ,\n",
       " 111]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we're only interested in analyzing word tokens, we can remove punctuation:\n",
    "for token in doc[:20]:\n",
    "    print(f'TOKEN: {token.text:15} IS_PUNCTUATION: {token.is_punct:}')\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "\n",
    "no_punct[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9df049",
   "metadata": {
    "id": "K63rP_PJVyRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even\n",
      "under\n",
      "the\n",
      "Assyrian\n",
      "monarchs\n",
      "and\n",
      "especially\n",
      "under\n",
      "Sardanapalus\n",
      "Babylon\n",
      "had\n",
      "been\n",
      "a\n",
      "scene\n",
      "of\n",
      "great\n",
      "intellectual\n",
      "activity\n",
      "111\n",
      "Sardanapalus\n",
      "though\n",
      "an\n",
      "Assyrian\n",
      "had\n",
      "been\n",
      "quite\n",
      "Babylon\n",
      "ized\n",
      "He\n",
      "made\n"
     ]
    }
   ],
   "source": [
    "# There are still some space tokens; here's how to remove spaces and newlines:\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[:30]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d52cf1c",
   "metadata": {
    "id": "YHjzgbbgVyRw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['even',\n",
       " 'under',\n",
       " 'the',\n",
       " 'assyrian',\n",
       " 'monarchs',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'under',\n",
       " 'sardanapalus',\n",
       " 'babylon',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'of',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'though',\n",
       " 'an',\n",
       " 'assyrian',\n",
       " 'had',\n",
       " 'been',\n",
       " 'quite',\n",
       " 'babylon',\n",
       " 'ized',\n",
       " 'he',\n",
       " 'made',\n",
       " 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say we also want to remove numbers and lowercase everything that remains\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b26978",
   "metadata": {
    "id": "sbpLhAqnVyR1"
   },
   "source": [
    "One additional common filtering step is to remove stopwords. In theory, stopwords can be any words we're not interested in analyzing, but in practice, they are often the most common words in a language that do not carry much semantic information (e.g., articles, conjunctions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec4b398",
   "metadata": {
    "id": "STyEpj96VyR2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assyrian',\n",
       " 'monarchs',\n",
       " 'especially',\n",
       " 'sardanapalus',\n",
       " 'babylon',\n",
       " 'scene',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'assyrian',\n",
       " 'babylon',\n",
       " 'ized',\n",
       " 'library',\n",
       " 'library',\n",
       " 'paper',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'writing',\n",
       " 'mesopotamia',\n",
       " 'early',\n",
       " 'sumerian',\n",
       " 'days',\n",
       " 'collection',\n",
       " 'unearthed',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'world']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36797",
   "metadata": {
    "id": "wWe736X7mwKF"
   },
   "source": [
    "We've used spaCy's built-in stopword list; membership in this list determines the property `is_stop` for each token. It's good practice to be wary of any built-in stopword list, however -- there's a good chance you will want to remove some words that aren't on the list and to include some that are, especially if you're working with specialized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4851bed0",
   "metadata": {
    "id": "tP8b8upcmx5q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['monarchs',\n",
       " 'especially',\n",
       " 'sardanapalus',\n",
       " 'scene',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'ized',\n",
       " 'library',\n",
       " 'library',\n",
       " 'paper',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'writing',\n",
       " 'mesopotamia',\n",
       " 'early',\n",
       " 'sumerian',\n",
       " 'days',\n",
       " 'collection',\n",
       " 'unearthed',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'world',\n",
       " 'chaldean',\n",
       " 'line',\n",
       " 'babylonian',\n",
       " 'monarchs',\n",
       " 'nabonidus',\n",
       " 'keener',\n",
       " 'literary',\n",
       " 'tastes',\n",
       " 'patronized',\n",
       " 'antiquarian',\n",
       " 'researches',\n",
       " 'date',\n",
       " 'worked',\n",
       " 'investigators',\n",
       " 'accession',\n",
       " 'sargon',\n",
       " 'commemorated',\n",
       " 'fact',\n",
       " 'inscriptions',\n",
       " 'signs',\n",
       " 'disunion',\n",
       " 'empire',\n",
       " 'sought',\n",
       " 'centralize',\n",
       " 'bringing',\n",
       " 'number',\n",
       " 'local',\n",
       " 'gods',\n",
       " 'setting',\n",
       " 'temples',\n",
       " 'device',\n",
       " 'practised',\n",
       " 'successfully',\n",
       " 'romans',\n",
       " 'later',\n",
       " 'times',\n",
       " 'roused',\n",
       " 'jealousy',\n",
       " 'powerful',\n",
       " 'priesthood',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'dominant',\n",
       " 'god',\n",
       " 'babylonians',\n",
       " 'cast',\n",
       " 'possible',\n",
       " 'alternative',\n",
       " 'nabonidus',\n",
       " 'found',\n",
       " 'cyrus',\n",
       " 'persian',\n",
       " 'ruler',\n",
       " 'adjacent',\n",
       " 'median',\n",
       " 'empire',\n",
       " 'cyrus',\n",
       " 'distinguished',\n",
       " 'conquering',\n",
       " 'croesus',\n",
       " 'rich',\n",
       " 'king',\n",
       " 'lydia',\n",
       " 'eastern',\n",
       " 'asia',\n",
       " 'minor',\n",
       " 'came',\n",
       " 'battle',\n",
       " 'outside',\n",
       " 'walls',\n",
       " 'gates',\n",
       " 'city',\n",
       " 'opened',\n",
       " 'soldiers',\n",
       " 'entered',\n",
       " 'city',\n",
       " 'fighting',\n",
       " 'crown',\n",
       " 'prince',\n",
       " 'belshazzar',\n",
       " 'son',\n",
       " 'nabonidus',\n",
       " 'feasting',\n",
       " 'bible',\n",
       " 'relates',\n",
       " 'hand',\n",
       " 'appeared',\n",
       " 'wrote',\n",
       " 'letters',\n",
       " 'fire',\n",
       " 'wall',\n",
       " 'mystical',\n",
       " 'words',\n",
       " 'mene',\n",
       " 'mene',\n",
       " 'tekel',\n",
       " 'upharsin',\n",
       " 'interpreted',\n",
       " 'prophet',\n",
       " 'daniel',\n",
       " 'summoned',\n",
       " 'read',\n",
       " 'riddle',\n",
       " 'god',\n",
       " 'numbered',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'finished',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'weighed',\n",
       " 'balance',\n",
       " 'found',\n",
       " 'wanting',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'given',\n",
       " 'medes',\n",
       " 'persians',\n",
       " 'possibly',\n",
       " 'priests',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'knew',\n",
       " 'writing',\n",
       " 'wall',\n",
       " 'belshazzar',\n",
       " 'killed',\n",
       " 'night',\n",
       " 'says',\n",
       " 'bible',\n",
       " 'nabonidus',\n",
       " 'taken',\n",
       " 'prisoner',\n",
       " 'occupation',\n",
       " 'city',\n",
       " 'peaceful',\n",
       " 'services',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'continued',\n",
       " 'intermission']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll just pick a couple of words we know are in the example\n",
    "custom_stopwords = [\"assyrian\", \"babylon\"]\n",
    "\n",
    "custom_clean = [token for token in clean if token not in custom_stopwords]\n",
    "custom_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dade0",
   "metadata": {
    "id": "_ENXxjBHVyR8"
   },
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on your analytical goals, you may or may not want to do this much cleaning, but hopefully you have a greater appreciation for the kinds of cleaning that can be done with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f31c7",
   "metadata": {
    "id": "qSGJfxiaVyR-"
   },
   "source": [
    "### Counting tokens\n",
    "\n",
    "Now that we've used spaCy to tokenize and clean our text, we can begin one of the most fundamental text analysis tasks: counting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab814841",
   "metadata": {
    "id": "NEFjnPPLVySA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in document:  442\n",
      "Number of tokens in cleaned document:  175\n",
      "Number of unique tokens in cleaned document:  147\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3a188b3",
   "metadata": {
    "id": "MZwZ3En1VySY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 36),\n",
       " (',', 26),\n",
       " ('of', 20),\n",
       " ('.', 16),\n",
       " (' ', 14),\n",
       " ('and', 13),\n",
       " ('in', 9),\n",
       " ('a', 8),\n",
       " ('was', 8),\n",
       " ('to', 8),\n",
       " ('he', 6),\n",
       " ('by', 6),\n",
       " ('babylon', 5),\n",
       " ('had', 4),\n",
       " ('that', 4),\n",
       " ('his', 4),\n",
       " ('nabonidus', 4),\n",
       " ('it', 4),\n",
       " ('\"', 4),\n",
       " ('been', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb9c459",
   "metadata": {
    "id": "cIrMQFp6VySg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('babylon', 5),\n",
       " ('nabonidus', 4),\n",
       " ('bel', 3),\n",
       " ('marduk', 3),\n",
       " ('city', 3),\n",
       " ('assyrian', 2),\n",
       " ('monarchs', 2),\n",
       " ('sardanapalus', 2),\n",
       " ('library', 2),\n",
       " ('writing', 2),\n",
       " ('empire', 2),\n",
       " ('god', 2),\n",
       " ('found', 2),\n",
       " ('cyrus', 2),\n",
       " ('belshazzar', 2),\n",
       " ('bible', 2),\n",
       " ('wall', 2),\n",
       " ('mene', 2),\n",
       " ('thy', 2),\n",
       " ('kingdom', 2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf197c",
   "metadata": {
    "id": "xRNYHP7wVySv"
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Let's consider some other aspects of the text that spaCy exposes for us. One of the most noteworthy features is part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4badcc0f",
   "metadata": {
    "id": "RLVUUOT9VySw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even ADV\n",
      "under ADP\n",
      "the DET\n",
      "Assyrian ADJ\n",
      "monarchs NOUN\n",
      "and CCONJ\n",
      "especially ADV\n",
      "under ADP\n",
      "Sardanapalus PROPN\n",
      ", PUNCT\n",
      "Babylon PROPN\n",
      "had AUX\n",
      "been AUX\n",
      "a DET\n",
      "scene NOUN\n",
      "of ADP\n",
      "great ADJ\n",
      "intellectual ADJ\n",
      "activity NOUN\n",
      ". PUNCT\n",
      "  SPACE\n",
      "{ PUNCT\n",
      "111 NUM\n",
      "} PUNCT\n",
      "Sardanapalus PROPN\n",
      ", PUNCT\n",
      "though SCONJ\n",
      "an DET\n",
      "Assyrian PROPN\n",
      ", PUNCT\n"
     ]
    }
   ],
   "source": [
    "# spaCy provides two levels of POS tagging. Here's the more general level.\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913aed7e",
   "metadata": {
    "id": "FbB2eeMTVyS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even RB\n",
      "under IN\n",
      "the DT\n",
      "Assyrian JJ\n",
      "monarchs NNS\n",
      "and CC\n",
      "especially RB\n",
      "under IN\n",
      "Sardanapalus NNP\n",
      ", ,\n",
      "Babylon NNP\n",
      "had VBD\n",
      "been VBN\n",
      "a DT\n",
      "scene NN\n",
      "of IN\n",
      "great JJ\n",
      "intellectual JJ\n",
      "activity NN\n",
      ". .\n",
      "  _SP\n",
      "{ -LRB-\n",
      "111 CD\n",
      "} -RRB-\n",
      "Sardanapalus NNP\n",
      ", ,\n",
      "though IN\n",
      "an DT\n",
      "Assyrian NNP\n",
      ", ,\n"
     ]
    }
   ],
   "source": [
    "# spaCy also provides the more specific Penn Treenbank tags.\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598b6e3",
   "metadata": {
    "id": "H6X3cbcmVyS4"
   },
   "source": [
    "We can count the occurrences of each part of speech in the text, which may be useful for document classification (fiction may have different proportions of parts of speech relative to nonfiction, for example) or stylistic analysis (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e8c267d",
   "metadata": {
    "id": "wVey8EXsVyS5"
   },
   "outputs": [],
   "source": [
    "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
    "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5012f212",
   "metadata": {
    "id": "Qp6pH7VjVyTC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nouns': 66, 'verbs': 43, 'proper_nouns': 45, 'adjectives': 24, 'adverbs': 12}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_counts = {\n",
    "    \"nouns\": len(nouns),\n",
    "    \"verbs\": len(verbs),\n",
    "    \"proper_nouns\": len(proper_nouns),\n",
    "    \"adjectives\": len(adjectives),\n",
    "    \"adverbs\": len(adverbs) \n",
    "}\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db737b6",
   "metadata": {
    "id": "3y4af1Z7bR4C"
   },
   "source": [
    "spaCy performs morphosyntactic analysis of individual tokens, including lemmatizing inflected or conjugated forms to their base (dictionary) forms. Reducing words to their lemmatized forms can help to make a large corpus more manageable and is generally more effective than just stemming words (trimming the inflected/conjugated endings of words until just the base portion remains), but should only be done if the inflections are not relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5443617b",
   "metadata": {
    "id": "LpYHR5pgb0Tn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarchs        monarch\n",
      "ized            ize\n",
      "made            make\n",
      "tablets         tablet\n",
      "used            use\n",
      "writing         write\n",
      "days            day\n",
      "unearthed       unearth\n",
      "monarchs        monarch\n",
      "had             have\n",
      "tastes          taste\n",
      "patronized      patronize\n",
      "researches      research\n",
      "worked          work\n",
      "investigators   investigator\n",
      "commemorated    commemorate\n",
      "inscriptions    inscription\n",
      "were            be\n",
      "signs           sign\n",
      "sought          seek\n",
      "bringing        bring\n",
      "gods            god\n",
      "setting         set\n",
      "temples         temple\n",
      "practised       practise\n",
      "times           time\n",
      "roused          rouse\n",
      "found           find\n",
      "distinguished   distinguish\n",
      "conquering      conquer\n",
      "came            come\n",
      "was             be\n",
      "walls           wall\n",
      "gates           gate\n",
      "opened          open\n",
      "soldiers        soldier\n",
      "entered         enter\n",
      "fighting        fight\n",
      "feasting        feast\n",
      "relates         relate\n",
      "appeared        appear\n",
      "wrote           write\n",
      "letters         letter\n",
      "words           word\n",
      "interpreted     interpret\n",
      "summoned        summon\n",
      "numbered        number\n",
      "finished        finish\n",
      "weighed         weigh\n",
      "found           find\n",
      "wanting         want\n",
      "given           give\n",
      "priests         priest\n",
      "knew            know\n",
      "killed          kill\n",
      "says            say\n",
      "taken           take\n",
      "services        service\n",
      "continued       continue\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ in [\"NOUN\", \"VERB\"] and token.orth_ != token.lemma_:\n",
    "        print(f\"{token.text:15} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466dc66",
   "metadata": {
    "id": "0_1y3C5LVyTP"
   },
   "source": [
    "### Parsing\n",
    "\n",
    "spaCy's trained models also provide full dependency parsing, tagging word tokens with their syntactic relations to other tokens. This functionality drives spaCy's built-in senticization as well.\n",
    "\n",
    "We won't spend much time exploring this feature, but it's useful to see how it enables the extraction of multi-word \"noun chunks\" from the text. Note also that textacy (discussed below) has a built-in function to extract subject-verb-object triples from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fba09a1c",
   "metadata": {
    "id": "qM28u6ZwE6v5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Assyrian monarchs\n",
      "Sardanapalus\n",
      "Babylon\n",
      "a scene\n",
      "great intellectual activity\n",
      " {111} Sardanapalus\n",
      "an Assyrian\n",
      "He\n",
      "a library\n",
      "a library\n",
      "paper\n",
      "the clay tablets\n",
      "that\n",
      "Mesopotamia\n",
      "early Sumerian days\n",
      "His collection\n",
      "the most precious store\n",
      "historical material\n",
      "the world\n",
      " The last\n"
     ]
    }
   ],
   "source": [
    "for chunk in itertools.islice(doc.noun_chunks, 20):\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aeead",
   "metadata": {
    "id": "29Mqf_S0VyTR"
   },
   "source": [
    "## Named-entity recognition\n",
    "\n",
    "spaCy's models do a pretty good job of identifying and classifying named entities (people, places, organizations).\n",
    "\n",
    "It is also fairly easy to customize and fine-tune these models by providing additional training data (e.g., texts with entities labeled according to the desired scheme), but that's out of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dac73c0",
   "metadata": {
    "id": "KodfOLmHVyTS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assyrian             NORP            Nationalities or religious or political groups\n",
      "Sardanapalus         WORK_OF_ART     Titles of books, songs, etc.\n",
      "Babylon              GPE             Countries, cities, states\n",
      "111                  CARDINAL        Numerals that do not fall under another type\n",
      "Assyrian             NORP            Nationalities or religious or political groups\n",
      "Babylon              ORG             Companies, agencies, institutions, etc.\n",
      "Mesopotamia          LOC             Non-GPE locations, mountain ranges, bodies of water\n",
      "early Sumerian days  DATE            Absolute or relative dates or periods\n",
      "Chaldean             NORP            Nationalities or religious or political groups\n",
      "Babylonian           NORP            Nationalities or religious or political groups\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Sargon               ORG             Companies, agencies, institutions, etc.\n",
      "Romans               NORP            Nationalities or religious or political groups\n",
      "Babylon              GPE             Countries, cities, states\n",
      "Bel Marduk           PERSON          People, including fictional\n",
      "Babylonians          NORP            Nationalities or religious or political groups\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Persian              NORP            Nationalities or religious or political groups\n",
      "Croesus              PERSON          People, including fictional\n",
      "Lydia                PERSON          People, including fictional\n",
      "Eastern Asia Minor   LOC             Non-GPE locations, mountain ranges, bodies of water\n",
      "112                  CARDINAL        Numerals that do not fall under another type\n",
      "Babylon              ORG             Companies, agencies, institutions, etc.\n",
      "538                  CARDINAL        Numerals that do not fall under another type\n",
      "B.C.                 GPE             Countries, cities, states\n",
      "Nabonidus            PERSON          People, including fictional\n",
      "Bible                WORK_OF_ART     Titles of books, songs, etc.\n",
      "Mene                 PERSON          People, including fictional\n",
      "Tekel                ORG             Companies, agencies, institutions, etc.\n",
      "Upharsin             PERSON          People, including fictional\n",
      "Medes                NORP            Nationalities or religious or political groups\n",
      "Persians             NORP            Nationalities or religious or political groups\n",
      "Bel Marduk           PERSON          People, including fictional\n",
      "that night           TIME            Times smaller than a day\n",
      "Bible                WORK_OF_ART     Titles of books, songs, etc.\n",
      "Nabonidus            PERSON          People, including fictional\n",
      "Bel Marduk           PERSON          People, including fictional\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:20} {ent.label_:15} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4102c",
   "metadata": {
    "id": "HGCSdUMAVyTa"
   },
   "source": [
    "What if we only care about geo-political entities or locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3504962a",
   "metadata": {
    "id": "AhIk-M0DVyTc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Babylon', 'GPE'),\n",
       " ('Mesopotamia', 'LOC'),\n",
       " ('Babylon', 'GPE'),\n",
       " ('Eastern Asia Minor', 'LOC'),\n",
       " ('B.C.', 'GPE')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "ent_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4ba8c",
   "metadata": {
    "id": "buJBVUPQVyTe"
   },
   "source": [
    "### Visualizing Parses\n",
    "\n",
    "The built-in displaCy visualizer can render the results of the named-entity recognition, as well as the dependency parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06ac0029",
   "metadata": {
    "id": "NIO_FEoLVyTi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Even under the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Assyrian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " monarchs and especially under \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sardanapalus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " had been a scene of great intellectual activity.  {\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    111\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "} Sardanapalus, though an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Assyrian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", had been quite \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "-ized.  He made a library, a library not of paper but of the clay tablets that were used for writing in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mesopotamia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    early Sumerian days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  His collection has been unearthed and is perhaps the most precious store of historical material in the world.  The last of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chaldean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " line of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylonian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " monarchs, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", had even keener literary tastes.  He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sargon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " I he commemorated the fact by inscriptions.  But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.  This device was to be practised quite successfully by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Romans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " in later times, but in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " it roused the jealousy of the powerful priesthood of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", the dominant god of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylonians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".  They cast about for a possible alternative to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and found it in Cyrus the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Persian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", the ruler of the adjacent Median Empire.  Cyrus had already distinguished himself by conquering \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Croesus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", the rich king of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lydia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eastern Asia Minor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".  {\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    112\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "} He came up against \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", there was a battle outside the walls, and the gates of the city were opened to him (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    538\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    B.C.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ").  His soldiers entered the city without fighting.  The crown prince Belshazzar, the son of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", was feasting, the \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bible\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " relates, when a hand appeared and wrote in letters of fire upon the wall these mystical words: _&quot;Mene, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mene\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tekel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Upharsin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ",&quot;_ which was interpreted by the prophet Daniel, whom he summoned to read the riddle, as &quot;God has numbered thy kingdom and finished it; thou art weighed in the balance and found wanting and thy kingdom is given to the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Medes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Persians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".&quot;  Possibly the priests of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " knew something about that writing on the wall.  Belshazzar was killed \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    that night\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       ", says the \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bible\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was taken prisoner, and the occupation of the city was so peaceful that the services of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " continued without intermission.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaa9f8",
   "metadata": {
    "id": "3F5P7cbMVyTl"
   },
   "source": [
    "# Corpus-level analysis with `textacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeecf51",
   "metadata": {
    "id": "FCwkgf9pVyTl"
   },
   "source": [
    "Let's shift to thinking about a whole corpus rather than a single document. We could analyze multiple documents with spaCy and then knit the results together with some extra Python. Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds corpus analysis features.\n",
    "\n",
    "For reference, here's the [online documentation for textacy](https://textacy.readthedocs.io/en/stable/api_reference/root.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf81ab",
   "metadata": {
    "id": "iMqTW64-VyTp"
   },
   "source": [
    "## Generating corpora\n",
    "\n",
    "We'll use some of the data that is included in textacy as our corpus. It is certainly possible to build your own corpus by importing data from files in plain text, XML, JSON, CSV or other formats, but working with one of textacy's \"pre-cooked\" datasets simplifies things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68366503",
   "metadata": {
    "id": "t8HmDN36VyTq"
   },
   "outputs": [],
   "source": [
    "import textacy.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f910e895",
   "metadata": {
    "id": "VYKe7-BCVyTs"
   },
   "outputs": [],
   "source": [
    "# We'll work with a dataset of ~8,400 (\"almost all\") U.S. Supreme Court\n",
    "# decisions from November 1946 through June 2016\n",
    "# https://github.com/bdewilde/textacy-data/releases/tag/supreme_court_py3_v1.0\n",
    "data = textacy.datasets.SupremeCourt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "538c109f",
   "metadata": {
    "id": "9xmidNbxVyTu"
   },
   "outputs": [],
   "source": [
    "data.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3316c1d",
   "metadata": {
    "id": "VzjG0hgUPP1M"
   },
   "source": [
    "The documentation indicates the metadata that is available with each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55cc5bf3",
   "metadata": {
    "id": "WHMbaig9VyTx"
   },
   "outputs": [],
   "source": [
    "# help(textacy.datasets.supreme_court)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e93e7c",
   "metadata": {
    "id": "6TwurMgHVyT0"
   },
   "source": [
    "textacy is based on the concept of a corpus, whereas spaCy focuses on single documents. A textacy corpus is instantiated with a spaCy language model (we're using the one from the first half of this workshop) that is used to apply its analytical pipeline to each text in the corpus, and also given a set of records consisting of texts with metadata (if metadata is available).\n",
    "\n",
    "Let's go ahead and define a set of records (texts with metadata) that we'll then add to our corpus. To keep the processing time of the data set a bit more manageable, we'll just look at a set of court decisions from a short span of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef249822",
   "metadata": {
    "id": "dTamJjiex_HE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>19/79: Adding SHADY GROVE ORTHOPEDIC ASSOCIATES, P. A. v. ALLSTATE INSURANCE COMPANY</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m display(HTML(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pre>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/79: Adding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</pre>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textacy/corpus.py:287\u001b[0m, in \u001b[0;36mCorpus.add_record\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_record\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: types\u001b[38;5;241m.\u001b[39mRecord) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Add one record to the corpus, processing it into a :class:`spacy.tokens.Doc`\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    using the :attr:`Corpus.spacy_lang` pipeline.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m        record\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspacy_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     doc\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mmeta \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_valid_doc(doc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1008\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    989\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1099\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1091\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1089\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1090\u001b[0m     )\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokenizer.pyx:155\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokenizer.pyx:191\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokenizer.pyx:395\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokenizer.pyx:473\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/vocab.pyx:160\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/vocab.pyx:197\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/lang/lex_attrs.py:145\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    141\u001b[0m             shape\u001b[38;5;241m.\u001b[39mappend(shape_char)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprefix\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "corpus = textacy.Corpus(nlp)\n",
    "\n",
    "# There are 79 docs in this range -- they'll take a minute or two to process\n",
    "recent_decisions = data.records(date_range=('2010-01-01', '2010-12-31'))\n",
    "\n",
    "for i, record in enumerate(recent_decisions):\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(f\"<pre>{i+1:>2}/79: Adding {record[1]['case_name']}</pre>\"))\n",
    "    corpus.add_record(record)\n",
    "\n",
    "# If the three lines above are taking too long to process all 79 docs,\n",
    "# comment them out and uncomment the two lines below to download and import\n",
    "# a preprocessed version of the corpus\n",
    "\n",
    "#!wget https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/scotus_2010.bin.gz\n",
    "#corpus = textacy.Corpus.load(nlp, \"scotus_2010.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ceaa438",
   "metadata": {
    "id": "tBnvE5ZJVyT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Doc(13007 tokens: \"Respondent New York City taxes the possession o...\")',\n",
       " 'Doc(72325 tokens: \"As amended by §203 of the Bipartisan Campaign R...\")',\n",
       " 'Doc(8333 tokens: \"Under 28 U. S. C. §2254(d)(2), a federal court ...\")',\n",
       " 'Doc(9947 tokens: \"The Illegal Immigration Reform and Immigrant Re...\")',\n",
       " 'Doc(5508 tokens: \"Per Curiam.  From beginning to end, judicial pr...\")']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "[doc._.preview for doc in corpus[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145585ed",
   "metadata": {
    "id": "JuBjFKQ4VyT8"
   },
   "source": [
    "We can see that the type of each item in the corpus is a `Doc` - this is a processed spaCy output document, with all of the extracted features. textacy provides some capacity to work with those features via its API, and also exposes new document-level features, such as ngrams and algorithms to determine a document's readability level, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028b9cf",
   "metadata": {
    "id": "-DFWqESvVyT8"
   },
   "source": [
    "We can filter this corpus based on metadata attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e0a21e4",
   "metadata": {
    "id": "zW_s4Eodx_HF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'issue': '80170',\n",
       " 'issue_area': 8,\n",
       " 'n_min_votes': 3,\n",
       " 'case_name': 'HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK',\n",
       " 'maj_opinion_author': 111,\n",
       " 'decision_date': '2010-01-25',\n",
       " 'decision_direction': 'conservative',\n",
       " 'n_maj_votes': 5,\n",
       " 'us_cite_id': '559 U.S. 1',\n",
       " 'argument_date': '2009-11-03'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]._.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a1987d8",
   "metadata": {
    "id": "j-CqNgQaVyUC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
    "supermajorities = [doc for doc in corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6)]\n",
    "len(supermajorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "479c3d7d",
   "metadata": {
    "id": "4o5hy0pCVyUG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(8333 tokens: \"Under 28 U. S. C. §2254(d)(2), a federal court ...\")'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supermajorities[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69e164",
   "metadata": {
    "id": "B8YO2bgIVyUM"
   },
   "source": [
    "## Finding important words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e91052d",
   "metadata": {
    "id": "_wOMpBE0VyUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents:  79\n",
      "number of sentences:  42470\n",
      "number of tokens:  1189205\n"
     ]
    }
   ],
   "source": [
    "print(\"number of documents: \", corpus.n_docs)\n",
    "print(\"number of sentences: \", corpus.n_sents)\n",
    "print(\"number of tokens: \", corpus.n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eac812c5",
   "metadata": {
    "id": "WiQQrx5SVyUO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Respondent': 250,\n",
       " 'New': 413,\n",
       " 'York': 284,\n",
       " 'City': 340,\n",
       " 'taxes': 56,\n",
       " 'the': 56729,\n",
       " 'possession': 216,\n",
       " 'of': 30659,\n",
       " 'cigarettes': 23,\n",
       " '.': 44541,\n",
       " 'Petitioner': 246,\n",
       " 'Hemi': 82,\n",
       " 'Group': 37,\n",
       " ',': 72059,\n",
       " 'based': 526,\n",
       " 'in': 14744,\n",
       " 'Mexico': 16,\n",
       " 'sells': 6,\n",
       " 'online': 49,\n",
       " 'to': 25427,\n",
       " 'residents': 25,\n",
       " 'Neither': 74,\n",
       " 'state': 1920,\n",
       " 'nor': 242,\n",
       " 'city': 48,\n",
       " 'law': 1899,\n",
       " 'requires': 352,\n",
       " 'out': 548,\n",
       " '-': 10949,\n",
       " 'sellers': 13,\n",
       " 'such': 1571,\n",
       " 'as': 5683,\n",
       " 'charge': 102,\n",
       " 'collect': 35,\n",
       " 'or': 5809,\n",
       " 'remit': 7,\n",
       " \"'s\": 9728,\n",
       " 'tax': 175,\n",
       " ';': 4864,\n",
       " 'instead': 175,\n",
       " 'must': 1197,\n",
       " 'recover': 50,\n",
       " 'its': 2590,\n",
       " 'on': 5214,\n",
       " 'sales': 48,\n",
       " 'directly': 134,\n",
       " 'from': 2842,\n",
       " 'purchasers': 16,\n",
       " 'But': 1066,\n",
       " 'Jenkins': 51,\n",
       " 'Act': 1278,\n",
       " '15': 449,\n",
       " 'U.': 5900,\n",
       " 'S.': 6072,\n",
       " 'C.': 1321,\n",
       " '§': 6375,\n",
       " '375': 37,\n",
       " '378': 36,\n",
       " 'submit': 57,\n",
       " 'customer': 9,\n",
       " 'information': 310,\n",
       " 'States': 2256,\n",
       " 'into': 572,\n",
       " 'which': 2646,\n",
       " 'they': 1302,\n",
       " 'ship': 22,\n",
       " 'and': 15113,\n",
       " 'State': 1262,\n",
       " 'has': 2191,\n",
       " 'agreed': 176,\n",
       " 'forward': 51,\n",
       " 'that': 18373,\n",
       " 'That': 507,\n",
       " 'helps': 26,\n",
       " 'track': 19,\n",
       " 'down': 115,\n",
       " 'cigarette': 14,\n",
       " 'who': 1201,\n",
       " 'do': 1015,\n",
       " 'not': 8499,\n",
       " 'pay': 124,\n",
       " 'their': 1507,\n",
       " 'Against': 27,\n",
       " 'backdrop': 10,\n",
       " 'filed': 402,\n",
       " 'this': 3792,\n",
       " 'lawsuit': 51,\n",
       " 'under': 2030,\n",
       " 'Racketeer': 6,\n",
       " 'Influenced': 6,\n",
       " 'Corrupt': 6,\n",
       " 'Organizations': 11,\n",
       " '(': 14766,\n",
       " 'RICO': 99,\n",
       " ')': 18634,\n",
       " 'alleging': 33,\n",
       " 'failure': 216,\n",
       " 'file': 195,\n",
       " 'reports': 84,\n",
       " 'with': 3669,\n",
       " 'constituted': 41,\n",
       " 'mail': 76,\n",
       " 'wire': 37,\n",
       " 'fraud': 328,\n",
       " 'are': 2675,\n",
       " 'defined': 144,\n",
       " '\"': 30807,\n",
       " 'racketeering': 5,\n",
       " 'activit[ies': 1,\n",
       " ']': 3140,\n",
       " '18': 499,\n",
       " '1961(1': 4,\n",
       " 'subject': 502,\n",
       " 'enforcement': 148,\n",
       " 'civil': 198,\n",
       " '1964(c': 12,\n",
       " 'The': 5952,\n",
       " 'District': 1052,\n",
       " 'Court': 5729,\n",
       " 'dismissed': 61,\n",
       " 'claims': 611,\n",
       " 'but': 1669,\n",
       " 'Second': 481,\n",
       " 'Circuit': 695,\n",
       " 'vacated': 47,\n",
       " 'judgment': 1061,\n",
       " 'remanded': 129,\n",
       " 'Among': 37,\n",
       " 'other': 1491,\n",
       " 'things': 81,\n",
       " 'Appeals': 794,\n",
       " 'held': 625,\n",
       " 'asserted': 113,\n",
       " 'injury': 174,\n",
       " '—': 1921,\n",
       " 'lost': 60,\n",
       " 'revenue': 51,\n",
       " 'came': 32,\n",
       " 'about': 767,\n",
       " 'by': 4551,\n",
       " 'reason': 416,\n",
       " 'predicate': 75,\n",
       " 'frauds': 23,\n",
       " 'It': 1106,\n",
       " 'accordingly': 30,\n",
       " 'determined': 150,\n",
       " 'had': 1656,\n",
       " 'stated': 232,\n",
       " 'a': 18294,\n",
       " 'valid': 136,\n",
       " 'claim': 938,\n",
       " 'Held': 68,\n",
       " ':': 1401,\n",
       " 'is': 9354,\n",
       " 'reversed': 181,\n",
       " 'case': 1985,\n",
       " '541': 78,\n",
       " 'F.': 1407,\n",
       " '3d': 1008,\n",
       " '425': 32,\n",
       " 'Chief': 146,\n",
       " 'Justice': 872,\n",
       " 'Roberts': 124,\n",
       " 'delivered': 157,\n",
       " 'opinion': 1212,\n",
       " 'part': 695,\n",
       " 'concluding': 116,\n",
       " 'because': 1303,\n",
       " 'can': 1523,\n",
       " 'show': 217,\n",
       " 'it': 4838,\n",
       " 'alleged': 202,\n",
       " 'violation': 393,\n",
       " 'Pp': 393,\n",
       " '5': 623,\n",
       " 'To': 306,\n",
       " 'establish': 166,\n",
       " 'an': 4200,\n",
       " 'plaintiff': 258,\n",
       " 'offense': 354,\n",
       " 'only': 1770,\n",
       " 'was': 3521,\n",
       " \"'\": 4309,\n",
       " 'for': 7978,\n",
       " 'cause': 375,\n",
       " 'his': 1845,\n",
       " 'proximate': 26,\n",
       " 'well': 492,\n",
       " 'Holmes': 34,\n",
       " 'v.': 4345,\n",
       " 'Securities': 57,\n",
       " 'Investor': 6,\n",
       " 'Protection': 60,\n",
       " 'Corporation': 43,\n",
       " '503': 63,\n",
       " '258': 22,\n",
       " '268': 37,\n",
       " 'Proximate': 2,\n",
       " 'purposes': 294,\n",
       " 'should': 1012,\n",
       " 'be': 4659,\n",
       " 'evaluated': 12,\n",
       " 'light': 286,\n",
       " 'common': 264,\n",
       " 'foundations': 7,\n",
       " 'thus': 437,\n",
       " 'some': 736,\n",
       " 'direct': 161,\n",
       " 'relation': 65,\n",
       " 'between': 624,\n",
       " 'injurious': 5,\n",
       " 'conduct': 686,\n",
       " 'Ibid': 461,\n",
       " 'A': 857,\n",
       " 'link': 22,\n",
       " 'too': 179,\n",
       " 'remote': 23,\n",
       " 'purely': 34,\n",
       " 'contingent': 12,\n",
       " 'indirec[t': 2,\n",
       " 'insufficient': 68,\n",
       " 'Id.': 1254,\n",
       " 'at': 9800,\n",
       " '271': 14,\n",
       " '274': 39,\n",
       " 'causal': 12,\n",
       " 'theory': 205,\n",
       " 'satisfy': 106,\n",
       " 'relationship': 136,\n",
       " 'requirement': 376,\n",
       " 'Indeed': 196,\n",
       " 'here': 652,\n",
       " 'far': 204,\n",
       " 'more': 1214,\n",
       " 'attenuated': 12,\n",
       " 'than': 1270,\n",
       " 'one': 1262,\n",
       " 'rejected': 248,\n",
       " 'According': 80,\n",
       " 'committed': 136,\n",
       " 'selling': 17,\n",
       " 'failing': 51,\n",
       " 'required': 397,\n",
       " 'Without': 38,\n",
       " 'could': 1033,\n",
       " 'pass': 41,\n",
       " 'even': 910,\n",
       " 'if': 1636,\n",
       " 'been': 1138,\n",
       " 'so': 1158,\n",
       " 'inclined': 8,\n",
       " 'Some': 67,\n",
       " 'customers': 53,\n",
       " 'legally': 61,\n",
       " 'obligated': 10,\n",
       " 'failed': 197,\n",
       " 'Because': 307,\n",
       " 'did': 1303,\n",
       " 'receive': 130,\n",
       " 'determine': 326,\n",
       " 'pursue': 78,\n",
       " 'those': 1158,\n",
       " 'payment': 52,\n",
       " 'thereby': 85,\n",
       " 'injured': 30,\n",
       " 'amount': 258,\n",
       " 'portion': 78,\n",
       " 'back': 130,\n",
       " 'were': 1191,\n",
       " 'never': 247,\n",
       " 'collected': 7,\n",
       " 'As': 716,\n",
       " 'reiterated': 20,\n",
       " '[': 3078,\n",
       " 't]he': 180,\n",
       " 'general': 417,\n",
       " 'tendency': 12,\n",
       " 'regard': 62,\n",
       " 'damages': 174,\n",
       " 'least': 289,\n",
       " 'go': 94,\n",
       " 'beyond': 230,\n",
       " 'first': 718,\n",
       " 'step': 94,\n",
       " 'i': 1008,\n",
       " 'd.': 975,\n",
       " '272': 29,\n",
       " 'applies': 365,\n",
       " 'full': 243,\n",
       " 'force': 233,\n",
       " 'inquiries': 17,\n",
       " 'e.g.': 879,\n",
       " 'ibid': 207,\n",
       " 'causation': 23,\n",
       " 'move': 26,\n",
       " 'suffers': 15,\n",
       " 'same': 765,\n",
       " 'defect': 17,\n",
       " 'Anza': 20,\n",
       " 'Ideal': 19,\n",
       " 'Steel': 25,\n",
       " 'Supply': 10,\n",
       " 'Corp.': 207,\n",
       " '547': 76,\n",
       " '451': 41,\n",
       " '458': 59,\n",
       " '461': 50,\n",
       " 'where': 588,\n",
       " 'causing': 13,\n",
       " 'harm': 146,\n",
       " 'distinct': 73,\n",
       " 'giving': 86,\n",
       " 'rise': 60,\n",
       " 'see': 1707,\n",
       " 'disconnect': 2,\n",
       " 'sharper': 2,\n",
       " 'In': 1893,\n",
       " 'party': 520,\n",
       " 'both': 465,\n",
       " 'engaged': 65,\n",
       " 'harmful': 22,\n",
       " 'fraudulent': 38,\n",
       " 'act': 231,\n",
       " 'Here': 107,\n",
       " 'liability': 247,\n",
       " 'rests': 65,\n",
       " 'just': 333,\n",
       " 'separate': 209,\n",
       " 'actions': 258,\n",
       " 'carried': 38,\n",
       " 'parties': 678,\n",
       " 'extend': 71,\n",
       " 'situations': 32,\n",
       " 'defendant': 613,\n",
       " 'third': 159,\n",
       " 'made': 653,\n",
       " 'easier': 33,\n",
       " 'fourth': 41,\n",
       " 'taxpayer': 11,\n",
       " 'taxpayers': 13,\n",
       " 'caused': 112,\n",
       " 'place': 389,\n",
       " 'decided': 169,\n",
       " 'Put': 8,\n",
       " 'simply': 305,\n",
       " 'obligation': 81,\n",
       " 'before': 826,\n",
       " 'stretched': 4,\n",
       " 'chain': 26,\n",
       " 'declines': 15,\n",
       " 'today': 275,\n",
       " 'See': 3037,\n",
       " '460': 41,\n",
       " '9': 509,\n",
       " 'b': 120,\n",
       " 'attempts': 59,\n",
       " 'avoid': 218,\n",
       " 'conclusion': 453,\n",
       " 'characterizing': 9,\n",
       " 'merely': 201,\n",
       " 'systematic': 37,\n",
       " 'scheme': 133,\n",
       " 'defraud': 39,\n",
       " 'escape': 26,\n",
       " 'embraced': 14,\n",
       " 'all': 1495,\n",
       " 'indirectly': 20,\n",
       " 'harmed': 15,\n",
       " 'precedent': 165,\n",
       " 'would': 2822,\n",
       " 'become': 106,\n",
       " 'mere': 81,\n",
       " 'pleading': 45,\n",
       " 'rule': 715,\n",
       " 'makes': 276,\n",
       " 'clear': 455,\n",
       " 'compensable': 5,\n",
       " 'flowing': 5,\n",
       " '...': 1600,\n",
       " 'necessarily': 133,\n",
       " 'acts': 187,\n",
       " 'supra': 994,\n",
       " '457': 31,\n",
       " 'led': 63,\n",
       " 'injuries': 28,\n",
       " 'also': 1734,\n",
       " 'errs': 17,\n",
       " 'relying': 43,\n",
       " 'Bridge': 14,\n",
       " 'Phoenix': 3,\n",
       " 'Bond': 5,\n",
       " '&': 499,\n",
       " 'Indemnity': 8,\n",
       " 'Co.': 462,\n",
       " '553': 61,\n",
       " '_': 2073,\n",
       " 'There': 268,\n",
       " 'plaintiffs': 336,\n",
       " 'straightforward': 45,\n",
       " 'involved': 144,\n",
       " 'easily': 77,\n",
       " 'identifiable': 4,\n",
       " 'connection': 70,\n",
       " 'issue': 788,\n",
       " 'there': 795,\n",
       " 'petitioners': 370,\n",
       " 'misrepresentations': 16,\n",
       " 'no': 1984,\n",
       " 'independent': 312,\n",
       " 'factors': 215,\n",
       " 'account[ed': 3,\n",
       " 'anything': 112,\n",
       " 'Multiple': 4,\n",
       " 'steps': 56,\n",
       " 'And': 721,\n",
       " 'contrast': 109,\n",
       " 'certainly': 79,\n",
       " '10': 567,\n",
       " '14': 413,\n",
       " 'J.': 1306,\n",
       " 'Scalia': 334,\n",
       " 'Thomas': 223,\n",
       " 'Alito': 154,\n",
       " 'JJ': 104,\n",
       " 'joined': 184,\n",
       " 'Ginsburg': 118,\n",
       " 'concurring': 560,\n",
       " 'Breyer': 176,\n",
       " 'dissenting': 483,\n",
       " 'Stevens': 381,\n",
       " 'Kennedy': 228,\n",
       " 'Sotomayor': 120,\n",
       " 'took': 146,\n",
       " 'consideration': 157,\n",
       " 'decision': 1001,\n",
       " 'HEMI': 3,\n",
       " 'GROUP': 3,\n",
       " 'LLC': 25,\n",
       " 'KAI': 3,\n",
       " 'GACHUPIN': 3,\n",
       " 'PETITIONERS': 68,\n",
       " 'CITY': 12,\n",
       " 'OF': 57,\n",
       " 'NEW': 8,\n",
       " 'YORK': 6,\n",
       " 'writ': 286,\n",
       " 'certiorari': 374,\n",
       " 'united': 168,\n",
       " 'states': 313,\n",
       " 'court': 2667,\n",
       " 'appeals': 221,\n",
       " 'second': 420,\n",
       " 'circuit': 174,\n",
       " 'January': 58,\n",
       " '25': 197,\n",
       " '2010': 352,\n",
       " 'seldom': 5,\n",
       " 'own': 406,\n",
       " 'Federal': 681,\n",
       " 'however': 606,\n",
       " 'vendors': 6,\n",
       " 'argues': 158,\n",
       " 'constitutes': 72,\n",
       " 'lose': 25,\n",
       " 'tens': 7,\n",
       " 'millions': 15,\n",
       " 'dollars': 18,\n",
       " 'unrecovered': 1,\n",
       " 'we': 1960,\n",
       " 'hold': 205,\n",
       " 'We': 899,\n",
       " 'therefore': 414,\n",
       " 'reverse': 63,\n",
       " 'contrary': 247,\n",
       " 'I': 1534,\n",
       " 'This': 818,\n",
       " 'arises': 26,\n",
       " 'motion': 320,\n",
       " 'dismiss': 90,\n",
       " 'accept': 144,\n",
       " 'true': 206,\n",
       " 'factual': 154,\n",
       " 'allegations': 57,\n",
       " 'amended': 139,\n",
       " 'complaint': 177,\n",
       " 'Leatherman': 1,\n",
       " 'Tarrant': 1,\n",
       " 'County': 123,\n",
       " 'Narcotics': 5,\n",
       " 'Intelligence': 4,\n",
       " 'Coordination': 3,\n",
       " 'Unit': 2,\n",
       " '507': 43,\n",
       " '163': 29,\n",
       " '164': 30,\n",
       " '1993': 96,\n",
       " 'authorizes': 79,\n",
       " 'impose': 182,\n",
       " 'N.': 260,\n",
       " 'Y.': 84,\n",
       " 'Unconsol': 2,\n",
       " 'Law': 303,\n",
       " 'Ann': 196,\n",
       " '9436(1': 2,\n",
       " 'West': 131,\n",
       " 'Supp': 333,\n",
       " '2009': 563,\n",
       " 'Under': 214,\n",
       " 'authority': 532,\n",
       " 'levied': 3,\n",
       " '$': 227,\n",
       " '1.50': 1,\n",
       " 'per': 215,\n",
       " 'pack': 3,\n",
       " 'each': 307,\n",
       " 'standard': 410,\n",
       " 'possessed': 24,\n",
       " 'within': 601,\n",
       " 'sale': 67,\n",
       " 'use': 505,\n",
       " 'Admin': 11,\n",
       " 'Code': 355,\n",
       " '11': 518,\n",
       " '1302(a': 1,\n",
       " '2008': 437,\n",
       " 'Record': 81,\n",
       " 'A1016': 1,\n",
       " 'When': 243,\n",
       " 'buy': 10,\n",
       " 'seller': 12,\n",
       " 'responsible': 68,\n",
       " 'charging': 24,\n",
       " 'collecting': 47,\n",
       " 'remitting': 1,\n",
       " 'Tax': 17,\n",
       " '471(2': 1,\n",
       " 'Out': 1,\n",
       " 'Smokes-Spirits.com': 3,\n",
       " 'Inc.': 546,\n",
       " '432': 26,\n",
       " '433': 20,\n",
       " 'CA2': 124,\n",
       " 'Instead': 129,\n",
       " 'recovering': 2,\n",
       " 'sold': 31,\n",
       " 'outside': 167,\n",
       " 'difficult': 148,\n",
       " 'often': 211,\n",
       " 'reluctant': 14,\n",
       " 'tough': 2,\n",
       " 'One': 98,\n",
       " 'way': 281,\n",
       " 'gather': 8,\n",
       " 'assist': 16,\n",
       " 'through': 426,\n",
       " '63': 65,\n",
       " 'Stat': 412,\n",
       " '884': 6,\n",
       " '69': 34,\n",
       " '627': 10,\n",
       " 'register': 74,\n",
       " 'report': 114,\n",
       " 'tobacco': 12,\n",
       " 'administrators': 15,\n",
       " 'listing': 41,\n",
       " 'name': 80,\n",
       " 'address': 246,\n",
       " 'quantity': 12,\n",
       " 'purchased': 18,\n",
       " 'have': 3351,\n",
       " 'executed': 25,\n",
       " 'agreement': 420,\n",
       " 'undertake': 32,\n",
       " 'cooperate': 12,\n",
       " 'fully': 87,\n",
       " 'keep': 250,\n",
       " 'promptly': 19,\n",
       " 'informed': 95,\n",
       " 'reference': 119,\n",
       " 'any': 2359,\n",
       " 'person': 421,\n",
       " 'transaction': 36,\n",
       " 'including': 397,\n",
       " 'i]nformation': 1,\n",
       " 'obtained': 83,\n",
       " 'may': 1890,\n",
       " 'result': 353,\n",
       " 'additional': 214,\n",
       " 'provided': 261,\n",
       " 'disclosure': 280,\n",
       " 'permissible': 48,\n",
       " 'existing': 105,\n",
       " 'laws': 397,\n",
       " 'agreements': 70,\n",
       " 'A1003': 1,\n",
       " 'asserts': 84,\n",
       " 'forwards': 1,\n",
       " 'A998': 1,\n",
       " 'Amended': 8,\n",
       " 'Compl': 5,\n",
       " '¶54': 1,\n",
       " '¶¶58': 2,\n",
       " '59': 49,\n",
       " 'company': 103,\n",
       " 'does': 1700,\n",
       " 'alleges': 24,\n",
       " 'cost': 43,\n",
       " 'hundreds': 20,\n",
       " 'year': 356,\n",
       " 'excise': 5,\n",
       " 'A996': 3,\n",
       " 'Based': 35,\n",
       " 'federal': 1542,\n",
       " 'B': 206,\n",
       " 'provides': 363,\n",
       " 'private': 339,\n",
       " 'action': 606,\n",
       " 'a]ny': 17,\n",
       " 'business': 445,\n",
       " 'property': 652,\n",
       " 'section': 200,\n",
       " '1962': 18,\n",
       " 'chapter': 48,\n",
       " 'Section': 267,\n",
       " 'turn': 125,\n",
       " 'contains': 75,\n",
       " 'criminal': 495,\n",
       " 'provisions': 483,\n",
       " 'Specifically': 22,\n",
       " '1962(c': 2,\n",
       " 'invokes': 22,\n",
       " 'unlawful': 90,\n",
       " 'employed': 52,\n",
       " 'associated': 35,\n",
       " 'enterprise': 19,\n",
       " 'activities': 218,\n",
       " 'affect': 77,\n",
       " 'interstate': 88,\n",
       " 'commerce': 83,\n",
       " 'participate': 35,\n",
       " 'affairs': 31,\n",
       " 'pattern': 22,\n",
       " 'activity': 154,\n",
       " 'R]acketeering': 1,\n",
       " 'include': 214,\n",
       " 'number': 220,\n",
       " 'called': 103,\n",
       " 'two': 757,\n",
       " 'identifying': 41,\n",
       " 'constitute': 120,\n",
       " 'offenses': 123,\n",
       " 'A980': 4,\n",
       " 'Invoking': 5,\n",
       " 'suffered': 53,\n",
       " 'form': 197,\n",
       " 'terms--\"by': 1,\n",
       " 'contest': 21,\n",
       " 'characterization': 26,\n",
       " 'violations': 109,\n",
       " 'actionable': 7,\n",
       " 'assume': 108,\n",
       " 'without': 736,\n",
       " 'deciding': 106,\n",
       " 'material': 172,\n",
       " 'serve': 111,\n",
       " 'determining': 172,\n",
       " 'owner': 40,\n",
       " 'officer': 105,\n",
       " 'Kai': 1,\n",
       " 'Gachupin': 3,\n",
       " 'individual': 316,\n",
       " 'duty': 194,\n",
       " 'Nexicon': 1,\n",
       " 'No': 393,\n",
       " '03': 5,\n",
       " 'CV': 1,\n",
       " '383': 38,\n",
       " 'DAB': 1,\n",
       " '2006': 287,\n",
       " 'WL': 20,\n",
       " '647716': 1,\n",
       " '*': 387,\n",
       " '7-*8': 1,\n",
       " 'SDNY': 24,\n",
       " 'Mar.': 48,\n",
       " 'formed': 60,\n",
       " '7-*10': 1,\n",
       " 'ground': 196,\n",
       " 'whether': 1708,\n",
       " 'loss': 58,\n",
       " '1964': 45,\n",
       " 'further': 411,\n",
       " 'proceedings': 325,\n",
       " 'established': 290,\n",
       " 'operated': 29,\n",
       " '447': 50,\n",
       " '448': 15,\n",
       " '444': 47,\n",
       " '445': 42,\n",
       " 'concluded': 310,\n",
       " '440': 25,\n",
       " 'viable': 5,\n",
       " 'Judge': 102,\n",
       " 'Winter': 15,\n",
       " 'dissented': 11,\n",
       " 'petition': 473,\n",
       " 'asking': 53,\n",
       " 'Pet': 233,\n",
       " 'Cert': 205,\n",
       " 'i.': 13,\n",
       " 'granted': 316,\n",
       " '556': 72,\n",
       " 'II': 307,\n",
       " 'Though': 26,\n",
       " 'framed': 16,\n",
       " 'single': 202,\n",
       " 'question': 966,\n",
       " 'raises': 43,\n",
       " 'issues': 125,\n",
       " 'First': 800,\n",
       " 'allegedly': 49,\n",
       " 'decide': 301,\n",
       " '1992': 105,\n",
       " 'set': 342,\n",
       " 'forth': 168,\n",
       " 'addressed': 110,\n",
       " 'brought': 178,\n",
       " 'SIPC': 8,\n",
       " 'against': 799,\n",
       " 'defendants': 177,\n",
       " 'whom': 156,\n",
       " 'manipulated': 4,\n",
       " 'stock': 38,\n",
       " 'prices': 33,\n",
       " '262': 20,\n",
       " '263': 42,\n",
       " 'reimburse': 2,\n",
       " 'certain': 445,\n",
       " 'registered': 87,\n",
       " 'broker': 8,\n",
       " 'dealers': 47,\n",
       " 'event': 105,\n",
       " 'unable': 44,\n",
       " 'meet': 64,\n",
       " 'financial': 80,\n",
       " 'obligations': 52,\n",
       " '261': 19,\n",
       " 'conspiracy': 59,\n",
       " 'manipulators': 1,\n",
       " 'detected': 2,\n",
       " 'collapsed': 5,\n",
       " 'insurer': 1,\n",
       " 'ultimately': 76,\n",
       " 'hook': 2,\n",
       " 'nearly': 73,\n",
       " '13': 452,\n",
       " 'million': 63,\n",
       " 'cover': 87,\n",
       " 'conspirators': 6,\n",
       " 'phrase': 187,\n",
       " 'used': 324,\n",
       " 'explained': 261,\n",
       " 'Applying': 43,\n",
       " 'hand': 92,\n",
       " 'quoting': 424,\n",
       " 'Associated': 7,\n",
       " 'Gen.': 42,\n",
       " 'Contractors': 4,\n",
       " 'Cal': 71,\n",
       " 'Carpenters': 1,\n",
       " '459': 44,\n",
       " '519': 42,\n",
       " '534': 55,\n",
       " '1983': 137,\n",
       " 'Southern': 38,\n",
       " 'Pacific': 90,\n",
       " 'Darnell': 1,\n",
       " 'Taenzer': 1,\n",
       " 'Lumber': 2,\n",
       " '245': 15,\n",
       " '531': 44,\n",
       " '533': 70,\n",
       " '1918': 5,\n",
       " 'internal': 412,\n",
       " 'quotation': 395,\n",
       " 'marks': 398,\n",
       " 'omitted': 484,\n",
       " 'Our': 158,\n",
       " 'cases': 934,\n",
       " 'confirm': 38,\n",
       " 'slip': 349,\n",
       " 'op': 346,\n",
       " '19': 240,\n",
       " 'us': 462,\n",
       " 'confirms': 28,\n",
       " 'indirect': 9,\n",
       " 'considered': 217,\n",
       " 'competitor': 10,\n",
       " 'National': 215,\n",
       " 'defrauded': 9,\n",
       " 'able': 116,\n",
       " 'undercut': 11,\n",
       " 'lower': 128,\n",
       " 'offered': 58,\n",
       " 'contended': 20,\n",
       " 'allowed': 105,\n",
       " 'attract': 11,\n",
       " 'expense': 17,\n",
       " 'Finding': 24,\n",
       " 'victim': 66,\n",
       " 'being': 186,\n",
       " 'recognized': 245,\n",
       " 'harms': 35,\n",
       " 'when': 1284,\n",
       " 'applicable': 184,\n",
       " 'offering': 10,\n",
       " 'entirely': 116,\n",
       " 'defrauding': 3,\n",
       " 'constituting': 33,\n",
       " 'Thus': 258,\n",
       " 'viewed': 48,\n",
       " 'point': 360,\n",
       " 'important': 232,\n",
       " 'nevertheless': 53,\n",
       " 'found': 348,\n",
       " 'distinction': 113,\n",
       " 'relevant': 386,\n",
       " 'sufficient': 207,\n",
       " 'defeat': 28,\n",
       " 'decline': 53,\n",
       " 'cf': 120,\n",
       " 'n.': 894,\n",
       " '46': 97,\n",
       " 'finding': 240,\n",
       " 'antitrust': 30,\n",
       " 'context': 325,\n",
       " 'stems': 7,\n",
       " 'most': 431,\n",
       " 'persons': 247,\n",
       " 'victims': 59,\n",
       " 'highlighted': 9,\n",
       " 'better': 85,\n",
       " 'situated': 14,\n",
       " 'incentive': 33,\n",
       " 'sue': 49,\n",
       " '269': 16,\n",
       " '270': 21,\n",
       " 'seek': 211,\n",
       " 'recovery': 43,\n",
       " 'imposes': 87,\n",
       " '2.75': 1,\n",
       " 'double': 34,\n",
       " 'what': 783,\n",
       " 'charges': 80,\n",
       " '471(1': 1,\n",
       " 'opine': 2,\n",
       " 'bring': 71,\n",
       " 'Suffice': 3,\n",
       " 'say': 221,\n",
       " 'concrete': 29,\n",
       " 'incentives': 18,\n",
       " 'try': 31,\n",
       " 'accuses': 2,\n",
       " 'Anzas': 1,\n",
       " 'substantial': 213,\n",
       " 'money': 134,\n",
       " 'If': 429,\n",
       " 'expected': 39,\n",
       " 'appropriate': 227,\n",
       " 'remedies': 79,\n",
       " 'dissent': 470,\n",
       " 'foreseeability': 6,\n",
       " 'rather': 317,\n",
       " 'existence': 97,\n",
       " 'sufficiently': 94,\n",
       " 'find': 233,\n",
       " 'satisfied': 77,\n",
       " 'foreseeable': 23,\n",
       " 'consequence': 79,\n",
       " 'intended': 231,\n",
       " 'indeed': 87,\n",
       " 'desired': 13,\n",
       " 'falls': 55,\n",
       " 'risks': 36,\n",
       " 'Congress': 1679,\n",
       " 'sought': 242,\n",
       " 'prevent': 189,\n",
       " 'Post': 202,\n",
       " '6': 594,\n",
       " 'line': 163,\n",
       " 'reasoning': 102,\n",
       " 'sounds': 10,\n",
       " 'familiar': 33,\n",
       " 'precisely': 50,\n",
       " 'argument': 541,\n",
       " 'lodged': 7,\n",
       " 'majority': 415,\n",
       " 'criticized': 16,\n",
       " 'view': 556,\n",
       " 'permit[ting': 1,\n",
       " 'evade': 9,\n",
       " 'consequences': 158,\n",
       " 'behavior': 76,\n",
       " '470': 56,\n",
       " 'carry': 60,\n",
       " 'day': 202,\n",
       " 'asked': 146,\n",
       " 'revisit': 12,\n",
       " 'concepts': 6,\n",
       " 'course': 264,\n",
       " 'many': 355,\n",
       " 'shapes': 5,\n",
       " 'precedents': 218,\n",
       " 'make': 467,\n",
       " 'focus': 62,\n",
       " 'directness': 4,\n",
       " 'mention': 42,\n",
       " 'concept': 62,\n",
       " 'offers': 59,\n",
       " 'responses': 32,\n",
       " 'challenges': 140,\n",
       " 'our': 1019,\n",
       " 'Brief': 919,\n",
       " '42': 177,\n",
       " 'Having': 25,\n",
       " 'broadly': 48,\n",
       " 'contends': 153,\n",
       " 'Otherwise': 13,\n",
       " 'example': 331,\n",
       " 'give': 215,\n",
       " 'competitive': 22,\n",
       " 'advantage': 34,\n",
       " 'over': 456,\n",
       " '454': 52,\n",
       " '455': 49,\n",
       " 'allegation': 16,\n",
       " 'circumvent': 10,\n",
       " 'claiming': 39,\n",
       " 'aim': 42,\n",
       " 'increase': 54,\n",
       " 'market': 167,\n",
       " 'share': 54,\n",
       " '460.1': 1,\n",
       " 'moreover': 80,\n",
       " 'Sedima': 2,\n",
       " 'P.': 107,\n",
       " 'R.': 263,\n",
       " 'L.': 350,\n",
       " 'Imrex': 2,\n",
       " '473': 60,\n",
       " '479': 68,\n",
       " '497': 28,\n",
       " '1985': 74,\n",
       " 'statement': 264,\n",
       " 'went': 39,\n",
       " 'allege': 24,\n",
       " 'assertion': 65,\n",
       " 'legal': 414,\n",
       " 'very': 226,\n",
       " 'relies': 92,\n",
       " 'reaffirmed': 24,\n",
       " 'wrongful': 42,\n",
       " 'competing': 32,\n",
       " 'bidders': 7,\n",
       " 'county': 48,\n",
       " 'lien': 4,\n",
       " 'auction': 18,\n",
       " 'liens': 5,\n",
       " 'profitable': 3,\n",
       " 'lowest': 8,\n",
       " 'possible': 180,\n",
       " 'bid': 7,\n",
       " 'multiple': 48,\n",
       " 'low': 22,\n",
       " 'bidding': 4,\n",
       " 'percentage': 28,\n",
       " 'penalty': 200,\n",
       " 'bidder': 6,\n",
       " 'require': 330,\n",
       " '0': 4,\n",
       " '%': 135,\n",
       " 'awarded': 56,\n",
       " 'devised': 11,\n",
       " 'plan': 163,\n",
       " 'allocate': 3,\n",
       " 'rotational': 2,\n",
       " 'basis': 381,\n",
       " '3': 747,\n",
       " 'noted': 256,\n",
       " 'created': 157,\n",
       " 'perverse': 9,\n",
       " 'Bidders': 1,\n",
       " 'addition': 121,\n",
       " 'themselves': 141,\n",
       " 'sen[t': 1,\n",
       " 'agents': 19,\n",
       " 'behalf': 110,\n",
       " 'obtain': 106,\n",
       " 'disproportionate': 29,\n",
       " 'prohibited': 108,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.word_counts(by=\"orth_\", filter_stops=False, filter_punct=False, filter_nums=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed263112",
   "metadata": {
    "id": "ELd2rxUZVyUU"
   },
   "outputs": [],
   "source": [
    "def show_doc_counts(input_corpus, weighting, limit=20):\n",
    "    doc_counts = input_corpus.word_doc_counts(weighting=weighting, filter_stops=True, by=\"orth_\")\n",
    "    print(\"\\n\".join(f\"{a:15} {b}\" for a, b in sorted(doc_counts.items(), key=lambda x:x[1], reverse=True)[:limit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1e418",
   "metadata": {
    "id": "N5M6Nyy0079J"
   },
   "source": [
    "`word_doc_counts` provides a few ways of quantifying the prevalence of individual words across the corpus: whether a word appears many times in most documents, just a few times in a few documents, many times in a few documents, or just a few times in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fb22b0a",
   "metadata": {
    "id": "wIIkImS2VyUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DOCS APPEARING IN / TOTAL # DOCS\n",
      "-----------\n",
      "Court           0.9873417721518988\n",
      "case            0.9873417721518988\n",
      "certiorari      0.9873417721518988\n",
      "granted         0.9873417721518988\n",
      "U.              0.9746835443037974\n",
      "S.              0.9746835443037974\n",
      "judgment        0.9746835443037974\n",
      "v.              0.9746835443037974\n",
      "decision        0.9746835443037974\n",
      "court           0.9746835443037974\n",
      "C.              0.9620253164556962\n",
      "held            0.9620253164556962\n",
      "opinion         0.9620253164556962\n",
      "2009            0.9620253164556962\n",
      "1               0.9620253164556962\n",
      "F.              0.9493670886075949\n",
      "Justice         0.9493670886075949\n",
      "Id.             0.9493670886075949\n",
      "e.g.            0.9493670886075949\n",
      "issue           0.9493670886075949\n",
      "\n",
      "LOG(TOTAL # DOCS / # DOCS APPEARING IN)\n",
      "-----------\n",
      "cigarettes      4.382026634673881\n",
      "Hemi            4.382026634673881\n",
      "cigarette       4.382026634673881\n",
      "RICO            4.382026634673881\n",
      "racketeering    4.382026634673881\n",
      "activit[ies     4.382026634673881\n",
      "1964(c          4.382026634673881\n",
      "Proximate       4.382026634673881\n",
      "indirec[t       4.382026634673881\n",
      "Anza            4.382026634673881\n",
      "Ideal           4.382026634673881\n",
      "disconnect      4.382026634673881\n",
      "sharper         4.382026634673881\n",
      "Phoenix         4.382026634673881\n",
      "account[ed      4.382026634673881\n",
      "HEMI            4.382026634673881\n",
      "GROUP           4.382026634673881\n",
      "KAI             4.382026634673881\n",
      "GACHUPIN        4.382026634673881\n",
      "YORK            4.382026634673881\n"
     ]
    }
   ],
   "source": [
    "print(\"# DOCS APPEARING IN / TOTAL # DOCS\", \"\\n\", \"-----------\", sep=\"\")\n",
    "show_doc_counts(corpus, \"freq\")\n",
    "print(\"\\n\", \"LOG(TOTAL # DOCS / # DOCS APPEARING IN)\", \"\\n\", \"-----------\", sep=\"\")\n",
    "show_doc_counts(corpus, \"idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b6a36",
   "metadata": {
    "id": "Wx69GIqUdqtx"
   },
   "source": [
    "textacy provides implementations of algorithms for identifying words and phrases that are representative of a document (aka **keyterm extraction**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac4b6864",
   "metadata": {
    "id": "aQCRD7WTeI1j"
   },
   "outputs": [],
   "source": [
    "from textacy.extract import keyterms as ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8183d2e4",
   "metadata": {
    "id": "FEfkQ1IWx_HH"
   },
   "outputs": [],
   "source": [
    "# corpus[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db31a0f5",
   "metadata": {
    "id": "-HWvJ8ube4Vc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York City', 0.002288298045327596),\n",
       " ('New York State', 0.0060401030525529436),\n",
       " ('U. S. C.', 0.0075325125188752005),\n",
       " ('Jenkins Act', 0.012460549374297763),\n",
       " ('York City customer', 0.021988206972901634),\n",
       " ('RICO', 0.027329026591127712),\n",
       " ('Hemi Group', 0.03384924285412936),\n",
       " ('York City cigarette', 0.03513697406836725),\n",
       " ('Jenkins Act information', 0.03892293549952245),\n",
       " ('RICO claim', 0.042756731344701926)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Yake algorithim (Campos et al., 2018) on a given document\n",
    "key_terms_yake = ke.yake(corpus[0])\n",
    "key_terms_yake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65da17",
   "metadata": {
    "id": "nuk7s1yZlvVi"
   },
   "source": [
    "## Keyword in context\n",
    "\n",
    "Sometimes researchers find it helpful just to see a particular keyword in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "316ff9a8",
   "metadata": {
    "id": "FxgjXFnRla1-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK \n",
      " -------------------------------------------------------------- \n",
      "\n",
      "ed the claims, but the Second Circuit vacated the  judgment  and remanded. Among other things, the Court of Ap\n",
      "the City had stated a valid RICO claim. Held: The  judgment  is reversed, and the case is remanded. 541 F. 3d \n",
      " opinion concurring in part and concurring in the  judgment . Breyer, J., filed a dissenting opinion, in which\n",
      "  The Second Circuit vacated the District Court's  judgment  and remanded for further proceedings. The Court o\n",
      " it. The City, therefore, has no RICO claim.  The  judgment  of the Court of Appeals for the Second Circuit is\n",
      "insburg, concurring in part and concurring in the  judgment .  As the Court points out, this is a case \"about \n",
      "he above-stated view, and I concur in the Court's  judgment . HEMI GROUP, LLC and KAI GACHUPIN, PETITIONERS v.\n",
      "\n",
      " CITIZENS UNITED v. FEDERAL ELECTION COMMISSION \n",
      " ---------------------------------------------- \n",
      "\n",
      "ppellee Federal Election Commission (FEC) summary  judgment . Held:  1. Because the question whether §441b app\n",
      "(Scalia, J., concurring in part and concurring in  judgment ). We agree with that conclusion and hold that sta\n",
      "t later convened to hear the cause. The resulting  judgment  gives rise to this appeal.  Citizens United has a\n",
      "m), and then granted the FEC's motion for summary  judgment , App. 261a-262a. See id., at 261a (\"Based on the \n",
      "or opinion, we find that the [FEC] is entitled to  judgment  as a matter of law. See Citizen[s] United v. FEC,\n",
      "onnell, supra, at 339 (Kennedy, J., concurring in  judgment  in part and dissenting in part). The Snowe-Jeffor\n",
      "rt's later opinion, which granted the FEC summary  judgment , was \"[b]ased on the reasoning of [its] prior opi\n",
      "62 (Scalia, J., concurring in part, concurring in  judgment  in part, and dissenting in part); id., at 273-275\n",
      "part, concurring in result in part, concurring in  judgment  in part, and dissenting in part); id., at 322-338\n",
      "pore over each word of a text to see if, in their  judgment , it accords with the 11-factor test they have pro\n",
      "er, 502 U. S., at 124 (Kennedy, J., concurring in  judgment ), the quoted language from WRTL provides a suffic\n",
      "toral opportunities means making and implementing  judgment s about which strengths should be permitted to con\n",
      "issenting); id., at 773 (White, J., concurring in  judgment ). With the advent of the Internet and the decline\n",
      "S. 334, 360-361 (1995) (Thomas, J., concurring in  judgment ). Yet television networks and major newspapers ow\n",
      "t 341-343; id., at 367 (Thomas, J., concurring in  judgment ). At the founding, speech was open, comprehensive\n",
      "endent expenditures; if they surrender their best  judgment ; and if they put expediency before principle, the\n",
      "ation's course; still others simply might suspend  judgment  on these points but decide to think more about is\n",
      "nell, supra, at 341 (opinion of Kennedy, J.). The  judgment  of the District Court is reversed with respect to\n",
      "ctions on corporate independent expenditures. The  judgment  is affirmed with respect to BCRA's disclaimer and\n",
      "Thomas, JJ., concurring in part and concurring in  judgment ); McConnell, 540 U. S., at 247, 264, 286 (opinion\n",
      "86 (Thomas, J., concurring in part, concurring in  judgment  in part, and dissenting in part).  These readings\n",
      "3) (Scalia, J., concurring in part, concurring in  judgment  in part, and dissenting in part) (quoting C. Cook\n",
      " U. S. 334, 360 (1995) (Thomas, J., concurring in  judgment ); see also McConnell, 540 U. S., at 252-253 (opin\n",
      " an affirmative answer to that question is, in my  judgment , profoundly misguided. Even more misguided is the\n",
      " Comm. (NRWC), and have accepted the \"legislative  judgment  that the special characteristics of the corporate\n",
      " of §203. App. 23a-24a. In its motion for summary  judgment , however, Citizens United expressly abandoned its\n",
      "Roberts, J., concurring in part and concurring in  judgment ).  Consider just three of the narrower grounds of\n",
      "s of longstanding practice and Congress' reasoned  judgment  that certain regulations which leave \"untouched f\n",
      "precedents \"represent respect for the legislative  judgment  that the special characteristics of the corporate\n",
      "oach taken by the majority cannot be right, in my  judgment . It disregards our constitutional history and the\n",
      "erting an \" 'undue influence on an officeholder's  judgment ' \" and from creating \" 'the appearance of such in\n",
      "orations).  When the McConnell Court affirmed the  judgment  of the District Court regarding §203, we did not \n",
      "ll, 540 U. S., at 306 (Kennedy, J., concurring in  judgment  in part and dissenting in part); see also id., at\n",
      "63 (Scalia, J., concurring in part, concurring in  judgment  in part, and dissenting in part), a disreputable \n",
      "nted where, as here, we deal with a congressional  judgment  that has remained essentially unchanged throughou\n",
      "tisfy heightened judicial scrutiny of legislative  judgment s will vary up or down with the novelty and plausi\n",
      "years of bipartisan deliberation and its reasoned  judgment  on this basis, without first confirming that the \n",
      " J., dissenting). \"In the meantime, a legislative  judgment  that 'enough is enough' should command the greate\n",
      "Congress' factual findings and its constitutional  judgment : It acknowledges the validity of the interest in \n",
      "O, 335 U. S., at 144 (Rutledge, J., concurring in  judgment )), and this, in turn, \"interferes with the 'open \n",
      "he expansive protections afforded by the business  judgment  rule. Blair & Stout 320; see also id., at 298-315\n",
      "relevance of established facts and the considered  judgment s of state and federal legislatures over many deca\n",
      " corporate money in politics.  I would affirm the  judgment  of the District Court. CITIZENS UNITED, APPELLANT\n",
      "3) (Thomas, J., concurring in part, concurring in  judgment  in part, and dissenting in part) (internal quotat\n",
      "64 (Thomas, J., concurring in part, concurring in  judgment  in part, and dissenting in part) (quoting Nixon v\n",
      "ordingly, I respectfully dissent from the Court's  judgment  upholding BCRA §§201 and 311. FOOTNOTESFootnote 1\n",
      "al Election Commission's (FEC) motion for summary  judgment , App. 261a-262a, any question about statutory val\n",
      "done \"on the basis of entirely subjective, ad hoc  judgment s,\" 523 U. S., at 690, that suggested anticompetit\n",
      "539 U. S., at 163-164 (Kennedy, J., concurring in  judgment ). Both Courts also heard criticisms of Austin fro\n",
      "concurring in part and dissenting in part). In my  judgment , such limitations may be justified to the extent \n",
      "\"We should defer to [the legislature's] political  judgment  that unlimited spending threatens the integrity o\n",
      "\n",
      " HOLLY WOOD, PETITIONER v. RICHARD F. ALLEN, COMMISSIONER, ALABAMA DEPARTMENT OF CORRECTIONS, et al. \n",
      " --------------------------------------------------------------------------------------------------- \n",
      "\n",
      "de a strategic decision, but to whether counsel's  judgment  was reasonable, a question not before this Court.\n",
      "onship to §2254(e)(1). Accordingly, we affirm the  judgment  of the Court of Appeals on that basis. I  In 1993\n",
      "umption that counsel exercised sound professional  judgment , supported by ample reasons, not to present the i\n",
      "rategic decision, but rather to whether counsel's  judgment  was reasonable — a question we do not reach. See \n",
      "ons were an unreasonable exercise of professional  judgment  and constituted deficient performance under Stric\n",
      " itself was a reasonable exercise of professional  judgment  under Strickland or whether the application of St\n",
      "mination of the facts. Accordingly, we affirm the  judgment  of the Court of Appeals for the Eleventh Circuit.\n",
      "ess. That was a strategic decision based on their  judgment  that the evidence would do more harm than good. B\n",
      "resulted from inattention, not reasoned strategic  judgment \"); Strickland, 466 U. S., at 690-691. Moreover, \"\n",
      " itself was a reasonable exercise of professional  judgment  under Strickland or whether the application of St\n",
      "\n",
      " AGRON KUCANA v. ERIC H. HOLDER, JR., ATTORNEY GENERAL \n",
      " ----------------------------------------------------- \n",
      "\n",
      ".  (1) The amicus defending the Seventh Circuit's  judgment  urges that regulations suffice to trigger §1252(a\n",
      "laces within the no-judicial-review category \"any  judgment  regarding the granting of relief under section 11\n",
      "ed. Alito, J., filed an opinion concurring in the  judgment . AGRON KUCANA, PETITIONER v. ERIC H. HOLDER,Jr., \n",
      "ubparagraph (D),[1] and regardless of whether the  judgment , decision, or action is made in removal proceedin\n",
      "urt shall have jurisdiction to review--  \"(i) any  judgment  regarding the granting of relief under section 11\n",
      "micus curiae, in support of the Seventh Circuit's  judgment . 557 U. S. ___ (2009). Ms. Leiter has ably discha\n",
      "mmigration decisions to motions for relief from a  judgment  under Federal Rule of Civil Procedure 60(b)). Fed\n",
      "Nevertheless, in defense of the Seventh Circuit's  judgment , amicus urges that regulations suffice to trigger\n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below 15, 17 (citing, inter alia, Florida Dept. o\n",
      "laces within the no-judicial-review category \"any  judgment  regarding the granting of relief under section 11\n",
      "  To the clause (i) enumeration of administrative  judgment s that are insulated from judicial review, Congres\n",
      "dicial review. * * *  For the reasons stated, the  judgment  of the United States Court of Appeals for the Sev\n",
      "nuary 20, 2010]  Justice Alito, concurring in the  judgment .  I agree that the Court of Appeals had jurisdict\n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below 41-42.  Amicus' argument is ingenious but u\n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below 19, n. 8 (quoting §1229a(c)(7)(B)). One can\n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below. In every one of those examples, Congress e\n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below 21-23. But §1252(a)(2)(B)(ii) does not say \n",
      "Congress want to exclude review for discretionary  judgment s by the Attorney General that are recited explici\n",
      "y in the statute, but provide judicial review for  judgment s that are just as lawfully discretionary because \n",
      "f for Court-Appointed Amicus Curiae in Support of  Judgment  Below 32-34. The report states that §1252(a)(2)(B\n",
      "\n",
      " MARCUS A. WELLONS v. HILTON HALL, WARDEN \n",
      " ---------------------------------------- \n",
      "\n",
      "rd for an order granting certiorari, vacating the  judgment  below, and remanding the case (GVR) remains as it\n",
      "ave to proceed in forma pauperis are granted. The  judgment  is vacated, and the case is remanded to the Eleve\n",
      "nts Wellons' petition for certiorari, vacates the  judgment  of the Eleventh Circuit, and remands (\"GVRs\") in \n",
      "erse or set the case for argument; otherwise, the  judgment  below must stand. The same is true if (as the Cou\n",
      "rits question. If they erred in that regard their  judgment  should be reversed rather than remanded \"in light\n",
      " we are, to vacate and send back their authorized  judgment s for inconsequential imperfection of opinion — as\n",
      " authority or development that casts doubt on the  judgment  of the court below. What the Court has done — usi\n",
      "t of 1996 (AEDPA) to the \"Georgia Supreme Court's  judgment  as to the substance and effect of the ex parte co\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus[:5]:\n",
    "    print(\"\\n\", doc._.meta.get('case_name'), \"\\n\", \"-\" * len(doc._.meta.get('case_name')), \"\\n\")\n",
    "    for match in textacy.extract.kwic.keyword_in_context(doc.text, \"judgment\"):\n",
    "        print(\" \".join(match).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d04a",
   "metadata": {
    "id": "SuSHX_2OVyUb"
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's continue with corpus-level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from `scikit-learn` to count the prevalence of certain tokens in each document of the corpus and to apply weights to these counts if desired. We could just work directly in `scikit-learn`, but it can be nice for mental overhead to learn one library and be able to do a great deal with it.\n",
    "\n",
    "We'll create a vectorizer, sticking with the normal term frequency defaults but discarding words that appear in fewer than 3 documents or more than 95% of documents. We'll also limit our features to the top 500 words according to document frequency. This means our feature set, or columns, will have a higher degree of representation across the corpus. We could further scale these counts according to document frequency (or inverse document frequency) weights, or normalize the weights so that they add up to 1 for each document row (L1 norm), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06519214",
   "metadata": {
    "id": "7LfE48GbVyUb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<79x500 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 22870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textacy.representations\n",
    "\n",
    "vectorizer = textacy.representations.Vectorizer(min_df=3, max_df=.95, max_n_terms=500)\n",
    "\n",
    "tokenized_corpus = [[token.orth_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=True, filter_punct=True))] for doc in corpus]\n",
    "\n",
    "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd629b",
   "metadata": {
    "id": "P5lQ10twVyUf"
   },
   "source": [
    "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do several things with it just within textacy, though we also can pass it into different algorithms within `scikit-learn` or other libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b8706d3",
   "metadata": {
    "id": "37KLE01TVyUg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$',\n",
       " '2d',\n",
       " '3d',\n",
       " 'A.',\n",
       " 'AEDPA',\n",
       " 'Act',\n",
       " 'Amendment',\n",
       " 'American',\n",
       " 'Ann',\n",
       " 'Ante',\n",
       " 'App',\n",
       " 'Appeals',\n",
       " 'B',\n",
       " 'Board',\n",
       " 'Breyer',\n",
       " 'Brief',\n",
       " 'Cert',\n",
       " 'Cf',\n",
       " 'Circuit',\n",
       " 'Citizens']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at some of the terms\n",
    "vectorizer.terms_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1d0d9",
   "metadata": {
    "id": "8_I6y0JIVyUh"
   },
   "source": [
    "We can see that we are still getting a number of terms which might be filtered out, such as symbols and abbreviations. The most straightforward solutions are to filter the terms against a dictionary during vectorization, which carries the risk of inadvertently filtering words that you'd prefer to keep in the dataset, or curating a custom stopword list, which can be inflexible and time consuming. Otherwise, it is often the case that the corpus analysis tools used with the vectorized texts (e.g., topic modeling or stylistic analysis -- see below) have ways of recognizing and sequestering unwanted terms so that they can be excluded from the results if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660493be",
   "metadata": {},
   "source": [
    "## Exercise - topic modeling\n",
    "\n",
    "1. Read through the below code to quickly look at one example of what we can do with a vectorized corpus. Topic modeling is very popular for semantic exploration of texts, and there are numerous implementations of it. Textacy uses implementations from scikit-learn. Our corpus is rather small for topic modeling, but just to see how it's done here, we'll go ahead. First, though, topic modeling works best when the texts are divided into approximately equal-sized \"chunks.\" A quick word-count of the corpus will show that the decisions are of quite variable lengths, which will skew the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecefc62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13007  HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK\n",
      "72325  CITIZENS UNITED v. FEDERAL ELECTION COMMISSION\n",
      " 8333  HOLLY WOOD, PETITIONER v. RICHARD F. ALLEN, COMMISSIONER, ALABAMA DEPARTMENT OF \n",
      " 9947  AGRON KUCANA v. ERIC H. HOLDER, JR., ATTORNEY GENERAL\n",
      " 5508  MARCUS A. WELLONS v. HILTON HALL, WARDEN\n",
      " 4083  ERIC PRESLEY v. GEORGIA\n",
      " 7609  NRG POWER MARKETING, LLC, et al. v. MAINE PUBLIC UTILITIES COMMISSION et al.\n",
      " 6983  E. K. MCDANIEL, WARDEN, et al. v. TROY BROWN\n",
      "13844  MARYLAND v. MICHAEL BLAINE SHATZER, SR.\n",
      " 8945  THE HERTZ CORPORATION v. MELINDA FRIEND et al.\n",
      "11605  FLORIDA v. KEVIN DEWAYNE POWELL\n",
      " 2343  RICK THALER, DIRECTOR, TEXAS DEPARTMENT OF CRIMINAL JUSTICE, CORRECTIONAL INSTIT\n",
      " 8531  MAC'S SHELL SERVICE, INC., et al. v. SHELL OIL PRODUCTS CO. LLC et al.\n",
      " 8259  REED ELSEVIER, INC., et al., v. IRVIN MUCHNICK et al.\n",
      " 9502  CURTIS DARNELL JOHNSON v. UNITED STATES\n",
      "  281  JAMAL KIYEMBA et al. v. BARACK H. OBAMA, PRESIDENT OF THE UNITED STATES et al.\n",
      "12424  MILAVETZ, GALLOP & MILAVETZ, P. A., et al. v. UNITED STATES\n",
      "13983  TAYLOR JAMES BLOATE v. UNITED STATES\n",
      "31066  SHADY GROVE ORTHOPEDIC ASSOCIATES, P. A. v. ALLSTATE INSURANCE COMPANY\n",
      "15135  JOSE PADILLA v. KENTUCKY\n",
      " 8236  JERRY N. JONES, et al. v. HARRIS ASSOCIATES L. P.\n",
      " 8634  MARY BERGHUIS, WARDEN v. DIAPOLIS SMITH\n",
      "15018  GRAHAM COUNTY SOIL AND WATER CONSERVATION DISTRICT, et al. v. UNITED STATES ex r\n",
      " 8025  UNITED STUDENT AID FUNDS, INC. v. FRANCISCO J. ESPINOSA\n",
      " 5499  ESTHER HUI, et al. v. YANIRA CASTANEDA, AS PERSONAL REPRESENTATIVE OF THE ESTATE\n",
      "14398  PAUL RENICO, WARDEN v. REGINALD LETT\n",
      "27245  KEN L. SALAZAR, SECRETARY OF THE INTERIOR, et al. v. FRANK BUONO\n",
      "16003  STOLT-NIELSEN S. A., et al. v. ANIMALFEEDS INTERNATIONAL CORP.\n",
      "12015  MERCK & CO., INC., et al. v. RICHARD REYNOLDS et al.\n",
      "25675  KAREN L. JERMAN v. CARLISLE, MCNELLIE, RINI, KRAMER & ULRICH LPA, et al.\n",
      "12618  SONNY PERDUE, GOVERNOR OF GEORGIA, et al. v. KENNY A., BY HIS NEXT FRIEND LINDA \n",
      "19636  UNITED STATES v. ROBERT J. STEVENS\n",
      "19849  TIMOTHY MARK CAMERON ABBOTT v. JACQUELYN VAYE ABBOTT\n",
      "31417  TERRANCE JAMAR GRAHAM v. FLORIDA\n",
      "23340  UNITED STATES v. GRAYDON EARL COMSTOCK, JR., et al.\n",
      "   20  JOE HARRIS SULLIVAN v. FLORIDA\n",
      " 8980  AMERICAN NEEDLE, INC. v. NATIONAL FOOTBALL LEAGUE et al.\n",
      " 5179  ARTHUR L. LEWIS, JR., et al. v. CITY OF CHICAGO, ILLINOIS\n",
      " 9721  UNITED STATES v. MARTIN O'BRIEN AND ARTHUR BURGESS\n",
      " 6672  BRIDGET HARDT v. RELIANCE STANDARD LIFE INSURANCE COMPANY\n",
      " 5771  UNITED STATES v. GLENN MARCUS\n",
      " 4438  JOHN ROBERTSON v. UNITED STATES ex rel. WYKENNA WATSON\n",
      " 8687  LAWRENCE JOSEPH JEFFERSON v. STEPHEN UPTON, WARDEN\n",
      "10645  MOHAMED ALI SAMANTAR v. BASHE ABDI YOUSUF et al.\n",
      "17847  MARY BERGHUIS, WARDEN v. VAN CHESTER THOMPKINS\n",
      "10183  RICHARD A. LEVIN, TAX COMMISSIONER OF OHIO v. COMMERCE ENERGY, INC., et al.\n",
      "15010  THOMAS CARR v. UNITED STATES\n",
      "12933  MICHAEL GARY BARBER, et al. v. J. E. THOMAS, WARDEN\n",
      "13676  JAN HAMILTON, CHAPTER 13 TRUSTEE v. STEPHANIE KAY LANNING\n",
      " 8312  WANDA KRUPSKI v. COSTA CROCIERE S. P. A.\n",
      " 9987  JOSE ANGEL CARACHURI-ROSENDO v. ERIC H. HOLDER, JR., ATTORNEY GENERAL\n",
      " 8029  MICHAEL J. ASTRUE, COMMISSIONER OF SOCIAL SECURITY v. CATHERINE G. RATLIFF\n",
      "10821  BRIAN RUSSELL DOLAN v. UNITED STATES\n",
      "17847  ALBERT HOLLAND v. FLORIDA\n",
      "11172  NEW PROCESS STEEL, L. P. v. NATIONAL LABOR RELATIONS BOARD\n",
      "18022  STOP THE BEACH RENOURISHMENT, INC. v. FLORIDA DEPARTMENT OF ENVIRONMENTAL PROTEC\n",
      " 9320  CITY OF ONTARIO, CALIFORNIA, et al. v. JEFF QUON et al.\n",
      "18128  WILLIAM G. SCHWAB v. NADEJDA REILLY\n",
      "13554  PERCY DILLON v. UNITED STATES\n",
      "25016  ERIC H. HOLDER, JR., ATTORNEY GENERAL, et al. v. HUMANITARIAN LAW PROJECT et al.\n",
      "10471  RENT-A-CENTER, WEST, INC. v. ANTONIO JACKSON\n",
      "20738  KAWASAKI KISEN KAISHA LTD. et al. v. REGAL-BELOIT CORP. et al.\n",
      "18474  MONSANTO COMPANY, et al. v. GEERTSON SEED FARMS et al.\n",
      "23994  JOHN DOE #1, et al. v. SAM REED, WASHINGTON SECRETARY OF STATE, et al.\n",
      "16696  ROBERT MORRISON, et al. v. NATIONAL AUSTRALIA BANK LTD. et al.\n",
      "14013  GRANITE ROCK COMPANY v. INTERNATIONAL BROTHERHOOD OF TEAMSTERS et al.\n",
      "15751  BILLY JOE MAGWOOD v. TONY PATTERSON, WARDEN, et al.\n",
      "48969  JEFFREY K. SKILLING v. UNITED STATES\n",
      " 4629  CONRAD M. BLACK, JOHN A. BOULTBEE, AND MARK S. KIPNIS v. UNITED STATES\n",
      "39727  FREE ENTERPRISE FUND AND BECKSTEAD AND WATTS, LLP v. PUBLIC COMPANY ACCOUNTING O\n",
      "29263  BERNARD L. BILSKI AND RAND A. WARSAW v. DAVID J. KAPPOS, UNDER SECRETARY OF COMM\n",
      "33725  CHRISTIAN LEGAL SOCIETY CHAPTER OF THE UNIVERSITY OF CALIFORNIA, HASTINGS COLLEG\n",
      "87587  OTIS MCDONALD, et al. v. CITY OF CHICAGO, ILLINOIS, et al.\n",
      " 8126  DEMARCUS ALI SEARS v. STEPHEN UPTON, WARDEN\n",
      " 2242  BILL K. WILSON, SUPERINTENDENT, INDIANA STATE PRISON, PETITIONER v. JOSEPH E. CO\n",
      " 8986  KEVIN ABBOTT, PETITIONER v. UNITED STATES\n",
      " 4244  LOS ANGELES COUNTY, CALIFORNIA, PETITIONER v. CRAIG ARTHUR HUMPHRIES et al.\n",
      "   29  COSTCO WHOLESALE CORPORATION, PETITIONER v. OMEGA, S.A.\n",
      "10250  KEITH SMITH, WARDEN v. FRANK G. SPISAK, JR.\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(f\"{len(doc): >5}  {doc._.meta['case_name'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3bd91e",
   "metadata": {},
   "source": [
    "We'll re-chunk the texts into documents of not more than 500 words and then recompute the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b64b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1006x500 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 91636 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_corpus_unflattened = [\n",
    "    [text[x:x+500] for x in range(0, len(text), 500)] for text in tokenized_corpus\n",
    "]\n",
    "chunked_corpus = list(itertools.chain.from_iterable(chunked_corpus_unflattened))\n",
    "chunked_dtm = vectorizer.fit_transform(chunked_corpus)\n",
    "chunked_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e96a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.tm\n",
    "\n",
    "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
    "model.fit(chunked_dtm)\n",
    "doc_topic_matrix = model.transform(chunked_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d630a319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  8% | right, rights, Clause, States, Justice, state, Amendment, bear, Constitution, law\n",
      " 1  4% | fees, carrier, party, attorney, fee, award, services, filed, $, Rule\n",
      " 2  4% | rights, child, right, Convention, State, custody, Ann, States, Stat, A.\n",
      " 3  4% | debtor, income, value, felony, delay, time, claimed, Code, property, exempt\n",
      " 4  8% | sentence, sentencing, time, life, habeas, year, State, federal, years, application\n",
      " 5  5% | counsel, attorney, Miranda, state, suspect, interrogation, police, right, evidence, advice\n",
      " 6  1% | business, Director, Office, General, place, corporation, patent, Fed, method, State\n",
      " 7  4% | Footnote, arbitration, agreement, parties, contract, dispute, clause, Inc., question, Brief\n",
      " 8  6% | process, petition, disclosure, plaintiffs, challenge, test, applied, referendum, claim, State\n",
      " 9 11% | Amendment, speech, Hastings, J., public, political, interest, policy, Government, corporations\n",
      "10  9% | jury, trial, Id., jurors, d., evidence, District, App, judge, reasonable\n",
      "11 11% | Congress, Board, States, United, statute, power, Act, Commission, authority, foreign\n",
      "12  8% | F., United, States, 3d, statute, Congress, law, error, criminal, conduct\n",
      "13 12% | state, federal, law, courts, Act, action, class, Rule, claims, City\n",
      "14  3% | cross, debt, District, injunction, relief, land, transfer, Government, bankruptcy, agency\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
    "  print(f\"{topic_idx: >2} {model.topic_weights(doc_topic_matrix)[topic_idx]: >3.0%}\", \"|\", \", \".join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb71eb",
   "metadata": {},
   "source": [
    "## Document similarity with word2vec and clustering\n",
    "\n",
    "spaCy and textacy provide several built-in methods for measuring the degree of similarity between two documents, including a `word2vec`-based approach that computes the semantic similarity between documents based on the word vector model included with the spaCy language model. This technique is capable of inferring, for example, that two documents are topically related even if they don't share any words but use synonyms for a shared concept.\n",
    "\n",
    "To evaluate this similarity comparison, we'll compute the similarity of each pair of docs in the corpus, and then branch out into `scikit-learn` a bit to look for clusters based on these similarity measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fa494cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.00428384, 0.00913359, ..., 0.0036863 , 0.05862846,\n",
       "        0.00781636],\n",
       "       [0.00428384, 0.        , 0.00910768, ..., 0.00630599, 0.05399509,\n",
       "        0.0058778 ],\n",
       "       [0.00913359, 0.00910768, 0.        , ..., 0.00614726, 0.04253501,\n",
       "        0.00566709],\n",
       "       ...,\n",
       "       [0.0036863 , 0.00630599, 0.00614726, ..., 0.        , 0.05483739,\n",
       "        0.00724887],\n",
       "       [0.05862846, 0.05399509, 0.04253501, ..., 0.05483739, 0.        ,\n",
       "        0.04504147],\n",
       "       [0.00781636, 0.0058778 , 0.00566709, ..., 0.00724887, 0.04504147,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dim = corpus.n_docs\n",
    "\n",
    "distance_matrix = np.zeros((dim,dim))\n",
    "    \n",
    "for i, doc_i in enumerate(corpus):\n",
    "    for j, doc_j in enumerate(corpus):\n",
    "        if i == j:\n",
    "            continue # defaults to 0\n",
    "        if i > j:\n",
    "            distance_matrix[i,j] = distance_matrix[j,i]\n",
    "        else:\n",
    "            distance_matrix[i,j] = 1 - doc_i.similarity(doc_j)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7851d4b",
   "metadata": {},
   "source": [
    "The [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) hierarchical density-based clustering algorithm only finds one cluster with its default settings, but an examination of the legal issue types coded to each decision indicates that the `word2vec`-based clustering has indeed produced a group of semantically related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "660b498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1\n",
      " -1  1 -1 -1 -1  0 -1 -1 -1 -1  0 -1 -1 -1 -1 -1  1 -1 -1  0  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1  0  0 -1 -1  1 -1 -1 -1  0\n",
      "  0  1 -1 -1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "clustering = OPTICS(metric='precomputed').fit(distance_matrix)\n",
    "print(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1eb6d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 \n",
      "---------\n",
      "558 U.S. 310 | First Amendment    | campaign spending (cf. governmental corruption):\n",
      "559 U.S. 393 | Judicial Power     | Federal Rules of Civil Procedure including Supreme Court Rul\n",
      "559 U.S. 335 | Economic Activity  | federal or state regulation of securities\n",
      "559 U.S. 573 | Civil Rights       | debtors' rights\n",
      "560 U.S. 126 | Federalism         | national supremacy: miscellaneous\n",
      "560 U.S. 305 | Economic Activity  | liability, other than as in sufficiency of evidence, electio\n",
      "561 U.S. 1   | First Amendment    | federal or state internal security legislation: Smith, Inter\n",
      "561 U.S. 186 | Privacy            | Freedom of Information Act and related federal or state stat\n",
      "561 U.S. 247 | Economic Activity  | federal or state regulation of securities\n",
      "561 U.S. 661 | First Amendment    | free exercise of religion\n",
      "561 U.S. 742 | Criminal Procedure | miscellaneous criminal procedure (cf. due process, prisoners\n",
      "\n",
      "\n",
      "\n",
      "Cluster 1 \n",
      "---------\n",
      "558 U.S. 220 | Criminal Procedure | discovery and inspection (in the context of criminal litigat\n",
      "559 U.S. 98  | Criminal Procedure | Miranda warnings\n",
      "559 U.S. 766 | Criminal Procedure | habeas corpus\n",
      "560 U.S. 258 | Criminal Procedure | Federal Rules of Criminal Procedure\n",
      "560 U.S. 370 | Criminal Procedure | Miranda warnings\n",
      "561 U.S. 358 | Criminal Procedure | statutory construction of criminal laws: fraud\n",
      "561 U.S. 945 | Criminal Procedure | right to counsel (cf. indigents appointment of counsel or in\n",
      "558 U.S. 139 | Criminal Procedure | cruel and unusual punishment, death penalty (cf. extra legal\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "clusters = groupby(sorted(enumerate(clustering.labels_), key=lambda x: x[1]), lambda x: x[1])\n",
    "\n",
    "for cluster_label, docs in clusters:\n",
    "    \n",
    "    if cluster_label == -1:\n",
    "        continue\n",
    "\n",
    "    print(f\"Cluster {cluster_label}\", \"\\n---------\")\n",
    "    print(\"\\n\".join(\n",
    "        f\"{corpus[i]._.meta['us_cite_id']: <12} | {data.issue_area_codes[corpus[i]._.meta['issue_area']]: <18}\"\n",
    "        f\" | {data.issue_codes[corpus[i]._.meta['issue']][:60]}\"\n",
    "        for i, _ in docs\n",
    "    ))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "059398fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assyrian',\n",
       " 'monarchs',\n",
       " 'especially',\n",
       " 'sardanapalus',\n",
       " 'babylon',\n",
       " 'scene',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'assyrian',\n",
       " 'babylon',\n",
       " 'ized',\n",
       " 'library',\n",
       " 'library',\n",
       " 'paper',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'writing',\n",
       " 'mesopotamia',\n",
       " 'early',\n",
       " 'sumerian',\n",
       " 'days',\n",
       " 'collection',\n",
       " 'unearthed',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'world',\n",
       " 'chaldean',\n",
       " 'line',\n",
       " 'babylonian',\n",
       " 'monarchs',\n",
       " 'nabonidus',\n",
       " 'keener',\n",
       " 'literary',\n",
       " 'tastes',\n",
       " 'patronized',\n",
       " 'antiquarian',\n",
       " 'researches',\n",
       " 'date',\n",
       " 'worked',\n",
       " 'investigators',\n",
       " 'accession',\n",
       " 'sargon',\n",
       " 'commemorated',\n",
       " 'fact',\n",
       " 'inscriptions',\n",
       " 'signs',\n",
       " 'disunion',\n",
       " 'empire',\n",
       " 'sought',\n",
       " 'centralize',\n",
       " 'bringing',\n",
       " 'number',\n",
       " 'local',\n",
       " 'gods',\n",
       " 'babylon',\n",
       " 'setting',\n",
       " 'temples',\n",
       " 'device',\n",
       " 'practised',\n",
       " 'successfully',\n",
       " 'romans',\n",
       " 'later',\n",
       " 'times',\n",
       " 'babylon',\n",
       " 'roused',\n",
       " 'jealousy',\n",
       " 'powerful',\n",
       " 'priesthood',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'dominant',\n",
       " 'god',\n",
       " 'babylonians',\n",
       " 'cast',\n",
       " 'possible',\n",
       " 'alternative',\n",
       " 'nabonidus',\n",
       " 'found',\n",
       " 'cyrus',\n",
       " 'persian',\n",
       " 'ruler',\n",
       " 'adjacent',\n",
       " 'median',\n",
       " 'empire',\n",
       " 'cyrus',\n",
       " 'distinguished',\n",
       " 'conquering',\n",
       " 'croesus',\n",
       " 'rich',\n",
       " 'king',\n",
       " 'lydia',\n",
       " 'eastern',\n",
       " 'asia',\n",
       " 'minor',\n",
       " 'came',\n",
       " 'babylon',\n",
       " 'battle',\n",
       " 'outside',\n",
       " 'walls',\n",
       " 'gates',\n",
       " 'city',\n",
       " 'opened',\n",
       " 'soldiers',\n",
       " 'entered',\n",
       " 'city',\n",
       " 'fighting',\n",
       " 'crown',\n",
       " 'prince',\n",
       " 'belshazzar',\n",
       " 'son',\n",
       " 'nabonidus',\n",
       " 'feasting',\n",
       " 'bible',\n",
       " 'relates',\n",
       " 'hand',\n",
       " 'appeared',\n",
       " 'wrote',\n",
       " 'letters',\n",
       " 'fire',\n",
       " 'wall',\n",
       " 'mystical',\n",
       " 'words',\n",
       " 'mene',\n",
       " 'mene',\n",
       " 'tekel',\n",
       " 'upharsin',\n",
       " 'interpreted',\n",
       " 'prophet',\n",
       " 'daniel',\n",
       " 'summoned',\n",
       " 'read',\n",
       " 'riddle',\n",
       " 'god',\n",
       " 'numbered',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'finished',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'weighed',\n",
       " 'balance',\n",
       " 'found',\n",
       " 'wanting',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'given',\n",
       " 'medes',\n",
       " 'persians',\n",
       " 'possibly',\n",
       " 'priests',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'knew',\n",
       " 'writing',\n",
       " 'wall',\n",
       " 'belshazzar',\n",
       " 'killed',\n",
       " 'night',\n",
       " 'says',\n",
       " 'bible',\n",
       " 'nabonidus',\n",
       " 'taken',\n",
       " 'prisoner',\n",
       " 'occupation',\n",
       " 'city',\n",
       " 'peaceful',\n",
       " 'services',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'continued',\n",
       " 'intermission']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ce45a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Filter the tokens from the HG Well's `text` variable to 1) lowercase all text, 2) remove punctuation, 3) remove spaces and line breaks, 4) remove numbers, and 5) remove stopwords - all in one line! \n",
    "2. Read through the spacy101 guide and begin to apply its principles to your own corpus: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67118c",
   "metadata": {},
   "source": [
    "## Topic modeling - going further\n",
    "\n",
    "There are many different approaches to modeling abstract topics in text data, such as [top2vec](https://github.com/ddangelov/Top2Vec) and [lda2vec](https://github.com/cemoody/lda2vec). \n",
    "\n",
    "Click ahead to see our coverage of the BERTopic algorithm in Chapter 10! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}