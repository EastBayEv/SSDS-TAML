{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - English text preprocessing basics - and applications\n",
    "2023 April 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/EastBayEv/SSDS-TAML/blob/main/spring2023/7_English_text_preprocessing_basics_applications.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](img/text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured text - text you find in the wild in books and websites - is generally not amenable to analysis. Before it can be analyzed, the text needs to be standardized to a format so that Python can recognize each unit of meaning **(called a \"token\")** as unique, no matter how many times it occurs and how it is stylized. \n",
    "\n",
    "Although not an exhaustive list, some key steps in preprocessing text include:  \n",
    "* Standardize text casing and spacing \n",
    "* Remove punctuation and special characters/symbols\n",
    "* Remove stop words\n",
    "* Stem or lemmatize: convert all non-base words to their base form \n",
    "\n",
    "Stemming/lemmatization and stop words (and some punctuation) are language-specific. The Natural Language ToolKit (NLTK) works for English out-of-the-box, but you'll need different code to work with other languages. Some languages (e.g. Chinese) also require *segmentation*: artificially inserting spaces between words. If you want to do text pre-processing for other languages, please let us know and we can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have the proper nltk modules\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 12:37:15.499891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus definition: United Nations Human Rights Council Documentation\n",
    "\n",
    "![unhrc](img/unhrc.jpg)\n",
    "\n",
    "We will select eleven .txt files from the UN HRC as our corpus, stored within the subfolder \"human_rights\" folder inside the main \"data\" directory. \n",
    "\n",
    "These documents contain information about human rights recommendations made by member nations towards countries deemed to be in violation of the HRC. \n",
    "\n",
    "[Learn more about the UN HRC by clicking here.](https://www.ohchr.org/en/hrbodies/hrc/pages/home.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the corpus directory\n",
    "\n",
    "Set the directory's file path and print the files it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/human_rights: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Make the directory \"human_rights\" inside of data\n",
    "!mkdir data\n",
    "!mkdir data/human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your \"data\" folder already exists in Colab and you want to delete it, type:\n",
    "# !rm -r data\n",
    "\n",
    "# If the \"human_rights\" folder already exists in Colab and you want to delete it, type:\n",
    "# !rm -r data/human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download elevent UN HRC files\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/afghanistan2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/bangladesh2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/cotedivoire2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/djibouti2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/fiji2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/jordan2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/kazakhstan2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/monaco2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/sanmarino2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/turkmenistan2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/tuvalu2013.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afghanistan2014.txt  fiji2014.txt         sanmarino2014.txt\r\n",
      "bangladesh2013.txt   jordan2013.txt       turkmenistan2013.txt\r\n",
      "cotedivoire2014.txt  kazakhstan2014.txt   tuvalu2013.txt\r\n",
      "djibouti2013.txt     monaco2013.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Check that we have eleven files, one for each country\n",
    "!ls data/human_rights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sanmarino2014.txt',\n",
       " 'tuvalu2013.txt',\n",
       " 'kazakhstan2014.txt',\n",
       " 'cotedivoire2014.txt',\n",
       " 'fiji2014.txt',\n",
       " 'bangladesh2013.txt',\n",
       " 'turkmenistan2013.txt',\n",
       " 'jordan2013.txt',\n",
       " 'monaco2013.txt',\n",
       " 'afghanistan2014.txt',\n",
       " 'djibouti2013.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "corpus = os.listdir('data/human_rights/')\n",
    "\n",
    "# View the contents of this directory\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store these documents in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in an empty dictionary for conversion to data frame\n",
    "empty_dictionary = {}\n",
    "\n",
    "# Loop through the folder of documents to open and read each one\n",
    "for document in corpus:\n",
    "    with open('data/human_rights/' + document, 'r', encoding = 'utf-8') as to_open:\n",
    "         empty_dictionary[document] = to_open.read()\n",
    "\n",
    "# Populate the data frame with two columns: file name and document text\n",
    "human_rights = (pd.DataFrame.from_dict(empty_dictionary, \n",
    "                                       orient = 'index')\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {'index': 'file_name', 0: 'document_text'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...\n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...\n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...\n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...\n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...\n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...\n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...\n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...\n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...\n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...\n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the text of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A/HRC/28/9 \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr.: General \n",
      "24 December 2014 \n",
      " \n",
      "Original: English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty-eighth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review* \n",
      " * The annex to the present report is circulated as received. \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction .............................................................................................................  1Ð4 3 \n",
      " I. Summary of the proceedings of the review process ................................................  5Ð77 3 \n",
      "  A. Presentation by the State under review ...........................................................  5Ð23 3 \n",
      "  B. Interactive dialogue and responses by the State under review ........................  24Ð77 6 \n",
      " II. Conclusions and/or recommendations .....................................................................  78Ð81 13 \n",
      " Annex \n",
      "  Composition of the delegation .......\n"
     ]
    }
   ],
   "source": [
    "# first thousand characters\n",
    "print(human_rights['document_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English text preprocessing\n",
    "\n",
    "Create a new column named \"clean_text\" to store the text as it is preprocessed. \n",
    "\n",
    "### What are some of the things we can do? \n",
    "\n",
    "These are just a few examples. How else could you improve this process? \n",
    "\n",
    "* Remove non-alphanumeric characters/punctuation\n",
    "* Remove digits\n",
    "* Remove [unicode characters](https://en.wikipedia.org/wiki/List_of_Unicode_characters)\n",
    "* Remove extra spaces\n",
    "* Convert to lowercase\n",
    "* Lemmatize (optional for now)\n",
    "\n",
    "Take a look at the first document after each step to see if you can notice what changed. \n",
    "\n",
    "> Remember: the process will likely be different for many other natural languages, which frequently require special considerations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-alphanumeric characters/punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'clean_text' to store the text we are standardizing\n",
    "human_rights['clean_text'] = human_rights['document_text'].str.replace(r'[^\\w\\s]', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC 28 9 \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "24 December 2014 \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                1Ð4 3 \n",
      " I  Summary of the proceedings of the review process                                                   5Ð77 3 \n",
      "  A  Presentation by the State under review                                                              5Ð23 3 \n",
      "  B  Interactive dialogue and responses by the State under review                           24Ð77 6 \n",
      " II  Conclusions and or recommendations                                                                        78Ð81 13 \n",
      " Annex \n",
      "  Composition of the delegation        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>\\nDistr   General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>\\nDistr   General 6 January 2014 \\nOriginal  E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>\\nDistr   General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>\\nDistr   General 4 April 2014 \\nOriginal  Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>\\n\\nDistr   General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "\n",
       "                                           clean_text  \n",
       "0    \\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...  \n",
       "1    \\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...  \n",
       "2    \\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...  \n",
       "3   \\nDistr   General 7 July 2014 English Original...  \n",
       "4    \\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...  \n",
       "5    \\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...  \n",
       "6    \\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...  \n",
       "7   \\nDistr   General 6 January 2014 \\nOriginal  E...  \n",
       "8   \\nDistr   General 3 January 2014 English Origi...  \n",
       "9   \\nDistr   General 4 April 2014 \\nOriginal  Eng...  \n",
       "10  \\n\\nDistr   General 8 July 2013 English Origin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view third column\n",
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\d', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC      \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "   December      \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                 Ð    \n",
      " I  Summary of the proceedings of the review process                                                    Ð     \n",
      "  A  Presentation by the State under review                                                               Ð     \n",
      "  B  Interactive dialogue and responses by the State under review                             Ð     \n",
      " II  Conclusions and or recommendations                                                                          Ð      \n",
      " Annex \n",
      "  Composition of the delegation        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unicode characters such as Ð and ð"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more on text encodings: https://www.w3.org/International/questions/qa-what-is-encoding\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC      \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "   December      \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                     \n",
      " I  Summary of the proceedings of the review process                                                         \n",
      "  A  Presentation by the State under review                                                                    \n",
      "  B  Interactive dialogue and responses by the State under review                                  \n",
      " II  Conclusions and or recommendations                                                                                \n",
      " Annex \n",
      "  Composition of the delegation             \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\s+', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " United Nations A HRC General Assembly Distr General December Original English Human Rights Council Twenty eighth session Agenda item Universal Periodic Review Report of the Working Group on the Universal Periodic Review The annex to the present report is circulated as received San Marino Contents Paragraphs Page Introduction I Summary of the proceedings of the review process A Presentation by the State under review B Interactive dialogue and responses by the State under review II Conclusions and or recommendations Annex Composition of the delegation Introduction The Working Group on the Universal Periodic Review established in accordance with Human Rights Council resolution of June held its twentieth session from October to November The review of San Marino was held at the th meeting on October The delegation of San Marino was headed by Pasquale Valentini Minister for Foreign Affairs At its th meeting held on October the Working Group adopted the report on San Marino On January the Hu\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " united nations a hrc general assembly distr general december original english human rights council twenty eighth session agenda item universal periodic review report of the working group on the universal periodic review the annex to the present report is circulated as received san marino contents paragraphs page introduction i summary of the proceedings of the review process a presentation by the state under review b interactive dialogue and responses by the state under review ii conclusions and or recommendations annex composition of the delegation introduction the working group on the universal periodic review established in accordance with human rights council resolution of june held its twentieth session from october to november the review of san marino was held at the th meeting on october the delegation of san marino was headed by pasquale valentini minister for foreign affairs at its th meeting held on october the working group adopted the report on san marino on january the hu\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "human_rights['clean_text'] = human_rights['clean_text'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(human_rights['clean_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the updated data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>distr general july english original english ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>distr general january original english gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>distr general january english original engli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>distr general april original english general...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>distr general july english original english ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "\n",
       "                                           clean_text  \n",
       "0     united nations a hrc general assembly distr ...  \n",
       "1     united nations a hrc general assembly distr ...  \n",
       "2     united nations a hrc general assembly distr ...  \n",
       "3     distr general july english original english ...  \n",
       "4     united nations a hrc general assembly distr ...  \n",
       "5     united nations a hrc general assembly distr ...  \n",
       "6     united nations a hrc general assembly distr ...  \n",
       "7     distr general january original english gener...  \n",
       "8     distr general january english original engli...  \n",
       "9     distr general april original english general...  \n",
       "10    distr general july english original english ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - redwoods webscraping\n",
    "\n",
    "This also works with data scraped from the web. Below is very brief BeautifulSoup example to save the contents of the Sequoioideae (redwood trees) Wikipedia page in a variable named `text`. \n",
    "\n",
    "1. Read through the code below\n",
    "2. Practice by repeating for a webpage of your choice\n",
    "\n",
    "![redwood](img/redwood.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import regex as re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three variables will get you started\n",
    "\n",
    "1. `url` - define the URL to be scraped \n",
    "2. `response` - perform the get request on the URL \n",
    "3. `soup` - create the soup object so we can parse the html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Sequoioideae\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the text\n",
    "\n",
    "HTML (hypertext markup language) is used to structure a webpage and the content it contains, including text.\n",
    "\n",
    "Below is a handy for loop that finds all everything within paragraph `<p>`, or paragraph tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in an empty string\n",
    "text = \"\"\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequoioideae, popularly known as redwoods, is a subfamily of coniferous trees within the family Cupressaceae. It includes the largest and tallest trees in the world.\n",
      "The three redwood subfamily genera are Sequoia from coastal California and Oregon, Sequoiadendron from California's Sierra Nevada, and Metasequoia in China. The redwood species contains the largest and tallest trees in the world. These trees can live for thousands of years. Threats include logging, fire suppression,[2] climate change, illegal marijuana cultivation, and burl poaching.[3][4][5]\n",
      "Only two of the genera, Sequoia and Sequoiadendron, are known for massive trees. Trees of Metasequoia, from the single living species Metasequoia glyptostroboides, are deciduous, grow much smaller (although are still large compared to most other trees) and can live in colder climates.[citation needed]\n",
      "Multiple studies of both morphological and molecular characters have strongly supported the assertion that the Sequoioideae are monophyletic.[6][7][8][9]\n",
      "Most modern phylogenies place Sequoia as sister to Sequoiadendron and Metasequoia as the out-group.[7][9][10] However, Yang et al. went on to investigate the origin of a peculiar genetic artifact of the Sequoioideae—the polyploidy of Sequoia—and generated a notable exception that calls into question the specifics of this relative consensus.[9]\n",
      "A 2006 paper based on non-molecular evidence suggested the following relationship among extant species:[11]\n",
      "\n",
      "M. glyptostroboides (dawn redwood)\n",
      "S. sempervirens (coast redwood)\n",
      "S. giganteum (giant sequoia)\n",
      "Taxodioideae\n",
      "A 2021 study using molecular evidence found the same relationships among Sequoioideae species, but found Sequoioideae to be the sister group to the Athrotaxidoideae (a superfamily presently known only from Tasmania) rather than to Taxodioideae. Sequoioideae and Athrotaxidoideae are thought to have diverged from each other during the Jurassic.[12]\n",
      "Reticulate evolution refers to the origination of a taxon through the merging of ancestor lineages.\n",
      "Polyploidy has come to be understood as quite common in plants—with estimates ranging from 47% to 100% of flowering plants and extant ferns having derived from ancient polyploidy.[13] Within the gymnosperms however it is quite rare. Sequoia sempervirens is hexaploid (2n= 6x= 66). To investigate the origins of this polyploidy Yang et al. used two single copy nuclear genes, LFY and NLY, to generate phylogenetic trees. Other researchers have had success with these genes in similar studies on different taxa.[9]\n",
      "Several hypotheses have been proposed to explain the origin of Sequoia's polyploidy: allopolyploidy by hybridization between Metasequoia and some probably extinct taxodiaceous plant; Metasequoia and Sequoiadendron, or ancestors of the two genera, as the parental species of Sequoia; and autohexaploidy, autoallohexaploidy, or segmental allohexaploidy.[citation needed]\n",
      "Yang et al. found that Sequoia was clustered with Metasequoia in the tree generated using the LFY gene but with Sequoiadendron in the tree generated with the NLY gene. Further analysis strongly supported the hypothesis that Sequoia was the result of a hybridization event involving Metasequoia and Sequoiadendron. Thus, Yang et al. hypothesize that the inconsistent relationships among Metasequoia, Sequoia, and Sequoiadendron could be a sign of reticulate evolution by hybrid speciation (in which two species hybridize and give rise to a third) among the three genera. However, the long evolutionary history of the three genera (the earliest fossil remains being from the Jurassic) make resolving the specifics of when and how Sequoia originated once and for all a difficult matter—especially since it in part depends on an incomplete fossil record.[10]\n",
      "Sequoioideae is an ancient taxon, with the oldest described Sequoioideae species, Sequoia jeholensis, recovered from Jurassic deposits.[14] A genus Medulloprotaxodioxylon, reported from the late Triassic of China supports the idea of a Norian origin.[1]\n",
      "\n",
      "The fossil record shows a massive expansion of range in the Cretaceous and dominance of the Arcto-Tertiary Geoflora, especially in northern latitudes. Genera of Sequoioideae were found in the Arctic Circle, Europe, North America, and throughout Asia and Japan.[15] A general cooling trend beginning in the late Eocene and Oligocene reduced the northern ranges of the Sequoioideae, as did subsequent ice ages.[16] Evolutionary adaptations to ancient environments persist in all three species despite changing climate, distribution, and associated flora, especially the specific demands of their reproduction ecology that ultimately forced each of the species into refugial ranges where they could survive.[citation needed]The entire subfamily is endangered. The IUCN Red List Category & Criteria assesses Sequoia sempervirens as Endangered (A2acd), Sequoiadendron giganteum as Endangered (B2ab) and Metasequoia glyptostroboides as Endangered (B1ab).\n",
      "New World Species:\n",
      "New World Species:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "Regular expressions are sequences of characters and symbols that represent search patterns in text - and are generally quite useful. \n",
    "\n",
    "[Check out the tutorial](https://docs.python.org/3/library/re.html) and [cheatsheet](https://www.dataquest.io/blog/regex-cheatsheet/) to find out what the below symbols mean and write your own code. Better yet you could write a pattern to do them simultaneously in one line/less lines of code in some cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'[^\\w\\s]','',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning with `TfidfVectorizer()`\n",
    "\n",
    "Remember `CountVectorizer()` for creating Bag of Word models? We can extend this idea of counting words, to _counting unique words_ within a document relative to the rest of the corpus with `TfidfVectorizer()`. Each row will still be a document in the document term matrix and each column will still be a linguistic feature, but the cells will now be populated by the word uniqueness weights instead of frequencies. Remember that: \n",
    "\n",
    "* For TF-IDF sparse matrices:\n",
    "    * A value closer to 1 indicate that a feature is more relevant to a particular document.\n",
    "    * A value closer to 0 indicates that that feature is less/not relevant to that document.\n",
    "\n",
    "![tf1](img/tf1.png)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "![tf2](img/tf2.png)\n",
    "\n",
    "[towardsdatascience](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range = (1, 3), \n",
    "                                stop_words = 'english', \n",
    "                                max_df = 0.50\n",
    "                                )\n",
    "tf_sparse = tf_vectorizer.fit_transform(human_rights['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 84181)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 39182)\t0.004856846879037771\n",
      "  (0, 31574)\t0.004856846879037771\n",
      "  (0, 50165)\t0.004856846879037771\n",
      "  (0, 79743)\t0.004856846879037771\n",
      "  (0, 46164)\t0.004856846879037771\n",
      "  (0, 70574)\t0.004856846879037771\n",
      "  (0, 67048)\t0.004856846879037771\n",
      "  (0, 48393)\t0.004856846879037771\n",
      "  (0, 55413)\t0.004856846879037771\n",
      "  (0, 5657)\t0.004856846879037771\n",
      "  (0, 2036)\t0.004856846879037771\n",
      "  (0, 18508)\t0.004856846879037771\n",
      "  (0, 4238)\t0.004856846879037771\n",
      "  (0, 49342)\t0.004856846879037771\n",
      "  (0, 2719)\t0.004856846879037771\n",
      "  (0, 39331)\t0.004856846879037771\n",
      "  (0, 7341)\t0.004856846879037771\n",
      "  (0, 80381)\t0.004856846879037771\n",
      "  (0, 49382)\t0.004856846879037771\n",
      "  (0, 2723)\t0.004856846879037771\n",
      "  (0, 43326)\t0.004856846879037771\n",
      "  (0, 20394)\t0.004856846879037771\n",
      "  (0, 27591)\t0.004856846879037771\n",
      "  (0, 53796)\t0.004856846879037771\n",
      "  (0, 74877)\t0.004856846879037771\n",
      "  :\t:\n",
      "  (10, 23861)\t0.004813404453682774\n",
      "  (10, 53868)\t0.005386104036626412\n",
      "  (10, 61809)\t0.005386104036626412\n",
      "  (10, 11732)\t0.006124442213746767\n",
      "  (10, 52233)\t0.006124442213746767\n",
      "  (10, 32036)\t0.00869094964616284\n",
      "  (10, 38685)\t0.00869094964616284\n",
      "  (10, 48447)\t0.012248884427493533\n",
      "  (10, 64367)\t0.00869094964616284\n",
      "  (10, 28795)\t0.004813404453682774\n",
      "  (10, 66492)\t0.03231662421975847\n",
      "  (10, 62562)\t0.005386104036626412\n",
      "  (10, 30131)\t0.004813404453682774\n",
      "  (10, 79340)\t0.006124442213746767\n",
      "  (10, 4251)\t0.00434547482308142\n",
      "  (10, 27107)\t0.00434547482308142\n",
      "  (10, 57250)\t0.026072848938488522\n",
      "  (10, 60838)\t0.030418323761569943\n",
      "  (10, 70491)\t0.026072848938488522\n",
      "  (10, 76162)\t0.00434547482308142\n",
      "  (10, 72779)\t0.005386104036626412\n",
      "  (10, 27613)\t0.00434547482308142\n",
      "  (10, 28707)\t0.010772208073252824\n",
      "  (10, 8151)\t0.010772208073252824\n",
      "  (10, 18289)\t0.00869094964616284\n"
     ]
    }
   ],
   "source": [
    "print(tf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tfidf sparse matrix to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone inclusive</th>\n",
       "      <th>zone inclusive education</th>\n",
       "      <th>zone senegal</th>\n",
       "      <th>zone senegal make</th>\n",
       "      <th>zone social</th>\n",
       "      <th>zone social benefit</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 84181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007608    0.007608            0.007608  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007165     0.007165   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004509   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.011198   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004721   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005111   \n",
       "10           0.007165  0.007165        0.007165           0.007165   0.000000   \n",
       "\n",
       "    ...      zone  zone inclusive  zone inclusive education  zone senegal  \\\n",
       "0   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "1   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "2   ...  0.011473        0.006711                  0.006711      0.000000   \n",
       "3   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "4   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "5   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "6   ...  0.006296        0.000000                  0.000000      0.007365   \n",
       "7   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "8   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "9   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "10  ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "\n",
       "    zone senegal make  zone social  zone social benefit     zouon  zouon bi  \\\n",
       "0            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "1            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "2            0.000000     0.006711             0.006711  0.000000  0.000000   \n",
       "3            0.000000     0.000000             0.000000  0.006249  0.006249   \n",
       "4            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "5            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "6            0.007365     0.000000             0.000000  0.000000  0.000000   \n",
       "7            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "8            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "9            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "10           0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "\n",
       "    zouon bi tidou  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.006249  \n",
       "4         0.000000  \n",
       "5         0.000000  \n",
       "6         0.000000  \n",
       "7         0.000000  \n",
       "8         0.000000  \n",
       "9         0.000000  \n",
       "10        0.000000  \n",
       "\n",
       "[11 rows x 84181 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_sparse.todense(), columns = tf_vectorizer.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 20 highest weighted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "monaco                        0.720348\n",
       "tuvalu                        0.639251\n",
       "kazakhstan                    0.615483\n",
       "fiji                          0.582410\n",
       "turkmenistan                  0.578904\n",
       "san                           0.553681\n",
       "jordan                        0.491254\n",
       "san marino                    0.456544\n",
       "marino                        0.456544\n",
       "divoire                       0.340046\n",
       "te divoire                    0.340046\n",
       "te                            0.306989\n",
       "elimination violence          0.253620\n",
       "elimination violence woman    0.253620\n",
       "djiboutis                     0.250777\n",
       "reconciliation                0.245711\n",
       "fgm                           0.195982\n",
       "afghan                        0.190201\n",
       "bangladeshs                   0.183356\n",
       "violence woman law            0.182593\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.max().sort_values(ascending = False).head(n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country name to `tfidf_df`\n",
    "\n",
    "This way, we will know which document is relative to which country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sanmarino',\n",
       " 'tuvalu',\n",
       " 'kazakhstan',\n",
       " 'cotedivoire',\n",
       " 'fiji',\n",
       " 'bangladesh',\n",
       " 'turkmenistan',\n",
       " 'jordan',\n",
       " 'monaco',\n",
       " 'afghanistan',\n",
       " 'djibouti']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrangle the country names from the human_rights data frame\n",
    "countries = human_rights['file_name'].str.slice(stop = -8)\n",
    "countries = list(countries)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['COUNTRY'] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone inclusive</th>\n",
       "      <th>zone inclusive education</th>\n",
       "      <th>zone senegal</th>\n",
       "      <th>zone senegal make</th>\n",
       "      <th>zone social</th>\n",
       "      <th>zone social benefit</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sanmarino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tuvalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kazakhstan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>cotedivoire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fiji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>turkmenistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>djibouti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 84182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007608    0.007608            0.007608  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007165     0.007165   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004509   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.011198   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004721   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005111   \n",
       "10           0.007165  0.007165        0.007165           0.007165   0.000000   \n",
       "\n",
       "    ...  zone inclusive  zone inclusive education  zone senegal  \\\n",
       "0   ...        0.000000                  0.000000      0.000000   \n",
       "1   ...        0.000000                  0.000000      0.000000   \n",
       "2   ...        0.006711                  0.006711      0.000000   \n",
       "3   ...        0.000000                  0.000000      0.000000   \n",
       "4   ...        0.000000                  0.000000      0.000000   \n",
       "5   ...        0.000000                  0.000000      0.000000   \n",
       "6   ...        0.000000                  0.000000      0.007365   \n",
       "7   ...        0.000000                  0.000000      0.000000   \n",
       "8   ...        0.000000                  0.000000      0.000000   \n",
       "9   ...        0.000000                  0.000000      0.000000   \n",
       "10  ...        0.000000                  0.000000      0.000000   \n",
       "\n",
       "    zone senegal make  zone social  zone social benefit     zouon  zouon bi  \\\n",
       "0            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "1            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "2            0.000000     0.006711             0.006711  0.000000  0.000000   \n",
       "3            0.000000     0.000000             0.000000  0.006249  0.006249   \n",
       "4            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "5            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "6            0.007365     0.000000             0.000000  0.000000  0.000000   \n",
       "7            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "8            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "9            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "10           0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "\n",
       "    zouon bi tidou       COUNTRY  \n",
       "0         0.000000     sanmarino  \n",
       "1         0.000000        tuvalu  \n",
       "2         0.000000    kazakhstan  \n",
       "3         0.006249   cotedivoire  \n",
       "4         0.000000          fiji  \n",
       "5         0.000000    bangladesh  \n",
       "6         0.000000  turkmenistan  \n",
       "7         0.000000        jordan  \n",
       "8         0.000000        monaco  \n",
       "9         0.000000   afghanistan  \n",
       "10        0.000000      djibouti  \n",
       "\n",
       "[11 rows x 84182 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine unique words by each document/country\n",
    "\n",
    "Change the country names to view their highest rated terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jordan                                0.491254\n",
       "jordanian                             0.140540\n",
       "press publication                     0.112432\n",
       "syrian                                0.105405\n",
       "reservation                           0.099133\n",
       "publication law                       0.091351\n",
       "press publication law                 0.091351\n",
       "constitutional amendment              0.089799\n",
       "syrian refugee                        0.084324\n",
       "publication                           0.080973\n",
       "reservation convention                0.078083\n",
       "reservation convention elimination    0.072077\n",
       "website                               0.072077\n",
       "commitment jordan                     0.063243\n",
       "news website                          0.056216\n",
       "news                                  0.054058\n",
       "al                                    0.054058\n",
       "personal status                       0.054058\n",
       "personal                              0.052823\n",
       "host                                  0.046879\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = tfidf_df[tfidf_df['COUNTRY'] == 'jordan']\n",
    "country.max(numeric_only = True).sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UN HRC text analysis - what next?\n",
    "\n",
    "What next? Keep in mind that we have not even begun to consider named entities and parts of speech. What problems immediately jump out from the above examples, such as with the number and uniqueness of country names?\n",
    "\n",
    "The next two chapters 8 and 9 introduce powerful text preprocessing and analysis techniques. Read ahead to see how we can handle roadblocks such as these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis is the contextual mining of text data that elicits abstract information in source materials to determine if data are positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sa](img/sa.jpg)\n",
    "\n",
    "[Repustate](https://www.repustate.com/blog/sentiment-analysis-challenges-with-solutions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the nltk built movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define x (reviews) and y (judgements) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Judgements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Judgements\n",
       "0  plot : two teen couples go to a church party ,...        neg\n",
       "1  the happy bastard's quick movie review \\ndamn ...        neg\n",
       "2  it is movies like these that make a jaded movi...        neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...        neg\n",
       "4  synopsis : a mentally unstable man undergoing ...        neg"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human review was: neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('steve martin is one of the funniest men alive . \\nif you can take that as a true statement , then your disappointment at this film will equal mine . \\nmartin can be hilarious , creating some of the best laugh-out-loud experiences that have ever taken place in movie theaters . \\nyou won\\'t find any of them here . \\nthe old television series that this is based on has its moments of humor and wit . \\nbilko ( and the name isn\\'t an accident ) is the head of an army motor pool group , but his passion is his schemes . \\nevery episode involves the sergeant and his men in one or another hair-brained plan to get rich quick while outwitting the officers of the base . \\n \" mchale\\'s navy \" \\'s granddaddy . \\nthat\\'s the idea behind this movie too , but the difference is that , as far-fetched and usually goofy as the television series was , it was funny . \\nthere is not one laugh in the film . \\nthe re-make retains the goofiness , but not the entertainment . \\neverything is just too clean . \\nit was obviously made on a hollywood back lot and looks every bit like it . \\nit all looks brand new , even the old beat-up stuff . \\nmartin is remarkably small in what should have been a bigger than life role . \\nin the original , phil silvers played the huckster with a heart of gold and more than a touch of sleaziness . \\nmartin\\'s bilko is a pale imitation . \\nthe only semi-bright spot is phil hartman as bilko\\'s arch-enemy . \\nit\\'s not saying much , considering martin\\'s lackluster character , but hartman leaves him in the dust . \\n',\n",
       " None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change x[0] and y[0] to see different reviews\n",
    "x[0], print(\"Human review was:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines - one example\n",
    "\n",
    "scikit-learn offers hand ways to build machine learning pipelines: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard training/test split (no cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# get tfidf values\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(x)\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "\n",
    "# instantiate, train, and test an logistic regression model\n",
    "logit_class = LogisticRegression(solver = 'liblinear',\n",
    "                                 penalty = 'l2', \n",
    "                                 C = 1000, \n",
    "                                 random_state = 1)\n",
    "model = logit_class.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216666666666667"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set accuracy\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-fold cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8155922  0.79910045 0.80630631] 0.8069996533264899\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated model!\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(solver = 'liblinear',\n",
    "                                               penalty = 'l2', \n",
    "                                               C = 1000, \n",
    "                                               random_state = 1))\n",
    "                     ])\n",
    "\n",
    "# for your own research, thesis, or publication\n",
    "# you would select cv equal to 10 or 20\n",
    "scores = cross_val_score(text_clf, x, y, cv = 3)\n",
    "\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 features for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for positive reviews:\n",
      "['gas', 'perfectly', 'family', 'political', 'will', 'seen', 'rocky', 'always', 'different', 'excellent', 'also', 'many', 'is', 'matrix', 'trek', 'well', 'definitely', 'truman', 'very', 'great', 'quite', 'fun', 'jackie', 'as', 'and']\n",
      "\n",
      "Top features for negative reviews:\n",
      "['bad', 'only', 'plot', 'worst', 'there', 'boring', 'script', 'why', 'have', 'unfortunately', 'dull', 'poor', 'any', 'waste', 'nothing', 'looks', 'ridiculous', 'supposed', 'no', 'even', 'harry', 'awful', 'then', 'reason', 'wasted']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bad_review = \"This was the most awful worst super bad movie ever!\"\n",
    "\n",
    "features = tfidf.transform([new_bad_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_good_review = 'WHAT A WONDERFUL, FANTASTIC MOVIE!!!'\n",
    "\n",
    "features = tfidf.transform([new_good_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a more complex statement\n",
    "my_review = 'I hated this movie, even though my friend loved it'\n",
    "my_features = tfidf.transform([my_review])\n",
    "model.predict(my_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - text classification\n",
    "\n",
    "1. Practice your text pre-processing skills on the classic novel Dracula! Here you'll just be performing the standardization operations on a text string instead of a DataFrame, so be sure to adapt the practices you saw with the UN HRC corpus processing appropriately. \n",
    "\n",
    "    Can you:\n",
    "    * Remove non-alphanumeric characters & punctuation?\n",
    "    * Remove digits?\n",
    "    * Remove unicode characters?\n",
    "    * Remove extraneous spaces?\n",
    "    * Standardize casing?\n",
    "    * Lemmatize tokens?\n",
    "\n",
    "2. Investigate classic horror novel vocabulary. Create a single TF-IDF sparse matrix that contains the vocabulary for _Frankenstein_ and _Dracula_. You should only have two rows (one for each of these novels), but potentially thousands of columns to represent the vocabulary across the two texts. What are the 20 most unique words in each? Make a dataframe or visualization to illustrate the differences.\n",
    "\n",
    "3. [Read through this 20 newsgroups dataset example](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) to get familiar with newspaper data. Do you best to understand and explain what is happening at each step of the workflow. \"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving preprocessing accuracy and efficiency\n",
    "\n",
    "Remember these are just the basics. There are more efficient ways to preprocess your text that you will want to consider. Read Chapter 8 \"spaCy and textaCy\" to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16c684165a00eba53f696e92e1de76bb4a10a33402bb31cdf5ab4f07210fc261"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}