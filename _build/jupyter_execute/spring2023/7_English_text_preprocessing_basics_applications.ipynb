{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - English text preprocessing basics - and applications\n",
    "2023 April 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/EastBayEv/SSDS-TAML/blob/main/spring2023/7_English_text_preprocessing_basics_applications.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](img/text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured text - text you find in the wild in books and websites - is generally not amenable to analysis. Before it can be analyzed, the text needs to be standardized to a format so that Python can recognize each unit of meaning **(called a \"token\")** as unique, no matter how many times it occurs and how it is stylized. \n",
    "\n",
    "Although not an exhaustive list, some key steps in preprocessing text include:  \n",
    "* Standardize text casing and spacing \n",
    "* Remove punctuation and special characters/symbols\n",
    "* Remove stop words\n",
    "* Stem or lemmatize: convert all non-base words to their base form \n",
    "\n",
    "Stemming/lemmatization and stop words (and some punctuation) are language-specific. The Natural Language ToolKit (NLTK) works for English out-of-the-box, but you'll need different code to work with other languages. Some languages (e.g. Chinese) also require *segmentation*: artificially inserting spaces between words. If you want to do text pre-processing for other languages, please let us know and we can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have the proper nltk modules\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 11:54:49.200589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus definition: United Nations Human Rights Council Documentation\n",
    "\n",
    "![unhrc](img/unhrc.jpg)\n",
    "\n",
    "We will select eleven .txt files from the UN HRC as our corpus, stored within the subfolder \"human_rights\" folder inside the main \"data\" directory. \n",
    "\n",
    "These documents contain information about human rights recommendations made by member nations towards countries deemed to be in violation of the HRC. \n",
    "\n",
    "[Learn more about the UN HRC by clicking here.](https://www.ohchr.org/en/hrbodies/hrc/pages/home.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the corpus directory\n",
    "\n",
    "Set the directory's file path and print the files it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/human_rights: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Make the directory \"human_rights\" inside of data\n",
    "!mkdir data\n",
    "!mkdir data/human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your \"data\" folder already exists in Colab and you want to delete it, type:\n",
    "# !rm -r data\n",
    "\n",
    "# If the \"human_rights\" folder already exists in Colab and you want to delete it, type:\n",
    "# !rm -r data/human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download elevent UN HRC files\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/afghanistan2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/bangladesh2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/cotedivoire2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/djibouti2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/fiji2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/jordan2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/kazakhstan2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/monaco2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/sanmarino2014.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/turkmenistan2013.txt\n",
    "# !wget -P data/human_rights/ https://raw.githubusercontent.com/EastBayEv/SSDS-TAML/main/spring2023/data/human_rights/tuvalu2013.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afghanistan2014.txt  fiji2014.txt         sanmarino2014.txt\r\n",
      "bangladesh2013.txt   jordan2013.txt       turkmenistan2013.txt\r\n",
      "cotedivoire2014.txt  kazakhstan2014.txt   tuvalu2013.txt\r\n",
      "djibouti2013.txt     monaco2013.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Check that we have eleven files, one for each country\n",
    "!ls data/human_rights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sanmarino2014.txt',\n",
       " 'tuvalu2013.txt',\n",
       " 'kazakhstan2014.txt',\n",
       " 'cotedivoire2014.txt',\n",
       " 'fiji2014.txt',\n",
       " 'bangladesh2013.txt',\n",
       " 'turkmenistan2013.txt',\n",
       " 'jordan2013.txt',\n",
       " 'monaco2013.txt',\n",
       " 'afghanistan2014.txt',\n",
       " 'djibouti2013.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "corpus = os.listdir('data/human_rights/')\n",
    "\n",
    "# View the contents of this directory\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store these documents in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in an empty dictionary for conversion to data frame\n",
    "empty_dictionary = {}\n",
    "\n",
    "# Loop through the folder of documents to open and read each one\n",
    "for document in corpus:\n",
    "    with open('data/human_rights/' + document, 'r', encoding = 'utf-8') as to_open:\n",
    "         empty_dictionary[document] = to_open.read()\n",
    "\n",
    "# Populate the data frame with two columns: file name and document text\n",
    "human_rights = (pd.DataFrame.from_dict(empty_dictionary, \n",
    "                                       orient = 'index')\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {'index': 'file_name', 0: 'document_text'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...\n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...\n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...\n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...\n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...\n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...\n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...\n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...\n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...\n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...\n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the text of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A/HRC/28/9 \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr.: General \n",
      "24 December 2014 \n",
      " \n",
      "Original: English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty-eighth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review* \n",
      " * The annex to the present report is circulated as received. \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction .............................................................................................................  1Ð4 3 \n",
      " I. Summary of the proceedings of the review process ................................................  5Ð77 3 \n",
      "  A. Presentation by the State under review ...........................................................  5Ð23 3 \n",
      "  B. Interactive dialogue and responses by the State under review ........................  24Ð77 6 \n",
      " II. Conclusions and/or recommendations .....................................................................  78Ð81 13 \n",
      " Annex \n",
      "  Composition of the delegation .......\n"
     ]
    }
   ],
   "source": [
    "# first thousand characters\n",
    "print(human_rights['document_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English text preprocessing\n",
    "\n",
    "Create a new column named \"clean_text\" to store the text as it is preprocessed. \n",
    "\n",
    "### What are some of the things we can do? \n",
    "\n",
    "These are just a few examples. How else could you improve this process? \n",
    "\n",
    "* Remove non-alphanumeric characters/punctuation\n",
    "* Remove digits\n",
    "* Remove [unicode characters](https://en.wikipedia.org/wiki/List_of_Unicode_characters)\n",
    "* Remove extra spaces\n",
    "* Convert to lowercase\n",
    "* Lemmatize (optional for now)\n",
    "\n",
    "Take a look at the first document after each step to see if you can notice what changed. \n",
    "\n",
    "> Remember: the process will likely be different for many other natural languages, which frequently require special considerations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-alphanumeric characters/punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'clean_text' to store the text we are standardizing\n",
    "human_rights['clean_text'] = human_rights['document_text'].str.replace(r'[^\\w\\s]', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC 28 9 \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "24 December 2014 \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                1Ð4 3 \n",
      " I  Summary of the proceedings of the review process                                                   5Ð77 3 \n",
      "  A  Presentation by the State under review                                                              5Ð23 3 \n",
      "  B  Interactive dialogue and responses by the State under review                           24Ð77 6 \n",
      " II  Conclusions and or recommendations                                                                        78Ð81 13 \n",
      " Annex \n",
      "  Composition of the delegation        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>\\nDistr   General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>\\nDistr   General 6 January 2014 \\nOriginal  E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>\\nDistr   General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>\\nDistr   General 4 April 2014 \\nOriginal  Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>\\n\\nDistr   General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "\n",
       "                                           clean_text  \n",
       "0    \\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...  \n",
       "1    \\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...  \n",
       "2    \\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...  \n",
       "3   \\nDistr   General 7 July 2014 English Original...  \n",
       "4    \\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...  \n",
       "5    \\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...  \n",
       "6    \\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...  \n",
       "7   \\nDistr   General 6 January 2014 \\nOriginal  E...  \n",
       "8   \\nDistr   General 3 January 2014 English Origi...  \n",
       "9   \\nDistr   General 4 April 2014 \\nOriginal  Eng...  \n",
       "10  \\n\\nDistr   General 8 July 2013 English Origin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view third column\n",
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\d', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC      \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "   December      \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                 Ð    \n",
      " I  Summary of the proceedings of the review process                                                    Ð     \n",
      "  A  Presentation by the State under review                                                               Ð     \n",
      "  B  Interactive dialogue and responses by the State under review                             Ð     \n",
      " II  Conclusions and or recommendations                                                                          Ð      \n",
      " Annex \n",
      "  Composition of the delegation        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unicode characters such as Ð and ð"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more on text encodings: https://www.w3.org/International/questions/qa-what-is-encoding\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " United Nations \n",
      " A HRC      \n",
      " \n",
      " \n",
      "\n",
      " General Assembly \n",
      " Distr   General \n",
      "   December      \n",
      " \n",
      "Original  English \n",
      " \n",
      "\n",
      "Human Rights Council \n",
      "\n",
      "Twenty eighth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "  Report of the Working Group on the Universal Periodic Review  \n",
      "   The annex to the present report is circulated as received  \n",
      "  San Marino \n",
      "Contents \n",
      " Paragraphs Page \n",
      "  Introduction                                                                                                                     \n",
      " I  Summary of the proceedings of the review process                                                         \n",
      "  A  Presentation by the State under review                                                                    \n",
      "  B  Interactive dialogue and responses by the State under review                                  \n",
      " II  Conclusions and or recommendations                                                                                \n",
      " Annex \n",
      "  Composition of the delegation             \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\s+', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " United Nations A HRC General Assembly Distr General December Original English Human Rights Council Twenty eighth session Agenda item Universal Periodic Review Report of the Working Group on the Universal Periodic Review The annex to the present report is circulated as received San Marino Contents Paragraphs Page Introduction I Summary of the proceedings of the review process A Presentation by the State under review B Interactive dialogue and responses by the State under review II Conclusions and or recommendations Annex Composition of the delegation Introduction The Working Group on the Universal Periodic Review established in accordance with Human Rights Council resolution of June held its twentieth session from October to November The review of San Marino was held at the th meeting on October The delegation of San Marino was headed by Pasquale Valentini Minister for Foreign Affairs At its th meeting held on October the Working Group adopted the report on San Marino On January the Hu\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " united nations a hrc general assembly distr general december original english human rights council twenty eighth session agenda item universal periodic review report of the working group on the universal periodic review the annex to the present report is circulated as received san marino contents paragraphs page introduction i summary of the proceedings of the review process a presentation by the state under review b interactive dialogue and responses by the state under review ii conclusions and or recommendations annex composition of the delegation introduction the working group on the universal periodic review established in accordance with human rights council resolution of june held its twentieth session from october to november the review of san marino was held at the th meeting on october the delegation of san marino was headed by pasquale valentini minister for foreign affairs at its th meeting held on october the working group adopted the report on san marino on january the hu\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "human_rights['clean_text'] = human_rights['clean_text'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(human_rights['clean_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the updated data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>distr general july english original english ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>distr general january original english gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>distr general january english original engli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>distr general april original english general...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>distr general july english original english ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "1         tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "2     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "3    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "6   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "7         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "8         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "9    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "10      djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "\n",
       "                                           clean_text  \n",
       "0     united nations a hrc general assembly distr ...  \n",
       "1     united nations a hrc general assembly distr ...  \n",
       "2     united nations a hrc general assembly distr ...  \n",
       "3     distr general july english original english ...  \n",
       "4     united nations a hrc general assembly distr ...  \n",
       "5     united nations a hrc general assembly distr ...  \n",
       "6     united nations a hrc general assembly distr ...  \n",
       "7     distr general january original english gener...  \n",
       "8     distr general january english original engli...  \n",
       "9     distr general april original english general...  \n",
       "10    distr general july english original english ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - redwoods webscraping\n",
    "\n",
    "This also works with data scraped from the web. Below is very brief BeautifulSoup example to save the contents of the Sequoioideae (redwood trees) Wikipedia page in a variable named `text`. \n",
    "\n",
    "1. Read through the code below\n",
    "2. Practice by repeating for a webpage of your choice\n",
    "\n",
    "![redwood](img/redwood.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import regex as re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three variables will get you started\n",
    "\n",
    "1. `url` - define the URL to be scraped \n",
    "2. `response` - perform the get request on the URL \n",
    "3. `soup` - create the soup object so we can parse the html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Sequoioideae\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the text\n",
    "\n",
    "HTML (hypertext markup language) is used to structure a webpage and the content it contains, including text.\n",
    "\n",
    "Below is a handy for loop that finds all everything within paragraph `<p>`, or paragraph tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in an empty string\n",
    "text = \"\"\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequoioideae, popularly known as redwoods, is a subfamily of coniferous trees within the family Cupressaceae. It includes the largest and tallest trees in the world.\n",
      "The three redwood subfamily genera are Sequoia from coastal California and Oregon, Sequoiadendron from California's Sierra Nevada, and Metasequoia in China. The redwood species contains the largest and tallest trees in the world. These trees can live for thousands of years. Threats include logging, fire suppression,[2] climate change, illegal marijuana cultivation, and burl poaching.[3][4][5]\n",
      "Only two of the genera, Sequoia and Sequoiadendron, are known for massive trees. Trees of Metasequoia, from the single living species Metasequoia glyptostroboides, are deciduous, grow much smaller (although are still large compared to most other trees) and can live in colder climates.[citation needed]\n",
      "Multiple studies of both morphological and molecular characters have strongly supported the assertion that the Sequoioideae are monophyletic.[6][7][8][9]\n",
      "Most modern phylogenies place Sequoia as sister to Sequoiadendron and Metasequoia as the out-group.[7][9][10] However, Yang et al. went on to investigate the origin of a peculiar genetic artifact of the Sequoioideae—the polyploidy of Sequoia—and generated a notable exception that calls into question the specifics of this relative consensus.[9]\n",
      "A 2006 paper based on non-molecular evidence suggested the following relationship among extant species:[11]\n",
      "\n",
      "M. glyptostroboides (dawn redwood)\n",
      "S. sempervirens (coast redwood)\n",
      "S. giganteum (giant sequoia)\n",
      "Taxodioideae\n",
      "A 2021 study using molecular evidence found the same relationships among Sequoioideae species, but found Sequoioideae to be the sister group to the Athrotaxidoideae (a superfamily presently known only from Tasmania) rather than to Taxodioideae. Sequoioideae and Athrotaxidoideae are thought to have diverged from each other during the Jurassic.[12]\n",
      "Reticulate evolution refers to the origination of a taxon through the merging of ancestor lineages.\n",
      "Polyploidy has come to be understood as quite common in plants—with estimates ranging from 47% to 100% of flowering plants and extant ferns having derived from ancient polyploidy.[13] Within the gymnosperms however it is quite rare. Sequoia sempervirens is hexaploid (2n= 6x= 66). To investigate the origins of this polyploidy Yang et al. used two single copy nuclear genes, LFY and NLY, to generate phylogenetic trees. Other researchers have had success with these genes in similar studies on different taxa.[9]\n",
      "Several hypotheses have been proposed to explain the origin of Sequoia's polyploidy: allopolyploidy by hybridization between Metasequoia and some probably extinct taxodiaceous plant; Metasequoia and Sequoiadendron, or ancestors of the two genera, as the parental species of Sequoia; and autohexaploidy, autoallohexaploidy, or segmental allohexaploidy.[citation needed]\n",
      "Yang et al. found that Sequoia was clustered with Metasequoia in the tree generated using the LFY gene but with Sequoiadendron in the tree generated with the NLY gene. Further analysis strongly supported the hypothesis that Sequoia was the result of a hybridization event involving Metasequoia and Sequoiadendron. Thus, Yang et al. hypothesize that the inconsistent relationships among Metasequoia, Sequoia, and Sequoiadendron could be a sign of reticulate evolution by hybrid speciation (in which two species hybridize and give rise to a third) among the three genera. However, the long evolutionary history of the three genera (the earliest fossil remains being from the Jurassic) make resolving the specifics of when and how Sequoia originated once and for all a difficult matter—especially since it in part depends on an incomplete fossil record.[10]\n",
      "Sequoioideae is an ancient taxon, with the oldest described Sequoioideae species, Sequoia jeholensis, recovered from Jurassic deposits.[14] A genus Medulloprotaxodioxylon, reported from the late Triassic of China supports the idea of a Norian origin.[1]\n",
      "\n",
      "The fossil record shows a massive expansion of range in the Cretaceous and dominance of the Arcto-Tertiary Geoflora, especially in northern latitudes. Genera of Sequoioideae were found in the Arctic Circle, Europe, North America, and throughout Asia and Japan.[15] A general cooling trend beginning in the late Eocene and Oligocene reduced the northern ranges of the Sequoioideae, as did subsequent ice ages.[16] Evolutionary adaptations to ancient environments persist in all three species despite changing climate, distribution, and associated flora, especially the specific demands of their reproduction ecology that ultimately forced each of the species into refugial ranges where they could survive.[citation needed]The entire subfamily is endangered. The IUCN Red List Category & Criteria assesses Sequoia sempervirens as Endangered (A2acd), Sequoiadendron giganteum as Endangered (B2ab) and Metasequoia glyptostroboides as Endangered (B1ab).\n",
      "New World Species:\n",
      "New World Species:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "Regular expressions are sequences of characters and symbols that represent search patterns in text - and are generally quite useful. \n",
    "\n",
    "[Check out the tutorial](https://docs.python.org/3/library/re.html) and [cheatsheet](https://www.dataquest.io/blog/regex-cheatsheet/) to find out what the below symbols mean and write your own code. Better yet you could write a pattern to do them simultaneously in one line/less lines of code in some cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'[^\\w\\s]','',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sequoioideae popularly known as redwoods is a subfamily of coniferous trees within the family cupressaceae it includes the largest and tallest trees in the world the three redwood subfamily genera are sequoia from coastal california and oregon sequoiadendron from californias sierra nevada and metasequoia in china the redwood species contains the largest and tallest trees in the world these trees can live for thousands of years threats include logging fire suppression climate change illegal marijuana cultivation and burl poaching only two of the genera sequoia and sequoiadendron are known for massive trees trees of metasequoia from the single living species metasequoia glyptostroboides are deciduous grow much smaller although are still large compared to most other trees and can live in colder climatescitation needed multiple studies of both morphological and molecular characters have strongly supported the assertion that the sequoioideae are monophyletic most modern phylogenies place sequoia as sister to sequoiadendron and metasequoia as the outgroup however yang et al went on to investigate the origin of a peculiar genetic artifact of the sequoioideaethe polyploidy of sequoiaand generated a notable exception that calls into question the specifics of this relative consensus a paper based on nonmolecular evidence suggested the following relationship among extant species m glyptostroboides dawn redwood s sempervirens coast redwood s giganteum giant sequoia taxodioideae a study using molecular evidence found the same relationships among sequoioideae species but found sequoioideae to be the sister group to the athrotaxidoideae a superfamily presently known only from tasmania rather than to taxodioideae sequoioideae and athrotaxidoideae are thought to have diverged from each other during the jurassic reticulate evolution refers to the origination of a taxon through the merging of ancestor lineages polyploidy has come to be understood as quite common in plantswith estimates ranging from to of flowering plants and extant ferns having derived from ancient polyploidy within the gymnosperms however it is quite rare sequoia sempervirens is hexaploid n x to investigate the origins of this polyploidy yang et al used two single copy nuclear genes lfy and nly to generate phylogenetic trees other researchers have had success with these genes in similar studies on different taxa several hypotheses have been proposed to explain the origin of sequoias polyploidy allopolyploidy by hybridization between metasequoia and some probably extinct taxodiaceous plant metasequoia and sequoiadendron or ancestors of the two genera as the parental species of sequoia and autohexaploidy autoallohexaploidy or segmental allohexaploidycitation needed yang et al found that sequoia was clustered with metasequoia in the tree generated using the lfy gene but with sequoiadendron in the tree generated with the nly gene further analysis strongly supported the hypothesis that sequoia was the result of a hybridization event involving metasequoia and sequoiadendron thus yang et al hypothesize that the inconsistent relationships among metasequoia sequoia and sequoiadendron could be a sign of reticulate evolution by hybrid speciation in which two species hybridize and give rise to a third among the three genera however the long evolutionary history of the three genera the earliest fossil remains being from the jurassic make resolving the specifics of when and how sequoia originated once and for all a difficult matterespecially since it in part depends on an incomplete fossil record sequoioideae is an ancient taxon with the oldest described sequoioideae species sequoia jeholensis recovered from jurassic deposits a genus medulloprotaxodioxylon reported from the late triassic of china supports the idea of a norian origin the fossil record shows a massive expansion of range in the cretaceous and dominance of the arctotertiary geoflora especially in northern latitudes genera of sequoioideae were found in the arctic circle europe north america and throughout asia and japan a general cooling trend beginning in the late eocene and oligocene reduced the northern ranges of the sequoioideae as did subsequent ice ages evolutionary adaptations to ancient environments persist in all three species despite changing climate distribution and associated flora especially the specific demands of their reproduction ecology that ultimately forced each of the species into refugial ranges where they could survivecitation neededthe entire subfamily is endangered the iucn red list category criteria assesses sequoia sempervirens as endangered a acd sequoiadendron giganteum as endangered b ab and metasequoia glyptostroboides as endangered b ab new world species new world species \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning with `TfidfVectorizer()`\n",
    "\n",
    "Remember `CountVectorizer()` for creating Bag of Word models? We can extend this idea of counting words, to _counting unique words_ within a document relative to the rest of the corpus with `TfidfVectorizer()`. Each row will still be a document in the document term matrix and each column will still be a linguistic feature, but the cells will now be populated by the word uniqueness weights instead of frequencies. Remember that: \n",
    "\n",
    "* For TF-IDF sparse matrices:\n",
    "    * A value closer to 1 indicate that a feature is more relevant to a particular document.\n",
    "    * A value closer to 0 indicates that that feature is less/not relevant to that document.\n",
    "\n",
    "![tf1](img/tf1.png)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "![tf2](img/tf2.png)\n",
    "\n",
    "[towardsdatascience](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range = (1, 3), \n",
    "                                stop_words = 'english', \n",
    "                                max_df = 0.50\n",
    "                                )\n",
    "tf_sparse = tf_vectorizer.fit_transform(human_rights['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 84181)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 39182)\t0.004856846879037771\n",
      "  (0, 31574)\t0.004856846879037771\n",
      "  (0, 50165)\t0.004856846879037771\n",
      "  (0, 79743)\t0.004856846879037771\n",
      "  (0, 46164)\t0.004856846879037771\n",
      "  (0, 70574)\t0.004856846879037771\n",
      "  (0, 67048)\t0.004856846879037771\n",
      "  (0, 48393)\t0.004856846879037771\n",
      "  (0, 55413)\t0.004856846879037771\n",
      "  (0, 5657)\t0.004856846879037771\n",
      "  (0, 2036)\t0.004856846879037771\n",
      "  (0, 18508)\t0.004856846879037771\n",
      "  (0, 4238)\t0.004856846879037771\n",
      "  (0, 49342)\t0.004856846879037771\n",
      "  (0, 2719)\t0.004856846879037771\n",
      "  (0, 39331)\t0.004856846879037771\n",
      "  (0, 7341)\t0.004856846879037771\n",
      "  (0, 80381)\t0.004856846879037771\n",
      "  (0, 49382)\t0.004856846879037771\n",
      "  (0, 2723)\t0.004856846879037771\n",
      "  (0, 43326)\t0.004856846879037771\n",
      "  (0, 20394)\t0.004856846879037771\n",
      "  (0, 27591)\t0.004856846879037771\n",
      "  (0, 53796)\t0.004856846879037771\n",
      "  (0, 74877)\t0.004856846879037771\n",
      "  :\t:\n",
      "  (10, 23861)\t0.004813404453682774\n",
      "  (10, 53868)\t0.005386104036626412\n",
      "  (10, 61809)\t0.005386104036626412\n",
      "  (10, 11732)\t0.006124442213746767\n",
      "  (10, 52233)\t0.006124442213746767\n",
      "  (10, 32036)\t0.00869094964616284\n",
      "  (10, 38685)\t0.00869094964616284\n",
      "  (10, 48447)\t0.012248884427493533\n",
      "  (10, 64367)\t0.00869094964616284\n",
      "  (10, 28795)\t0.004813404453682774\n",
      "  (10, 66492)\t0.03231662421975847\n",
      "  (10, 62562)\t0.005386104036626412\n",
      "  (10, 30131)\t0.004813404453682774\n",
      "  (10, 79340)\t0.006124442213746767\n",
      "  (10, 4251)\t0.00434547482308142\n",
      "  (10, 27107)\t0.00434547482308142\n",
      "  (10, 57250)\t0.026072848938488522\n",
      "  (10, 60838)\t0.030418323761569943\n",
      "  (10, 70491)\t0.026072848938488522\n",
      "  (10, 76162)\t0.00434547482308142\n",
      "  (10, 72779)\t0.005386104036626412\n",
      "  (10, 27613)\t0.00434547482308142\n",
      "  (10, 28707)\t0.010772208073252824\n",
      "  (10, 8151)\t0.010772208073252824\n",
      "  (10, 18289)\t0.00869094964616284\n"
     ]
    }
   ],
   "source": [
    "print(tf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tfidf sparse matrix to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone inclusive</th>\n",
       "      <th>zone inclusive education</th>\n",
       "      <th>zone senegal</th>\n",
       "      <th>zone senegal make</th>\n",
       "      <th>zone social</th>\n",
       "      <th>zone social benefit</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 84181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007608    0.007608            0.007608  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007165     0.007165   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004509   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.011198   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004721   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005111   \n",
       "10           0.007165  0.007165        0.007165           0.007165   0.000000   \n",
       "\n",
       "    ...      zone  zone inclusive  zone inclusive education  zone senegal  \\\n",
       "0   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "1   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "2   ...  0.011473        0.006711                  0.006711      0.000000   \n",
       "3   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "4   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "5   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "6   ...  0.006296        0.000000                  0.000000      0.007365   \n",
       "7   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "8   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "9   ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "10  ...  0.000000        0.000000                  0.000000      0.000000   \n",
       "\n",
       "    zone senegal make  zone social  zone social benefit     zouon  zouon bi  \\\n",
       "0            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "1            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "2            0.000000     0.006711             0.006711  0.000000  0.000000   \n",
       "3            0.000000     0.000000             0.000000  0.006249  0.006249   \n",
       "4            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "5            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "6            0.007365     0.000000             0.000000  0.000000  0.000000   \n",
       "7            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "8            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "9            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "10           0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "\n",
       "    zouon bi tidou  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.006249  \n",
       "4         0.000000  \n",
       "5         0.000000  \n",
       "6         0.000000  \n",
       "7         0.000000  \n",
       "8         0.000000  \n",
       "9         0.000000  \n",
       "10        0.000000  \n",
       "\n",
       "[11 rows x 84181 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_sparse.todense(), columns = tf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 20 highest weighted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "monaco                        0.720348\n",
       "tuvalu                        0.639251\n",
       "kazakhstan                    0.615483\n",
       "fiji                          0.582410\n",
       "turkmenistan                  0.578904\n",
       "san                           0.553681\n",
       "jordan                        0.491254\n",
       "san marino                    0.456544\n",
       "marino                        0.456544\n",
       "divoire                       0.340046\n",
       "te divoire                    0.340046\n",
       "te                            0.306989\n",
       "elimination violence          0.253620\n",
       "elimination violence woman    0.253620\n",
       "djiboutis                     0.250777\n",
       "reconciliation                0.245711\n",
       "fgm                           0.195982\n",
       "afghan                        0.190201\n",
       "bangladeshs                   0.183356\n",
       "violence woman law            0.182593\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.max().sort_values(ascending = False).head(n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country name to `tfidf_df`\n",
    "\n",
    "This way, we will know which document is relative to which country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sanmarino',\n",
       " 'tuvalu',\n",
       " 'kazakhstan',\n",
       " 'cotedivoire',\n",
       " 'fiji',\n",
       " 'bangladesh',\n",
       " 'turkmenistan',\n",
       " 'jordan',\n",
       " 'monaco',\n",
       " 'afghanistan',\n",
       " 'djibouti']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrangle the country names from the human_rights data frame\n",
    "countries = human_rights['file_name'].str.slice(stop = -8)\n",
    "countries = list(countries)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['COUNTRY'] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone inclusive</th>\n",
       "      <th>zone inclusive education</th>\n",
       "      <th>zone senegal</th>\n",
       "      <th>zone senegal make</th>\n",
       "      <th>zone social</th>\n",
       "      <th>zone social benefit</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sanmarino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tuvalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kazakhstan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>cotedivoire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fiji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>turkmenistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>djibouti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 84182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007608    0.007608            0.007608  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007165     0.007165   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004509   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.011198   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004721   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005111   \n",
       "10           0.007165  0.007165        0.007165           0.007165   0.000000   \n",
       "\n",
       "    ...  zone inclusive  zone inclusive education  zone senegal  \\\n",
       "0   ...        0.000000                  0.000000      0.000000   \n",
       "1   ...        0.000000                  0.000000      0.000000   \n",
       "2   ...        0.006711                  0.006711      0.000000   \n",
       "3   ...        0.000000                  0.000000      0.000000   \n",
       "4   ...        0.000000                  0.000000      0.000000   \n",
       "5   ...        0.000000                  0.000000      0.000000   \n",
       "6   ...        0.000000                  0.000000      0.007365   \n",
       "7   ...        0.000000                  0.000000      0.000000   \n",
       "8   ...        0.000000                  0.000000      0.000000   \n",
       "9   ...        0.000000                  0.000000      0.000000   \n",
       "10  ...        0.000000                  0.000000      0.000000   \n",
       "\n",
       "    zone senegal make  zone social  zone social benefit     zouon  zouon bi  \\\n",
       "0            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "1            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "2            0.000000     0.006711             0.006711  0.000000  0.000000   \n",
       "3            0.000000     0.000000             0.000000  0.006249  0.006249   \n",
       "4            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "5            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "6            0.007365     0.000000             0.000000  0.000000  0.000000   \n",
       "7            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "8            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "9            0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "10           0.000000     0.000000             0.000000  0.000000  0.000000   \n",
       "\n",
       "    zouon bi tidou       COUNTRY  \n",
       "0         0.000000     sanmarino  \n",
       "1         0.000000        tuvalu  \n",
       "2         0.000000    kazakhstan  \n",
       "3         0.006249   cotedivoire  \n",
       "4         0.000000          fiji  \n",
       "5         0.000000    bangladesh  \n",
       "6         0.000000  turkmenistan  \n",
       "7         0.000000        jordan  \n",
       "8         0.000000        monaco  \n",
       "9         0.000000   afghanistan  \n",
       "10        0.000000      djibouti  \n",
       "\n",
       "[11 rows x 84182 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine unique words by each document/country\n",
    "\n",
    "Change the country names to view their highest rated terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuvalu                            0.639251\n",
       "safe drinking water               0.094018\n",
       "safe drinking                     0.094018\n",
       "water sanitation                  0.093357\n",
       "sanitation                        0.092709\n",
       "rapporteur human right            0.090329\n",
       "human right safe                  0.090329\n",
       "drinking                          0.088689\n",
       "drinking water                    0.088689\n",
       "drinking water sanitation         0.078348\n",
       "special rapporteur human          0.077210\n",
       "rapporteur human                  0.077210\n",
       "island                            0.077210\n",
       "climate change                    0.073125\n",
       "right safe                        0.073125\n",
       "right safe drinking               0.073125\n",
       "national strategic development    0.069484\n",
       "constraint                        0.065350\n",
       "national strategic                0.065331\n",
       "strategic development plan        0.059392\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = tfidf_df[tfidf_df['COUNTRY'] == 'tuvalu']\n",
    "country.max(numeric_only = True).sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UN HRC text analysis - what next?\n",
    "\n",
    "What next? Keep in mind that we have not even begun to consider named entities and parts of speech. What problems immediately jump out from the above examples, such as with the number and uniqueness of country names?\n",
    "\n",
    "The next two chapters 8 and 9 introduce powerful text preprocessing and analysis techniques. Read ahead to see how we can handle roadblocks such as these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis is the contextual mining of text data that elicits abstract information in source materials to determine if data are positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sa](img/sa.jpg)\n",
    "\n",
    "[Repustate](https://www.repustate.com/blog/sentiment-analysis-challenges-with-solutions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the nltk built movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define x (reviews) and y (judgements) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Judgements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Judgements\n",
       "0  plot : two teen couples go to a church party ,...        neg\n",
       "1  the happy bastard's quick movie review \\ndamn ...        neg\n",
       "2  it is movies like these that make a jaded movi...        neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...        neg\n",
       "4  synopsis : a mentally unstable man undergoing ...        neg"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human review was: pos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('confession time : i have never , ever seen gone with the wind . \\ni don\\'t know why , really . \\nhaven\\'t wanted to check it out on video , haven\\'t been at home the nights it was on network tv , and it was too far to drive the last time it was on the big-screen . \\nso right up front , i\\'ll admit that i don\\'t know what the heck i\\'m talking about , but here goes . . . \\nis titanic the gone with the wind of the 1990\\'s ? \\nmaybe that\\'s going a little bit too far . \\nas good a job as leonardo dicaprio and kate winslet do in this movie , they\\'re no clark gable and vivien leigh . \\nbut . . . \\nthe parallels are there . \\ngwtw was the first movie to take real advantage of the most revolutionary technology available -- technicolor . \\ntitanic takes revolutionary steps forward in seamlessly integrating computer graphic design with actors . \\ngwtw places america\\'s greatest tragedy in the background of a classic love story , titanic does the same with the atlantic\\'s most legendary tragedy . \\nthey both have strong-willed redheaded heroines , they both exploit the class differences between the aristocracy and the slaves/steerage bums , they were both incredibly expensive and popular . . . \\nok , maybe that\\'s not enough parallels . \\nso titanic\\'s not in gwtw\\'s league . \\nno matter . \\ntitanic is a great movie in its own right , complete with spills , thrills and ( especially ) chills . \\nmuch has been made of the humongous cost of the production , and all of the care than went into making the huge luxury liner come alive again . \\nthe money was obviously well-spent . \\nthe costumes look great , the sets look great , the cgi graphics look great . \\ni especially liked the expensive little touches , like spending tons of money on authentic titanic china only to break it all on the floor as the ship sinks . \\nbut writer/director/producer james cameron\\'s real challenge in writing/directing/ producing titanic wasn\\'t just costuming and set design and special effects . \\ncameron\\'s major headache was keeping the audience interested in a tale where everybody knows the ending going in . \\nhe succeeds masterfully . \\ncameron does two things that work incredibly well . \\nfirst , he shows us modern-day salvage operations on titanic ( that\\'s just \" titanic \" , not \" the titanic \" , mind you ) . \\nthe first glimpse we get of titanic is the ship in its present state , corroding slowly away under the hammering pressure of the north atlantic , from the window of a minisub piloted by treasure hunter brock lovett ( bill paxton ) . \\ntelevision coverage of the exploration of titanic intrigues 101-year-old rose calvert ( gloria stuart ) , who survived the wreck in 1912 . \\nstuart does a phenomenal job in a brief role , narrating the story of her experience to a stunned paxton and his roughneck crew . \\nsecondly , cameron keeps the storyline focused almost exclusively on the rose character , and the romantic triangle between rose ( winslet ) , her bastard millionaire fiancee cal hockley ( billy zane ) and the irrepressible young artist jack daswon ( dicaprio ) . \\nthe way that big-budget disaster movies usually go wrong is to have an all-star cast , so we see the impact of the disaster on a wide group of people . \\ncameron wisely chooses to stick with rose and jack , while paying scant heed to the celebrities on board . \\nthe supporting cast is professional , but mostly anonymous -- other than kathy bates as the unsinkable molly brown , there\\'s no moment when you stop and say , oh , yeah , i know him , what\\'s he been in . \\n ( although i would like to have seen colm meaney in a white star uniform , or even as the ill-fated engineer . ) \\nthe love story itself is rather conventional . \\ni think some reviewers found it weak , and that may be a fair criticism . \\nthe performances are the key here . \\nzane has the meatiest part in the movie , and he plays the arrogant , condescending steel millionaire to the hilt . \\nhe\\'s smooth , he looks great in a tuxedo , and he\\'s a convincing enough jerk that the winslet-dicaprio relationship looks plausible . \\nat the moment when he sees a little girl too frightened to get aboard a lifeboat , you can hear the wheels in his mind turning , saying not \" can i save this little girl ? \" , \\nbut \" can she help me get on a lifeboat ? \" \\ndicaprio is a revelation . \\ni hadn\\'t seen him before in anything , and didn\\'t know what the heck to expect , really . \\n ( honestly , i expected a bad irish accent , but cameron evidently decided that was a bad idea , so dicaprio plays a poor american artist who wins a ticket in a poker game . ) \\ndicaprio exhibits an infectious joy at being alive , and being on the titanic , that it\\'s hard not to like him . \\nfrom the moment that the ship leaves port until it hits the iceberg , dicaprio has to carry the movie and keep our interest , and he never falters . \\nwinslet\\'s character grows up a lot during the movie , and so does her performance . \\nat first , she\\'s not required to do anything but wear period clothing and look drop-dead gorgeous . \\nwe know from the narration that she\\'s monstrously unhappy with her arranged marriage to zane , but there isn\\'t any expression of these feelings until she encounters dicaprio . \\nwinslet and dicaprio develop a chemistry that manages to propel the movie along until the ship hits the iceberg . \\nit\\'s at that moment where winslet\\'s character really comes alive . \\nfaced with real danger , she drops her spoiled-rich-girl mannerisms and does a splendid job . \\nas rose and jack race around the doomed ship , looking for shelter from the freezing water and cal\\'s fiery temper , winslet turns in a superb acting performance , mixing courage and compassion and anger with sheer shrieking terror . \\nof course , the most interesting character is the ship itself . \\ncameron has clearly fallen in love with titanic , and shows her in every mood -- as a deserted wreck , down in the boiler room , up on the bridge , down in the hold , at the captain\\'s table , down in steerage -- and manages to bring the great ship back from the dead . \\ncameron\\'s greatest gift is that he allows us to fall in love with titanic as well . \\n',\n",
       " None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change x[0] and y[0] to see different reviews\n",
    "x[3], print(\"Human review was:\", y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines - one example\n",
    "\n",
    "scikit-learn offers hand ways to build machine learning pipelines: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tfidf.transform` not found.\n"
     ]
    }
   ],
   "source": [
    "?tfidf.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard training/test split (no cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# get tfidf values\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(x)\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "\n",
    "# instantiate, train, and test an logistic regression model\n",
    "logit_class = LogisticRegression(solver = 'liblinear',\n",
    "                                 penalty = 'l2', \n",
    "                                 C = 1000, \n",
    "                                 random_state = 1)\n",
    "model = logit_class.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216666666666667"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set accuracy\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-fold cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "?cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m text_clf \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvect\u001b[39m\u001b[38;5;124m'\u001b[39m, CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))),\n\u001b[1;32m      3\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfTransformer()),\n\u001b[1;32m      4\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m                      ])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for your own research, thesis, or publication\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# you would select cv equal to 10 or 20\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores, np\u001b[38;5;241m.\u001b[39mmean(scores))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    507\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 509\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 267\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 390\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1327\u001b[0m             )\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cross-validated model!\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(solver = 'liblinear',\n",
    "                                               penalty = 'l2', \n",
    "                                               C = 1000, \n",
    "                                               random_state = 1))\n",
    "                     ])\n",
    "\n",
    "# for your own research, thesis, or publication\n",
    "# you would select cv equal to 10 or 20\n",
    "scores = cross_val_score(text_clf, x, y, cv = 10)\n",
    "\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 features for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for positive reviews:\n",
      "['gas', 'perfectly', 'family', 'political', 'will', 'seen', 'rocky', 'always', 'different', 'excellent', 'also', 'many', 'is', 'matrix', 'trek', 'well', 'definitely', 'truman', 'very', 'great', 'quite', 'fun', 'jackie', 'as', 'and']\n",
      "\n",
      "Top features for negative reviews:\n",
      "['bad', 'only', 'plot', 'worst', 'there', 'boring', 'script', 'why', 'have', 'unfortunately', 'dull', 'poor', 'any', 'waste', 'nothing', 'looks', 'ridiculous', 'supposed', 'no', 'even', 'harry', 'awful', 'then', 'reason', 'wasted']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bad_review = \"This was the most awful worst super bad movie ever!\"\n",
    "\n",
    "features = tfidf.transform([new_bad_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_good_review = 'WHAT A WONDERFUL, FANTASTIC MOVIE!!!'\n",
    "\n",
    "features = tfidf.transform([new_good_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a more complex statement\n",
    "my_review = \"Newspapers are not food.\"\n",
    "my_features = tfidf.transform([my_review])\n",
    "model.predict(my_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - text classification\n",
    "\n",
    "1. Practice your text pre-processing skills on the classic novel Dracula! Here you'll just be performing the standardization operations on a text string instead of a DataFrame, so be sure to adapt the practices you saw with the UN HRC corpus processing appropriately. \n",
    "\n",
    "    Can you:\n",
    "    * Remove non-alphanumeric characters & punctuation?\n",
    "    * Remove digits?\n",
    "    * Remove unicode characters?\n",
    "    * Remove extraneous spaces?\n",
    "    * Standardize casing?\n",
    "    * Lemmatize tokens?\n",
    "\n",
    "2. Investigate classic horror novel vocabulary. Create a single TF-IDF sparse matrix that contains the vocabulary for _Frankenstein_ and _Dracula_. You should only have two rows (one for each of these novels), but potentially thousands of columns to represent the vocabulary across the two texts. What are the 20 most unique words in each? Make a dataframe or visualization to illustrate the differences.\n",
    "\n",
    "3. [Read through this 20 newsgroups dataset example](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) to get familiar with newspaper data. Do you best to understand and explain what is happening at each step of the workflow. \"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving preprocessing accuracy and efficiency\n",
    "\n",
    "Remember these are just the basics. There are more efficient ways to preprocess your text that you will want to consider. Read Chapter 8 \"spaCy and textaCy\" to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                Type                                Data/Info\n",
      "---------------------------------------------------------------------\n",
      "BeautifulSoup           type                                <class 'bs4.BeautifulSoup'>\n",
      "CountVectorizer         type                                <class 'sklearn.feature_e<...>on.text.CountVectorizer'>\n",
      "Counter                 type                                <class 'collections.Counter'>\n",
      "LogisticRegression      type                                <class 'sklearn.linear_mo<...>stic.LogisticRegression'>\n",
      "Pipeline                ABCMeta                             <class 'sklearn.pipeline.Pipeline'>\n",
      "PorterStemmer           ABCMeta                             <class 'nltk.stem.porter.PorterStemmer'>\n",
      "TfidfTransformer        type                                <class 'sklearn.feature_e<...>n.text.TfidfTransformer'>\n",
      "TfidfVectorizer         type                                <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "WordNetLemmatizer       type                                <class 'nltk.stem.wordnet.WordNetLemmatizer'>\n",
      "accuracy_score          function                            <function accuracy_score at 0x7fa6296c9040>\n",
      "classification_report   function                            <function classification_<...>report at 0x7fa6296c9940>\n",
      "confusion_matrix        function                            <function confusion_matrix at 0x7fa6296c90d0>\n",
      "corpus                  list                                n=11\n",
      "countries               list                                n=11\n",
      "country                 DataFrame                              abasi  abasi desk  aba<...>n[1 rows x 84182 columns]\n",
      "cross_val_score         function                            <function cross_val_score at 0x7fa6297159d0>\n",
      "document                str                                 djibouti2013.txt\n",
      "empty_dictionary        dict                                n=11\n",
      "feature_names           ndarray                             39659: 39659 elems, type `object`, 317272 bytes (309.8359375 kb)\n",
      "features                csr_matrix                            (0, 39086)\t0.5889877844<...>12776)\t0.7435961012857455\n",
      "human_rights            DataFrame                                          file_name <...>sh original english ...  \n",
      "judgements              list                                n=2000\n",
      "logit_class             LogisticRegression                  LogisticRegression(C=1000<...>te=1, solver='liblinear')\n",
      "model                   LogisticRegression                  LogisticRegression(C=1000<...>te=1, solver='liblinear')\n",
      "movie_reviews           CategorizedPlaintextCorpusReader    <CategorizedPlaintextCorp<...>a/corpora/movie_reviews'>\n",
      "movies                  DataFrame                                                    <...>\\n[2000 rows x 2 columns]\n",
      "my_features             csr_matrix                            (0, 24044)\t0.1411492820<...> 2217)\t0.1341357443684459\n",
      "my_review               str                                 Newspapers are not food.\n",
      "new_bad_review          str                                 This was the most awful w<...>rst super bad movie ever!\n",
      "new_good_review         str                                 WHAT A WONDERFUL, FANTASTIC MOVIE!!!\n",
      "nlp                     English                             <spacy.lang.en.English object at 0x7fa616022940>\n",
      "nltk                    module                              <module 'nltk' from '/Use<...>ckages/nltk/__init__.py'>\n",
      "np                      module                              <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "os                      module                              <module 'os' from '/Users<...>da3/lib/python3.8/os.py'>\n",
      "paragraph               Tag                                 <p><b>New World Species</b>:\\n</p>\n",
      "pd                      module                              <module 'pandas' from '/U<...>ages/pandas/__init__.py'>\n",
      "punctuation             str                                 !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "re                      module                              <module 'regex' from '/Us<...>kages/regex/__init__.py'>\n",
      "requests                module                              <module 'requests' from '<...>es/requests/__init__.py'>\n",
      "response                Response                            <Response [200]>\n",
      "reviews                 list                                n=2000\n",
      "roc_auc_score           function                            <function roc_auc_score at 0x7fa6296add30>\n",
      "roc_curve               function                            <function roc_curve at 0x7fa6296adf70>\n",
      "scores                  ndarray                             10: 10 elems, type `float64`, 80 bytes\n",
      "shuffle                 function                            <function shuffle at 0x7fa62939c4c0>\n",
      "sns                     module                              <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
      "soup                    BeautifulSoup                       <!DOCTYPE html>\\n<html cl<...>script>\\n</body>\\n</html>\n",
      "spacy                   module                              <module 'spacy' from '/Us<...>kages/spacy/__init__.py'>\n",
      "stopwords               LazyCorpusLoader                    <WordListCorpusReader in <...>pwords' (not loaded yet)>\n",
      "text                    str                                  sequoioideae popularly k<...>pecies new world species \n",
      "text_clf                Pipeline                            Pipeline(steps=[('vect', <...>   solver='liblinear'))])\n",
      "tf_sparse               csr_matrix                            (0, 39182)\t0.0048568468<...>8289)\t0.00869094964616284\n",
      "tf_vectorizer           TfidfVectorizer                     TfidfVectorizer(max_df=0.<...>3), stop_words='english')\n",
      "tfidf                   TfidfVectorizer                     TfidfVectorizer()\n",
      "tfidf_df                DataFrame                                  abasi  abasi desk <...>[11 rows x 84182 columns]\n",
      "to_open                 TextIOWrapper                       <_io.TextIOWrapper name='<...>ode='r' encoding='utf-8'>\n",
      "top25neg                ndarray                             25: 25 elems, type `int64`, 200 bytes\n",
      "top25pos                ndarray                             25: 25 elems, type `int64`, 200 bytes\n",
      "train_test_split        function                            <function train_test_split at 0x7fa6296a7c10>\n",
      "url                     str                                 https://en.wikipedia.org/wiki/Sequoioideae\n",
      "warnings                module                              <module 'warnings' from '<...>b/python3.8/warnings.py'>\n",
      "x                       ndarray                             2000: 2000 elems, type `object`, 16000 bytes\n",
      "x_test                  csr_matrix                            (0, 39482)\t0.0556930153<...>750)\t0.018569075100128132\n",
      "x_train                 csr_matrix                            (0, 39028)\t0.0233503390<...>, 338)\t0.1198810191867183\n",
      "y                       ndarray                             2000: 2000 elems, type `object`, 16000 bytes\n",
      "y_test                  ndarray                             600: 600 elems, type `object`, 4800 bytes\n",
      "y_train                 ndarray                             1400: 1400 elems, type `object`, 11200 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16c684165a00eba53f696e92e1de76bb4a10a33402bb31cdf5ab4f07210fc261"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}