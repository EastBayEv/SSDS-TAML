{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Appendix**\n",
    "\n",
    "* Welcome to the appendix! Here you'll find instructions/guides to various Python techniques, tasks, and operations. If you have any suggestions as to what you'd like a section to be written about, please let us know! ad2weng@stanford.edu would be happy to receive your feedback! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: *Virtual environments in Python*\n",
    "\n",
    "When operating in Python, you'll often hear/read the advice that you should set up a \"virtual environment\" for each project you are working on. What exactly is a virtual environment, and why do you need one for every project? \n",
    "\n",
    "* From Python's official documentation: \n",
    "\n",
    "    * A virtual environment is a Python environment such that the Python interpreter, libraries, and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a \"system\" Python; i.e., the version of Python which is installed as part of your operating system.  \n",
    "\n",
    "Put another way, activating your project in a virtual environment allows it to become it's own self-contained application. A few advantages of doing this include: \n",
    "\n",
    "* Allows you to download packages into your project without administrator privileges/status. \n",
    "\n",
    "* Compartmentalizes your project materials for easy sharing and replication. \n",
    "\n",
    "* *Avoids inter-project conflicts regarding versions and dependencies for packages.*\n",
    "\n",
    "That last point can become especially relevant as you work on multiple projects in Python, as one critical version/dependency for one project can cause your other projects to stop working. And, the process of uninstalling packages and/or switching versions for projects is tedious and time-consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the benefits demonstrated, how exactly do we go about setting up a virtual environment? So glad you asked! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A.1: *Virtual environments in Anaconda*\n",
    "\n",
    "Anaconda is the preferred distribution for local Python installs, with many functionalities presented in a user-friendly interface and offering a suite of applications to aid in data science projects. You can download it [here](https://www.anaconda.com/products/distribution).\n",
    "\n",
    "\n",
    "#### A.1.1: *Setting up a virtual environment*\n",
    "* Once you've downloaded Anaconda, you can set up a virtual environment by: \n",
    "\n",
    "    1. Open Anaconda Navigator on your computer. \n",
    "\n",
    "    2. On the left-hand side of the Navigator window, find and click on the button that say 'Environments': \n",
    "    \n",
    "        <div>\n",
    "        <img src=\"img/ana_env.png\" width=\"500\"/>\n",
    "        <div>\n",
    "\n",
    "    3. In the 'Environments' page, go to the bottom-left of the page and click the button that says 'Create'.\n",
    "\n",
    "        * When you do so, you'll be prompted by a pop-up window to provide a name for your new virtual environment. The location for the virutal environment will be shown to you, and you can install a specific version of Python and/or R: \n",
    "        \n",
    "            <div>\n",
    "            <img src=\"img/test_env.png\"/>\n",
    "            <div>\n",
    "\n",
    "        * Click the 'Create' button in the pop-up window, and wait for the virtual environment to finish being created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.2: *Activating your virtual environment*\n",
    "Now you're ready to use your virtual environment! To work in this environment, anytime you open Anaconda: \n",
    "\n",
    "* Navigate to the 'Environments' page; \n",
    "\n",
    "* Find the environment you want to use; and \n",
    "\n",
    "* Click on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.3: *Adding packages to your virutal environment*\n",
    "\n",
    "To install packages in this virtual environment, either: \n",
    "\n",
    "* Stay on the 'Environments' page, and on the right-hand side of the page:\n",
    "     \n",
    "     * Change the field dictating displayed packages to 'Not installed' or 'All': \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/packages_ana_env.png\" width=\"500\"/>\n",
    "          <div>\n",
    "\n",
    "     * Go to the 'Search Packages' field and type in the name(s) of the package you want to install; \n",
    "\n",
    "     * If your package is available, click the open checkbox to the left of the package name: \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/install_numpy_base.png\" width=\"500\"/>\n",
    "          <div>\n",
    "\n",
    "     * Once you've selected all of the packages of interest, click the 'Apply' button in the bottom right-hand corner of the page to install them.  \n",
    "\n",
    "* Go to the 'Home' page, and (install and then) open the 'CMD.exe Prompt' program:\n",
    "\n",
    "     * In your command prompt window, you'll see that you're operating in your previously-selected virtual environment: \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/my_env_cmd_prompt.png\" width=\"500\">\n",
    "          <div>\n",
    "\n",
    "     * In this window, type ``` pip install [name of package] ``` for each package you want to install/weren't able to install in the 'Environments' page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A.2: *Virtual environments in Command Window*\n",
    "\n",
    "If you prefer to run Python in a command window-type interface, you can still use virtual environments! For this purpose, I recommend the user-friendly [pipenv](https://pipenv.pypa.io/en/latest/) package (which utilizes the ```pip``` and ```virtualenv``` packages, but handles most of the nitty-gritty details so you don't have to).\n",
    "\n",
    "Let's take a look at how to use it: \n",
    "\n",
    "**More coming soon...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: *More on text preprocessing*\n",
    "\n",
    "While the exact steps you elect to use for text preprocessing will ultimately depend on applications, there are some more generalizable techniques that you can usually look to apply: \n",
    "\n",
    "* **Expand contractions** - contractions like \"don't\", \"they're\", and \"it's\" all count as unique tokens if punctuation is simply removed (converting them to \"dont\", \"theyre\", \"its\", respectively). Decompose contractions into their constituent words to get more accurate counts of tokens like \"is,\" \"they,\" etc. (or for ensuring their removal when omitting stopwords from text -- read on for more about those!)\n",
    "\n",
    "    * The [contractions](https://github.com/kootenpv/contractions) package can be useful here! Let's see an example:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/evanmuzzall/.local/share/virtualenvs/SSDS-TAML-xaUfvlpM/lib/python3.9/site-packages (0.1.72)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/evanmuzzall/.local/share/virtualenvs/SSDS-TAML-xaUfvlpM/lib/python3.9/site-packages (from contractions) (0.0.21)\r\n",
      "Requirement already satisfied: anyascii in /Users/evanmuzzall/.local/share/virtualenvs/SSDS-TAML-xaUfvlpM/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\r\n",
      "Requirement already satisfied: pyahocorasick in /Users/evanmuzzall/.local/share/virtualenvs/SSDS-TAML-xaUfvlpM/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\r\n"
     ]
    }
   ],
   "source": [
    "# required install: \n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to know how you are doing! You are her best friend, but I am your friend too, are not I?\n",
      "I Am About To go sign up for an SSDS consult - you all got to do it too!\n"
     ]
    }
   ],
   "source": [
    "import contractions \n",
    "\n",
    "contractions_sentence = \"I'd like to know how you're doing! You're her best friend, but I'm your friend too, aren't I?\"\n",
    "\n",
    "# let's see the text, de-contraction-afied!\n",
    "print(contractions.fix(contractions_sentence))\n",
    "\n",
    "# this package can also resolve some slang! an example in action: \n",
    "slang_sentence = \"Ima go sign up for an SSDS consult - yall gotta do it too!\"\n",
    "\n",
    "print(contractions.fix(slang_sentence, slang=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you ran the previous code cell, you might notice that one of the contraction expansions in ```contractions_sentence``` isn't quite grammatically correct; \"aren't I?\" is coverted to \"are not I?\" where it should be \"am I not?\"\n",
    " \n",
    "    * This is because the ```contractions``` package only offers *static* contraction conversions--i.e., it doesn't consider the context surrounding a contraction and always converts a specific contraction into a specific expansion. \n",
    "\n",
    "    * For a package that converts contractions with more contextual analysis, consider the [pycontractions](https://pypi.org/project/pycontractions/) package. Where multiple expansions are possible, this package evaluates which expansions are gramatically correct and then which one is most likely to be correct (by a metric called [Word Mover's Distance](https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632)).\n",
    "\n",
    "    * If you would like to use ```pycontractions```, be note that the install can be rather finnicky--the package requires another module called ```language-check``` which requires an up-to-date version of [Java](https://www.java.com/en/download/help/download_options.html). If you have issues during the install, please reach out to SSDS for help! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Remove stopwords** - stopwords are words like \"a,\" \"from,\" and \"the\" which are typically filtered out from text before analysis as they do not meaningfully contribute to the content of a document. Leaving in stopwords can lead to irrelevant topics in topic modeling, dilute the strength of sentiment in sentiment analysis, etc. \n",
    "\n",
    "    * Here's a quick loop that can help filter out the stopwords from a string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example stopwords include: ['i', 'me', 'my', 'myself', 'we']\n",
      "['Hi', '!', 'This', 'is', 'a', 'needlessly', 'wordy', 'sentence', 'with', 'lots', 'of', 'stopwords', '.', 'My', 'favorite', 'words', 'are', ':', 'a', ',', 'the', ',', 'with', ',', 'which', '.', 'You', 'may', 'think', 'that', 'is', 'strange', '-', 'and', 'it', 'is', '!']\n",
      "['Hi', '!', 'needlessly', 'wordy', 'sentence', 'lots', 'stopwords', '.', 'favorite', 'words', ':', ',', ',', ',', '.', 'may', 'think', 'strange', '-', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence = \"Hi! This is a needlessly wordy sentence with lots of stopwords. My favorite words are: a, the, with, which. You may think that is strange - and it is!\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Example stopwords include: \" + str(stopwords.words('english')[:5])) # if you want to see what are considered English stopwords by the NLTK package\n",
    "\n",
    "word_tokens = word_tokenize(example_sentence)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "# let's see the difference!\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that different packages have different lists which define stopwords, so make sure you pick a suitable one. Also, feel free to define your own custom stopwords lists!  \n",
    "\n",
    "* **Standardize phrases** - oftentimes text preprocessing is carried out as a precursor to a matching exercise (e.g. using company name to merge two databases). In such cases, we may want to standardize key phrases. For example, \"My Little Startup, LLC\" and \"my little startup\" clearly refer to the same entity, but will not match currently. \n",
    "\n",
    "    * In such cases, we may need to write a custom script to standardize key phrases, or there may be a packages out there that already do this for us. Let's take a look at one for our example, standardizing company names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cleanco in /Users/evanmuzzall/.local/share/virtualenvs/SSDS-TAML-xaUfvlpM/lib/python3.9/site-packages (2.2)\r\n"
     ]
    }
   ],
   "source": [
    "# required install: \n",
    "!pip install cleanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from cleanco import basename\n",
    "\n",
    "business_name_one = \"My Little Startup, LLC\"\n",
    "cleaned_name_one  = basename(business_name_one) # feel free to print this out! just add: 'print(cleaned_name_one)' below. \n",
    "\n",
    "business_name_two = \"My Little Startup\"\n",
    "cleaned_name_two  = basename(business_name_two)\n",
    "\n",
    "# sanity check - are the cleaned company names identical?  \n",
    "print(cleaned_name_one == cleaned_name_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How and where you choose to standardize phrases in text will of course depend on your end goal, but there are plenty of resources/examples out there for you to model an approach after if a package doesn't already exist!\n",
    "\n",
    "* **Normalize text** - normalization refers to the process of transforming text into a canonical (standard) form. Sometimes, people take this to mean the entire text pre-processing pipeline, but here we're using it to refer to conversions like \"2mrrw\" to \"tomorrow\" and \"b4\" to \"before.\" \n",
    "\n",
    "    * This process is especially useful when using social media comments as your base text for analysis but often requires custom scripting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16c684165a00eba53f696e92e1de76bb4a10a33402bb31cdf5ab4f07210fc261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}