{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Appendix**\n",
    "\n",
    "* Welcome to the appendix! Here you'll find instructions/guides to various Python techniques, tasks, and operations. If you have any suggestions as to what you'd like a section to be written about, please let us know! ad2weng@stanford.edu would be happy to receive your feedback! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: *Virtual environments in Python*\n",
    "\n",
    "When operating in Python, you'll often hear/read the advice that you should set up a \"virtual environment\" for each project you are working on. What exactly is a virtual environment, and why do you need one for every project? \n",
    "\n",
    "* From Python's official documentation: \n",
    "\n",
    "    * A virtual environment is a Python environment such that the Python interpreter, libraries, and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a \"system\" Python; i.e., the version of Python which is installed as part of your operating system.  \n",
    "\n",
    "Put another way, activating your project in a virtual environment allows it to become it's own self-contained application. A few advantages of doing this include: \n",
    "\n",
    "* Allows you to download packages into your project without administrator privileges/status. \n",
    "\n",
    "* Compartmentalizes your project materials for easy sharing and replication. \n",
    "\n",
    "* *Avoids inter-project conflicts regarding versions and dependencies for packages.*\n",
    "\n",
    "That last point can become especially relevant as you work on multiple projects in Python, as one critical version/dependency for one project can cause your other projects to stop working. And, the process of uninstalling packages and/or switching versions for projects is tedious and time-consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the benefits demonstrated, how exactly do we go about setting up a virtual environment? So glad you asked! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A.1: *Virtual environments in Anaconda*\n",
    "\n",
    "Anaconda is the preferred distribution for local Python installs, with many functionalities presented in a user-friendly interface and offering a suite of applications to aid in data science projects. You can download it [here](https://www.anaconda.com/products/distribution).\n",
    "\n",
    "\n",
    "#### A.1.1: *Setting up a virtual environment*\n",
    "* Once you've downloaded Anaconda, you can set up a virtual environment by: \n",
    "\n",
    "    1. Open Anaconda Navigator on your computer. \n",
    "\n",
    "    2. On the left-hand side of the Navigator window, find and click on the button that say 'Environments': \n",
    "    \n",
    "        <div>\n",
    "        <img src=\"img/ana_env.png\" width=\"500\"/>\n",
    "        <div>\n",
    "\n",
    "    3. In the 'Environments' page, go to the bottom-left of the page and click the button that says 'Create'.\n",
    "\n",
    "        * When you do so, you'll be prompted by a pop-up window to provide a name for your new virtual environment. The location for the virutal environment will be shown to you, and you can install a specific version of Python and/or R: \n",
    "        \n",
    "            <div>\n",
    "            <img src=\"img/test_env.png\"/>\n",
    "            <div>\n",
    "\n",
    "        * Click the 'Create' button in the pop-up window, and wait for the virtual environment to finish being created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.2: *Activating your virtual environment*\n",
    "Now you're ready to use your virtual environment! To work in this environment, anytime you open Anaconda: \n",
    "\n",
    "* Navigate to the 'Environments' page; \n",
    "\n",
    "* Find the environment you want to use; and \n",
    "\n",
    "* Click on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.3: *Adding packages to your virutal environment*\n",
    "\n",
    "To install packages in this virtual environment, either: \n",
    "\n",
    "* Stay on the 'Environments' page, and on the right-hand side of the page:\n",
    "     \n",
    "     * Change the field dictating displayed packages to 'Not installed' or 'All': \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/packages_ana_env.png\" width=\"500\"/>\n",
    "          <div>\n",
    "\n",
    "     * Go to the 'Search Packages' field and type in the name(s) of the package you want to install; \n",
    "\n",
    "     * If your package is available, click the open checkbox to the left of the package name: \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/install_numpy_base.png\" width=\"500\"/>\n",
    "          <div>\n",
    "\n",
    "     * Once you've selected all of the packages of interest, click the 'Apply' button in the bottom right-hand corner of the page to install them.  \n",
    "\n",
    "* Go to the 'Home' page, and (install and then) open the 'CMD.exe Prompt' program:\n",
    "\n",
    "     * In your command prompt window, you'll see that you're operating in your previously-selected virtual environment: \n",
    "\n",
    "          <div>\n",
    "          <img src=\"img/my_env_cmd_prompt.png\" width=\"500\">\n",
    "          <div>\n",
    "\n",
    "     * In this window, type ``` pip install [name of package] ``` for each package you want to install/weren't able to install in the 'Environments' page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: *More on text preprocessing*\n",
    "\n",
    "While the exact steps you elect to use for text preprocessing will ultimately depend on applications, there are some more generalizable techniques that you can usually look to apply: \n",
    "\n",
    "* **Expand contractions** - contractions like \"don't\", \"they're\", and \"it's\" all count as unique tokens if punctuation is simply removed (converting them to \"dont\", \"theyre\", \"its\", respectively). Decompose contractions into their constituent words to get more accurate counts of tokens like \"is,\" \"they,\" etc. [pycontractions](https://pypi.org/project/pycontractions/) can be useful here! \n",
    "\n",
    "    * Let's see an example:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required install: \n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycontractions import Contractions as ct\n",
    "\n",
    "# load contractions from a vector model - many models accepted!\n",
    "contractions = ct('GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# optional: load the model before the first .expand_texts call \n",
    "ct.load_models() \n",
    "\n",
    "example_sentence = \"I'd like to know how you're doing! You're her best friend, but I'm your friend too, aren't I?\"\n",
    "\n",
    "# let's see the text, de-contraction-afied!\n",
    "print(list(ct.expand_texts([example_sentence])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Remove stopwords** - stopwords are words like \"a,\" \"from,\" and \"the\" which are typically filtered out from text before analysis as they do not meaningfully contribute to the content of a document. Leaving in stopwords can lead to irrelevant topics in topic modeling, dilute the strength of sentiment in sentiment analysis, etc. \n",
    "\n",
    "    * Here's a quick loop that can help filter out the stopwords from a string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence = \"Hi! This is a needlessly wordy sentence with lots of stopwords. My favorite words are: a, the, with, which. You may think that is strange - and it is!\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Example stopwords include: \" + str(stopwords.words('english')[:5])) # if you want to see what are considered English stopwords by the NLTK package\n",
    "\n",
    "word_tokens = word_tokenize(example_sentence)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "# let's see the difference!\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that different packages have different lists which define stopwords, so make sure you pick a suitable one. Also, feel free to define your own custom stopwords lists!  \n",
    "\n",
    "* **Standardize phrases** - oftentimes text preprocessing is carried out as a precursor to a matching exercise (e.g. using company name to merge two databases). In such cases, we may want to standardize key phrases. For example, \"My Little Startup, LLC\" and \"my little startup\" clearly refer to the same entity, but will not match currently. \n",
    "\n",
    "    * In such cases, we may need to write a custom script to standardize key phrases, or there may be a packages out there that already do this for us. Let's take a look at one for our example, standardizing company names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required install: \n",
    "!pip install cleanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanco import basename\n",
    "\n",
    "business_name_one = \"My Little Startup, LLC\"\n",
    "cleaned_name_one  = basename(business_name_one) # feel free to print this out! just add: 'print(cleaned_name_one)' below. \n",
    "\n",
    "business_name_two = \"My Little Startup\"\n",
    "cleaned_name_two  = basename(business_name_two)\n",
    "\n",
    "# sanity check - are the cleaned company names identical?  \n",
    "print(cleaned_name_one == cleaned_name_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How and where you choose to standardize phrases in text will of course depend on your end goal, but there are plenty of resources/examples out there for you to model an approach after if a package doesn't already exist!\n",
    "\n",
    "* **Normalize text** - normalization refers to the process of transforming text into a canonical (standard) form. Sometimes, people take this to mean the entire text pre-processing pipeline, but here we're using it to refer to conversions like \"2mrrw\" to \"tomorrow\" and \"b4\" to \"before.\" \n",
    "\n",
    "    * This process is especially useful when using social media comments as your base text for analysis but often requires custom scripting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16c684165a00eba53f696e92e1de76bb4a10a33402bb31cdf5ab4f07210fc261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
