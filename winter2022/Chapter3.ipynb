{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f09d185",
   "metadata": {},
   "source": [
    "# Chapter 3 - Document encoding (TF-IDF), topic modeling, sentiment analysis, building text classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfa4cb",
   "metadata": {},
   "source": [
    "2022 February 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a9bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee8c99",
   "metadata": {},
   "source": [
    "## Document encoding for machine learning\n",
    "\n",
    "In the last chapter you saw that we do not change text to numbers, but instead changed the _representation_ of the text to the numbers in sparse matrix format. \n",
    "\n",
    "In this format, each row represents a document and each column represents a token from the shared text vocabulary called a **feature**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b898835",
   "metadata": {},
   "source": [
    "## Key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55026a14",
   "metadata": {},
   "source": [
    "* **Document term matrix:** contains the frequencies (or TF-IDF scores) of vocabulary terms in a collection of documents in sparse format. \n",
    "    * Each row is a document in the corpus.\n",
    "    * Each column represents a term (uni-gram, bi-gram, etc.) called a feature.\n",
    "\n",
    "* **Bag of words:** The simplest text analysis model that standardizes text in a document by removing punctuation, converting the words to lowercase, and counting the token frequencies.\n",
    "    * Numeric values indicate that a particular feature is found in a document that number of times.\n",
    "    * A 0 indicates that the feature is _not_ found in that document. \n",
    "\n",
    "![dtm](img/dtm.png)\n",
    "\n",
    "[modified from \"The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\"](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ch04.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bebc0e",
   "metadata": {},
   "source": [
    "* **TF-IDF:** Term frequency–inverse document frequency; a weighted numerical statistic that indicates the uniqueness of a word is in a given document or corpus.\n",
    "\n",
    "For TF-IDF sparse matrices:\n",
    "* A value closer to 1 indicate that a feature is more relevant to a particular document.\n",
    "* A value closer to 0 indicates that that feature is less/not relevant to that document.\n",
    "\n",
    "![tf1](img/tf1.png)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "![tf2](img/tf2.png)\n",
    "\n",
    "[towardsdatascience](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e59b1",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "![topic](img/topic.png)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Topic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf173b9",
   "metadata": {},
   "source": [
    "![unhrc](img/unhrc.jpg)\n",
    "\n",
    "### Corpus definition: United Nations Human Rights Council Documentation\n",
    "\n",
    "We will select eleven .txt files from the UN HRC as our corpus, stored within the subfolder \"human_rights\" folder inside the main \"data\" directory. \n",
    "\n",
    "These documents contain information about human rights recommendations made by member nations towards countries deemed to be in violation of the HRC. \n",
    "\n",
    "[Learn more about the UN HRC by clicking here.](https://www.ohchr.org/en/hrbodies/hrc/pages/home.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf3a3d",
   "metadata": {},
   "source": [
    "### Define the corpus directory\n",
    "\n",
    "Set the directory's file path and print the files it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fd3f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afghanistan2014.txt',\n",
       " 'bangladesh2013.txt',\n",
       " 'cotedivoire2014.txt',\n",
       " 'djibouti2013.txt',\n",
       " 'fiji2014.txt',\n",
       " 'jordan2013.txt',\n",
       " 'kazakhstan2014.txt',\n",
       " 'monaco2013.txt',\n",
       " 'sanmarino2014.txt',\n",
       " 'turkmenistan2013.txt',\n",
       " 'tuvalu2013.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "corpus = os.listdir('data/human_rights/')\n",
    "\n",
    "# View the contents of this directory\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ba33f",
   "metadata": {},
   "source": [
    "### Store these documents in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39d91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Store in an empty dictionary for conversion to data frame\n",
    "empty_dictionary = {}\n",
    "\n",
    "# Loop through the folder of documents to open and read each one\n",
    "for document in corpus:\n",
    "    with open('data/human_rights/' + document, 'r', encoding = 'utf-8') as to_open:\n",
    "         empty_dictionary[document] = to_open.read()\n",
    "\n",
    "# Populate the data frame with two columns: file name and document text\n",
    "human_rights = (pd.DataFrame.from_dict(empty_dictionary, \n",
    "                                       orient = 'index')\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {'index': 'file_name', 0: 'document_text'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c951f4",
   "metadata": {},
   "source": [
    "### View the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4818a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text\n",
       "0    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...\n",
       "1     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...\n",
       "2    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...\n",
       "3       djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...\n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...\n",
       "5         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...\n",
       "6     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...\n",
       "7         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...\n",
       "8      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...\n",
       "9   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...\n",
       "10        tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2740ed",
   "metadata": {},
   "source": [
    "### View the text of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088bfab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distr.: General 4 April 2014 \n",
      "Original: English \n",
      "General Assembly \n",
      "Human Rights Council Twenty-sixth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "\n",
      "Report of the Working Group on the Universal Periodic Review* \n",
      "Afghanistan \n",
      "* The annex to the present report is circulated as received. \n",
      "\n",
      "GE.14-12952 \n",
      "*1412952* \n",
      "Contents \n",
      "Paragraphs Page \n",
      "Introduction............................................................................................................. 1Ð4 3 \n",
      "I. Summary of the proceedings of the review process................................................ 5Ð135 3 \n",
      "A. Presentation by the State under review........................................................... 5Ð28 3 \n",
      "B. Interactive dialogue and responses by the State under review........................ 29Ð135 6 \n",
      "II. Conclusions and/or recommendations .................................................................... 136Ð139 13  Annex Composition of the delegation .......................................................\n"
     ]
    }
   ],
   "source": [
    "# first thousand characters\n",
    "print(human_rights['document_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90a9df",
   "metadata": {},
   "source": [
    "## English text preprocessing\n",
    "\n",
    "Create a new column named \"clean_text\" to store the text as it is preprocessed. \n",
    "\n",
    "* Remove non-alphanumeric characters/punctuation\n",
    "* Remove digits\n",
    "* Remove [unicode characters](https://en.wikipedia.org/wiki/List_of_Unicode_characters)\n",
    "* Remove extra spaces\n",
    "* Convert to lowercase\n",
    "* Lemmatize (optional for now)\n",
    "\n",
    "Take a look at the first document after each step to see if you can notice what changed. \n",
    "\n",
    "[How else could you improve this process?](/SSDS-TAML/winter2022/Appendix.ipynb#appendix-b-more-on-text-preprocessing) \n",
    "\n",
    "> NOTE: Remember, this is just a bare bones, basic process. Furthermore, it will not likely work for many other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274571c",
   "metadata": {},
   "source": [
    "### Remove non-alphanumeric characters/punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b07757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['document_text'].str.replace(r'[^\\w\\s]', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91c20326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distr   General 4 April 2014 \n",
      "Original  English \n",
      "General Assembly \n",
      "Human Rights Council Twenty sixth session \n",
      "Agenda item 6 \n",
      "Universal Periodic Review \n",
      "\n",
      "Report of the Working Group on the Universal Periodic Review  \n",
      "Afghanistan \n",
      "  The annex to the present report is circulated as received  \n",
      "\n",
      "GE 14 12952 \n",
      " 1412952  \n",
      "Contents \n",
      "Paragraphs Page \n",
      "Introduction                                                                                                              1Ð4 3 \n",
      "I  Summary of the proceedings of the review process                                                 5Ð135 3 \n",
      "A  Presentation by the State under review                                                            5Ð28 3 \n",
      "B  Interactive dialogue and responses by the State under review                         29Ð135 6 \n",
      "II  Conclusions and or recommendations                                                                      136Ð139 13  Annex Composition of the delegation                                                        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b669251a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>\\nDistr   General 4 April 2014 \\nOriginal  Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>\\nDistr   General 7 July 2014 English Original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>\\n\\nDistr   General 8 July 2013 English Origin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>\\nDistr   General 6 January 2014 \\nOriginal  E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>\\nDistr   General 3 January 2014 English Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>\\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>\\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "1     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "2    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "3       djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "6     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "7         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "8      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "9   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "10        tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   \\nDistr   General 4 April 2014 \\nOriginal  Eng...  \n",
       "1    \\n United Nations \\n A HRC 24 12  \\n \\n \\n\\n ...  \n",
       "2   \\nDistr   General 7 July 2014 English Original...  \n",
       "3   \\n\\nDistr   General 8 July 2013 English Origin...  \n",
       "4    \\n United Nations \\n A HRC 28 8 \\n \\n \\n\\n Ge...  \n",
       "5   \\nDistr   General 6 January 2014 \\nOriginal  E...  \n",
       "6    \\n United Nations \\n A HRC 28 10 \\n \\n \\n\\n G...  \n",
       "7   \\nDistr   General 3 January 2014 English Origi...  \n",
       "8    \\n United Nations \\n A HRC 28 9 \\n \\n \\n\\n Ge...  \n",
       "9    \\n United Nations \\n A HRC 24 3  \\n \\n \\n\\n G...  \n",
       "10   \\n United Nations \\n A HRC 24 8  \\n \\n \\n\\n G...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view third column\n",
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db0c33",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef79161",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\d', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b167de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distr   General   April      \n",
      "Original  English \n",
      "General Assembly \n",
      "Human Rights Council Twenty sixth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "\n",
      "Report of the Working Group on the Universal Periodic Review  \n",
      "Afghanistan \n",
      "  The annex to the present report is circulated as received  \n",
      "\n",
      "GE          \n",
      "          \n",
      "Contents \n",
      "Paragraphs Page \n",
      "Introduction                                                                                                               Ð    \n",
      "I  Summary of the proceedings of the review process                                                  Ð      \n",
      "A  Presentation by the State under review                                                             Ð     \n",
      "B  Interactive dialogue and responses by the State under review                           Ð      \n",
      "II  Conclusions and or recommendations                                                                         Ð        Annex Composition of the delegation                                                        \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39553ef8",
   "metadata": {},
   "source": [
    "### Remove unicode characters such as Ð and ð"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4adae8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more on text encodings: https://www.w3.org/International/questions/qa-what-is-encoding\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4475eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distr   General   April      \n",
      "Original  English \n",
      "General Assembly \n",
      "Human Rights Council Twenty sixth session \n",
      "Agenda item   \n",
      "Universal Periodic Review \n",
      "\n",
      "Report of the Working Group on the Universal Periodic Review  \n",
      "Afghanistan \n",
      "  The annex to the present report is circulated as received  \n",
      "\n",
      "GE          \n",
      "          \n",
      "Contents \n",
      "Paragraphs Page \n",
      "Introduction                                                                                                                   \n",
      "I  Summary of the proceedings of the review process                                                        \n",
      "A  Presentation by the State under review                                                                  \n",
      "B  Interactive dialogue and responses by the State under review                                 \n",
      "II  Conclusions and or recommendations                                                                                 Annex Composition of the delegation                                                             \n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d990d",
   "metadata": {},
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbbc4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "human_rights['clean_text'] = human_rights['clean_text'].str.replace(r'\\s+', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a251985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Distr General April Original English General Assembly Human Rights Council Twenty sixth session Agenda item Universal Periodic Review Report of the Working Group on the Universal Periodic Review Afghanistan The annex to the present report is circulated as received GE Contents Paragraphs Page Introduction I Summary of the proceedings of the review process A Presentation by the State under review B Interactive dialogue and responses by the State under review II Conclusions and or recommendations Annex Composition of the delegation Introduction The Working Group on the Universal Periodic Review established in accordance with Human Rights Council resolution of June held its eighteenth session from January to February The review of Afghanistan was held at the nd meeting on January The delegation of Afghanistan was headed by Mohammad Qasim Hashemzai Senior Advisor Ministry of Justice At its th meeting held on January the Working Group adopted the report on Afghanistan On January the Human R\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729ca0d",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dfb60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['clean_text'] = human_rights['clean_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c6d10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " distr general april original english general assembly human rights council twenty sixth session agenda item universal periodic review report of the working group on the universal periodic review afghanistan the annex to the present report is circulated as received ge contents paragraphs page introduction i summary of the proceedings of the review process a presentation by the state under review b interactive dialogue and responses by the state under review ii conclusions and or recommendations annex composition of the delegation introduction the working group on the universal periodic review established in accordance with human rights council resolution of june held its eighteenth session from january to february the review of afghanistan was held at the nd meeting on january the delegation of afghanistan was headed by mohammad qasim hashemzai senior advisor ministry of justice at its th meeting held on january the working group adopted the report on afghanistan on january the human r\n"
     ]
    }
   ],
   "source": [
    "print(human_rights['clean_text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4cbd1",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a6bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4aa5fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e0eaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# human_rights['clean_text'] = human_rights['clean_text'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f17da19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(human_rights['clean_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b472b2",
   "metadata": {},
   "source": [
    "### View the updated data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7e73854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan2014.txt</td>\n",
       "      <td>\\nDistr.: General 4 April 2014 \\nOriginal: Eng...</td>\n",
       "      <td>distr general april original english general ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangladesh2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cotedivoire2014.txt</td>\n",
       "      <td>\\nDistr.: General 7 July 2014 English Original...</td>\n",
       "      <td>distr general july english original english f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>djibouti2013.txt</td>\n",
       "      <td>\\n\\nDistr.: General 8 July 2013 English Origin...</td>\n",
       "      <td>distr general july english original english f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiji2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jordan2013.txt</td>\n",
       "      <td>\\nDistr.: General 6 January 2014 \\nOriginal: E...</td>\n",
       "      <td>distr general january original english genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kazakhstan2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monaco2013.txt</td>\n",
       "      <td>\\nDistr.: General 3 January 2014 English Origi...</td>\n",
       "      <td>distr general january english original englis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sanmarino2014.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turkmenistan2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tuvalu2013.txt</td>\n",
       "      <td>\\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...</td>\n",
       "      <td>united nations a hrc general assembly distr g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name                                      document_text  \\\n",
       "0    afghanistan2014.txt  \\nDistr.: General 4 April 2014 \\nOriginal: Eng...   \n",
       "1     bangladesh2013.txt   \\n United Nations \\n A/HRC/24/12  \\n \\n \\n\\n ...   \n",
       "2    cotedivoire2014.txt  \\nDistr.: General 7 July 2014 English Original...   \n",
       "3       djibouti2013.txt  \\n\\nDistr.: General 8 July 2013 English Origin...   \n",
       "4           fiji2014.txt   \\n United Nations \\n A/HRC/28/8 \\n \\n \\n\\n Ge...   \n",
       "5         jordan2013.txt  \\nDistr.: General 6 January 2014 \\nOriginal: E...   \n",
       "6     kazakhstan2014.txt   \\n United Nations \\n A/HRC/28/10 \\n \\n \\n\\n G...   \n",
       "7         monaco2013.txt  \\nDistr.: General 3 January 2014 English Origi...   \n",
       "8      sanmarino2014.txt   \\n United Nations \\n A/HRC/28/9 \\n \\n \\n\\n Ge...   \n",
       "9   turkmenistan2013.txt   \\n United Nations \\n A/HRC/24/3  \\n \\n \\n\\n G...   \n",
       "10        tuvalu2013.txt   \\n United Nations \\n A/HRC/24/8  \\n \\n \\n\\n G...   \n",
       "\n",
       "                                           clean_text  \n",
       "0    distr general april original english general ...  \n",
       "1    united nations a hrc general assembly distr g...  \n",
       "2    distr general july english original english f...  \n",
       "3    distr general july english original english f...  \n",
       "4    united nations a hrc general assembly distr g...  \n",
       "5    distr general january original english genera...  \n",
       "6    united nations a hrc general assembly distr g...  \n",
       "7    distr general january english original englis...  \n",
       "8    united nations a hrc general assembly distr g...  \n",
       "9    united nations a hrc general assembly distr g...  \n",
       "10   united nations a hrc general assembly distr g...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5c2f1",
   "metadata": {},
   "source": [
    "## Unsupervised learning with `TfidfVectorizer()`\n",
    "\n",
    "Remember `CountVectorizer()` for creating Bag of Word models? Bag of Words models are inputs for [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "\n",
    "However, let's extend this idea to `TfidfVectorizer()`. Each row will still be a colunm in our matrix and each column will still be a linguistic feature, but the cells will now be populated by the word uniqueness weights instead of frequencies. \n",
    "\n",
    "This will be the input for [Truncated Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) instead of LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efd9394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range = (1, 3), \n",
    "                                stop_words = 'english', \n",
    "                                max_df = 0.50\n",
    "                                )\n",
    "tf_sparse = tf_vectorizer.fit_transform(human_rights['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b59554c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 88194)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48eca133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 41201)\t0.004841792538927591\n",
      "  (0, 33118)\t0.004841792538927591\n",
      "  (0, 52297)\t0.004841792538927591\n",
      "  (0, 83563)\t0.004841792538927591\n",
      "  (0, 48098)\t0.004841792538927591\n",
      "  (0, 73938)\t0.004841792538927591\n",
      "  (0, 70099)\t0.004841792538927591\n",
      "  (0, 50457)\t0.004841792538927591\n",
      "  (0, 57730)\t0.004841792538927591\n",
      "  (0, 5933)\t0.004841792538927591\n",
      "  (0, 2131)\t0.004841792538927591\n",
      "  (0, 19496)\t0.004841792538927591\n",
      "  (0, 4456)\t0.004841792538927591\n",
      "  (0, 51455)\t0.004841792538927591\n",
      "  (0, 2831)\t0.004841792538927591\n",
      "  (0, 41350)\t0.004841792538927591\n",
      "  (0, 7670)\t0.004841792538927591\n",
      "  (0, 84240)\t0.004841792538927591\n",
      "  (0, 51495)\t0.004841792538927591\n",
      "  (0, 2835)\t0.004841792538927591\n",
      "  (0, 45547)\t0.004138587998252598\n",
      "  (0, 21498)\t0.004841792538927591\n",
      "  (0, 29085)\t0.004841792538927591\n",
      "  (0, 56075)\t0.004841792538927591\n",
      "  (0, 78289)\t0.004841792538927591\n",
      "  :\t:\n",
      "  (10, 56156)\t0.005334258825700086\n",
      "  (10, 64482)\t0.005334258825700086\n",
      "  (10, 12086)\t0.006065489955079169\n",
      "  (10, 54457)\t0.006065489955079169\n",
      "  (10, 24386)\t0.004303646433350054\n",
      "  (10, 40670)\t0.008607292866700108\n",
      "  (10, 50511)\t0.006065489955079169\n",
      "  (10, 67275)\t0.008607292866700108\n",
      "  (10, 30214)\t0.00476707189726019\n",
      "  (10, 65265)\t0.005334258825700086\n",
      "  (10, 21793)\t0.00476707189726019\n",
      "  (10, 67704)\t0.004303646433350054\n",
      "  (10, 31604)\t0.005334258825700086\n",
      "  (10, 83182)\t0.006065489955079169\n",
      "  (10, 80749)\t0.006065489955079169\n",
      "  (10, 79281)\t0.014301215691780571\n",
      "  (10, 28564)\t0.004303646433350054\n",
      "  (10, 59645)\t0.025821878600100327\n",
      "  (10, 63422)\t0.03012552503345038\n",
      "  (10, 73852)\t0.025821878600100327\n",
      "  (10, 76145)\t0.005334258825700086\n",
      "  (10, 50217)\t0.008607292866700108\n",
      "  (10, 30122)\t0.010668517651400172\n",
      "  (10, 8598)\t0.010668517651400172\n",
      "  (10, 19266)\t0.008607292866700108\n"
     ]
    }
   ],
   "source": [
    "print(tf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308f666",
   "metadata": {},
   "source": [
    "### Convert the tfidf sparse matrix to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b9cecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone social</th>\n",
       "      <th>zone social benefits</th>\n",
       "      <th>zones</th>\n",
       "      <th>zones inclusive</th>\n",
       "      <th>zones inclusive education</th>\n",
       "      <th>zones senegal</th>\n",
       "      <th>zones senegal recommendations</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.006234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 88194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007519    0.007519            0.007519  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007096     0.007096   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004775   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.010880   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004741   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005051   \n",
       "10           0.007096  0.007096        0.007096           0.007096   0.000000   \n",
       "\n",
       "    ...  zone social  zone social benefits     zones  zones inclusive  \\\n",
       "0   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "1   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "2   ...     0.007108              0.007108  0.006076         0.007108   \n",
       "3   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "4   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "5   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "6   ...     0.000000              0.000000  0.006219         0.000000   \n",
       "7   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "8   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "9   ...     0.000000              0.000000  0.000000         0.000000   \n",
       "10  ...     0.000000              0.000000  0.000000         0.000000   \n",
       "\n",
       "    zones inclusive education  zones senegal  zones senegal recommendations  \\\n",
       "0                    0.000000       0.000000                       0.000000   \n",
       "1                    0.000000       0.000000                       0.000000   \n",
       "2                    0.007108       0.000000                       0.000000   \n",
       "3                    0.000000       0.000000                       0.000000   \n",
       "4                    0.000000       0.000000                       0.000000   \n",
       "5                    0.000000       0.000000                       0.000000   \n",
       "6                    0.000000       0.007276                       0.007276   \n",
       "7                    0.000000       0.000000                       0.000000   \n",
       "8                    0.000000       0.000000                       0.000000   \n",
       "9                    0.000000       0.000000                       0.000000   \n",
       "10                   0.000000       0.000000                       0.000000   \n",
       "\n",
       "       zouon  zouon bi  zouon bi tidou  \n",
       "0   0.000000  0.000000        0.000000  \n",
       "1   0.000000  0.000000        0.000000  \n",
       "2   0.000000  0.000000        0.000000  \n",
       "3   0.006234  0.006234        0.006234  \n",
       "4   0.000000  0.000000        0.000000  \n",
       "5   0.000000  0.000000        0.000000  \n",
       "6   0.000000  0.000000        0.000000  \n",
       "7   0.000000  0.000000        0.000000  \n",
       "8   0.000000  0.000000        0.000000  \n",
       "9   0.000000  0.000000        0.000000  \n",
       "10  0.000000  0.000000        0.000000  \n",
       "\n",
       "[11 rows x 88194 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_sparse.todense(), columns = tf_vectorizer.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7b254",
   "metadata": {},
   "source": [
    "### View 20 highest weighted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ac27766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "monaco                        0.718573\n",
       "tuvalu                        0.638645\n",
       "fiji                          0.566999\n",
       "turkmenistan                  0.566983\n",
       "san                           0.551964\n",
       "jordan                        0.482716\n",
       "marino                        0.455128\n",
       "san marino                    0.455128\n",
       "kazakhstan                    0.448850\n",
       "divoire                       0.330871\n",
       "te divoire                    0.330871\n",
       "te                            0.306268\n",
       "kazakhstans                   0.284333\n",
       "bangladeshs                   0.251033\n",
       "elimination violence women    0.250640\n",
       "elimination violence          0.250640\n",
       "djiboutis                     0.248364\n",
       "reconciliation                0.245134\n",
       "fgm                           0.194096\n",
       "afghan                        0.180447\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.max().sort_values(ascending = False).head(n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c7716",
   "metadata": {},
   "source": [
    "### Add country name to `tfidf_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b91f3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sanmarino',\n",
       " 'tuvalu',\n",
       " 'kazakhstan',\n",
       " 'cotedivoire',\n",
       " 'fiji',\n",
       " 'bangladesh',\n",
       " 'turkmenistan',\n",
       " 'jordan',\n",
       " 'monaco',\n",
       " 'afghanistan',\n",
       " 'djibouti']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrangle the country names from the human_rights data frame\n",
    "countries = human_rights['file_name'].str.slice(stop = -8)\n",
    "countries = list(countries)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffed2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['COUNTRY'] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd783358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abasi</th>\n",
       "      <th>abasi desk</th>\n",
       "      <th>abasi desk officer</th>\n",
       "      <th>abdi</th>\n",
       "      <th>abdi ismael</th>\n",
       "      <th>abdi ismael hersi</th>\n",
       "      <th>abdou</th>\n",
       "      <th>abdou prsident</th>\n",
       "      <th>abdou prsident la</th>\n",
       "      <th>abduction</th>\n",
       "      <th>...</th>\n",
       "      <th>zone social benefits</th>\n",
       "      <th>zones</th>\n",
       "      <th>zones inclusive</th>\n",
       "      <th>zones inclusive education</th>\n",
       "      <th>zones senegal</th>\n",
       "      <th>zones senegal recommendations</th>\n",
       "      <th>zouon</th>\n",
       "      <th>zouon bi</th>\n",
       "      <th>zouon bi tidou</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sanmarino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tuvalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kazakhstan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>cotedivoire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fiji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>turkmenistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>djibouti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 88195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abasi  abasi desk  abasi desk officer      abdi  abdi ismael  \\\n",
       "0   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "1   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "2   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "3   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "4   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "5   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "6   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "7   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "8   0.000000    0.000000            0.000000  0.000000     0.000000   \n",
       "9   0.007519    0.007519            0.007519  0.000000     0.000000   \n",
       "10  0.000000    0.000000            0.000000  0.007096     0.007096   \n",
       "\n",
       "    abdi ismael hersi     abdou  abdou prsident  abdou prsident la  abduction  \\\n",
       "0            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "1            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "2            0.000000  0.000000        0.000000           0.000000   0.004775   \n",
       "3            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "4            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "5            0.000000  0.000000        0.000000           0.000000   0.010880   \n",
       "6            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "7            0.000000  0.000000        0.000000           0.000000   0.004741   \n",
       "8            0.000000  0.000000        0.000000           0.000000   0.000000   \n",
       "9            0.000000  0.000000        0.000000           0.000000   0.005051   \n",
       "10           0.007096  0.007096        0.007096           0.007096   0.000000   \n",
       "\n",
       "    ...  zone social benefits     zones  zones inclusive  \\\n",
       "0   ...              0.000000  0.000000         0.000000   \n",
       "1   ...              0.000000  0.000000         0.000000   \n",
       "2   ...              0.007108  0.006076         0.007108   \n",
       "3   ...              0.000000  0.000000         0.000000   \n",
       "4   ...              0.000000  0.000000         0.000000   \n",
       "5   ...              0.000000  0.000000         0.000000   \n",
       "6   ...              0.000000  0.006219         0.000000   \n",
       "7   ...              0.000000  0.000000         0.000000   \n",
       "8   ...              0.000000  0.000000         0.000000   \n",
       "9   ...              0.000000  0.000000         0.000000   \n",
       "10  ...              0.000000  0.000000         0.000000   \n",
       "\n",
       "    zones inclusive education  zones senegal  zones senegal recommendations  \\\n",
       "0                    0.000000       0.000000                       0.000000   \n",
       "1                    0.000000       0.000000                       0.000000   \n",
       "2                    0.007108       0.000000                       0.000000   \n",
       "3                    0.000000       0.000000                       0.000000   \n",
       "4                    0.000000       0.000000                       0.000000   \n",
       "5                    0.000000       0.000000                       0.000000   \n",
       "6                    0.000000       0.007276                       0.007276   \n",
       "7                    0.000000       0.000000                       0.000000   \n",
       "8                    0.000000       0.000000                       0.000000   \n",
       "9                    0.000000       0.000000                       0.000000   \n",
       "10                   0.000000       0.000000                       0.000000   \n",
       "\n",
       "       zouon  zouon bi  zouon bi tidou       COUNTRY  \n",
       "0   0.000000  0.000000        0.000000     sanmarino  \n",
       "1   0.000000  0.000000        0.000000        tuvalu  \n",
       "2   0.000000  0.000000        0.000000    kazakhstan  \n",
       "3   0.006234  0.006234        0.006234   cotedivoire  \n",
       "4   0.000000  0.000000        0.000000          fiji  \n",
       "5   0.000000  0.000000        0.000000    bangladesh  \n",
       "6   0.000000  0.000000        0.000000  turkmenistan  \n",
       "7   0.000000  0.000000        0.000000        jordan  \n",
       "8   0.000000  0.000000        0.000000        monaco  \n",
       "9   0.000000  0.000000        0.000000   afghanistan  \n",
       "10  0.000000  0.000000        0.000000      djibouti  \n",
       "\n",
       "[11 rows x 88195 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb258c7",
   "metadata": {},
   "source": [
    "### Examine unique words by each document/country\n",
    "\n",
    "Change the country names to view their highest rated terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a7fe5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jordan                                 0.482716\n",
       "jordanian                              0.127019\n",
       "press publications                     0.112906\n",
       "publications                           0.108571\n",
       "syrian                                 0.098792\n",
       "constitutional amendments              0.096508\n",
       "publications law                       0.091736\n",
       "press publications law                 0.091736\n",
       "syrian refugees                        0.084679\n",
       "websites                               0.072381\n",
       "reservations                           0.071108\n",
       "commitment jordan                      0.063509\n",
       "reservations convention                0.060317\n",
       "news websites                          0.056453\n",
       "al                                     0.054286\n",
       "personal status                        0.054286\n",
       "news                                   0.054286\n",
       "personal                               0.053046\n",
       "reservations convention elimination    0.048254\n",
       "hosting                                0.042437\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = tfidf_df[tfidf_df['COUNTRY'] == 'jordan']\n",
    "country.max(numeric_only = True).sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b758f",
   "metadata": {},
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "![tsvd](img/tsvd.png)\n",
    "\n",
    "[Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/part-16-step-by-step-guide-to-master-nlp-topic-modelling-using-lsa/)\n",
    "\n",
    "* Look ahead to Chapter 4.5 for new techniques in topic modeling - [BERTopic!](Chapter4_add.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7d2e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=5, random_state=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 5, \n",
    "                   random_state = 1, \n",
    "                   algorithm = 'arpack')\n",
    "tsvd.fit(tf_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7244ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00816051 0.10375827 0.10320114 0.09968319 0.10115778]\n"
     ]
    }
   ],
   "source": [
    "print(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e10a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13997839 1.01408739 1.00306128 0.99825596 0.99320396]\n"
     ]
    }
   ],
   "source": [
    "print(tsvd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be1bb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15a4e77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "turkmenistan kazakhstan jordan fiji tuvalu divoire te divoire monaco te reconciliation bangladeshs san elimination violence elimination violence women kazakhstans fgm djiboutis marino san marino violence women law\n",
      "\n",
      "Topic #1:\n",
      "monaco san marino san marino te divoire divoire te reconciliation fgm djiboutis op crc fiji san marinos marinos monegasque national reconciliation elimination forms racial forms racial discrimination forms racial international convention elimination\n",
      "\n",
      "Topic #2:\n",
      "tuvalu fiji djiboutis national strategic climate change strategic water sanitation sanitation fgm safe drinking safe drinking water rapporteur human right rapporteur human special rapporteur human human right safe climate national gender policy national gender fijian drinking water sanitation\n",
      "\n",
      "Topic #3:\n",
      "te divoire divoire te reconciliation fgm national reconciliation jordan djiboutis ivorian national development plan elimination violence elimination violence women truth genital mutilation genital female genital female genital mutilation dialogue truth reconciliation truth reconciliation dialogue truth\n",
      "\n",
      "Topic #4:\n",
      "monaco jordan tuvalu jordanian press publications publications monegasque syrian constitutional amendments bangladeshs publications law press publications law principality nationality commended monaco syrian refugees reservations des sanitation websites\n"
     ]
    }
   ],
   "source": [
    "tf_features = tf_vectorizer.get_feature_names()\n",
    "topics(tsvd, tf_features, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45155207",
   "metadata": {},
   "source": [
    "## UN HRC text analysis - what next? \n",
    "\n",
    "Keep in mind that we have not even begun to consider named entities and parts of speech. How might country names be swamping the five topics produced? \n",
    "\n",
    "[Read this stack overflow post to learn about the possibility of having too few documents in your corpus](https://stats.stackexchange.com/questions/302965/some-topics-with-all-equal-weights-when-using-latentdirichletallocation-from-sci)\n",
    "\n",
    "[Also, read this post about how to grid search for the best topic models](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/)\n",
    "\n",
    "Use BERTopic (see Chapter 4.5)\n",
    "\n",
    "TODO: visualize, named entity recognition, part of speech tagging, hyphenated words, contractions, context, importance of stopwords, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1c801",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis is the contextual mining of text data that elicits abstract information in source materials to determine if data are positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01ab7e",
   "metadata": {},
   "source": [
    "![sa](img/sa.jpg)\n",
    "\n",
    "[Repustate](https://www.repustate.com/blog/sentiment-analysis-challenges-with-solutions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54ad0b",
   "metadata": {},
   "source": [
    "## Download the nltk built-in movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1254416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/evanmuzzall/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e177b6",
   "metadata": {},
   "source": [
    "### Define x (reviews) and y (judgements) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6690fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "636aade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Judgements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Judgements\n",
       "0  plot : two teen couples go to a church party ,...        neg\n",
       "1  the happy bastard's quick movie review \\ndamn ...        neg\n",
       "2  it is movies like these that make a jaded movi...        neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...        neg\n",
       "4  synopsis : a mentally unstable man undergoing ...        neg"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bab88e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcf327",
   "metadata": {},
   "source": [
    "### Shuffle the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3655ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8f384c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human review was: neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('steve martin is one of the funniest men alive . \\nif you can take that as a true statement , then your disappointment at this film will equal mine . \\nmartin can be hilarious , creating some of the best laugh-out-loud experiences that have ever taken place in movie theaters . \\nyou won\\'t find any of them here . \\nthe old television series that this is based on has its moments of humor and wit . \\nbilko ( and the name isn\\'t an accident ) is the head of an army motor pool group , but his passion is his schemes . \\nevery episode involves the sergeant and his men in one or another hair-brained plan to get rich quick while outwitting the officers of the base . \\n \" mchale\\'s navy \" \\'s granddaddy . \\nthat\\'s the idea behind this movie too , but the difference is that , as far-fetched and usually goofy as the television series was , it was funny . \\nthere is not one laugh in the film . \\nthe re-make retains the goofiness , but not the entertainment . \\neverything is just too clean . \\nit was obviously made on a hollywood back lot and looks every bit like it . \\nit all looks brand new , even the old beat-up stuff . \\nmartin is remarkably small in what should have been a bigger than life role . \\nin the original , phil silvers played the huckster with a heart of gold and more than a touch of sleaziness . \\nmartin\\'s bilko is a pale imitation . \\nthe only semi-bright spot is phil hartman as bilko\\'s arch-enemy . \\nit\\'s not saying much , considering martin\\'s lackluster character , but hartman leaves him in the dust . \\n',\n",
       " None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change x[0] and y[0] to see different reviews\n",
    "x[0], print(\"Human review was:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed7844",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "scikit-learn offers hand ways to build machine learning pipelines: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5219a",
   "metadata": {},
   "source": [
    "### One standard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e40d414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard training/test split (no cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# get tfidf values\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(x)\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "\n",
    "# instantiate, train, and test an logistic regression model\n",
    "logit_class = LogisticRegression(solver = 'liblinear',\n",
    "                                 penalty = 'l2', \n",
    "                                 C = 1000, \n",
    "                                 random_state = 1)\n",
    "model = logit_class.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db28c7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216666666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set accuracy\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639817da",
   "metadata": {},
   "source": [
    "### $k$-fold cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44b16805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8155922  0.79910045 0.80630631] 0.8069996533264899\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated model!\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(solver = 'liblinear',\n",
    "                                               penalty = 'l2', \n",
    "                                               C = 1000, \n",
    "                                               random_state = 1))\n",
    "                     ])\n",
    "\n",
    "# for your own research, thesis, or publication\n",
    "# you would select cv equal to 10 or 20\n",
    "scores = cross_val_score(text_clf, x, y, cv = 3)\n",
    "\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f57d9",
   "metadata": {},
   "source": [
    "### Top 25 features for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd66b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for positive reviews:\n",
      "['gas', 'perfectly', 'family', 'political', 'will', 'seen', 'rocky', 'always', 'different', 'excellent', 'also', 'many', 'is', 'matrix', 'trek', 'well', 'definitely', 'truman', 'very', 'great', 'quite', 'fun', 'jackie', 'as', 'and']\n",
      "\n",
      "Top features for negative reviews:\n",
      "['bad', 'only', 'plot', 'worst', 'there', 'boring', 'script', 'why', 'have', 'unfortunately', 'dull', 'poor', 'any', 'waste', 'nothing', 'looks', 'ridiculous', 'supposed', 'no', 'even', 'harry', 'awful', 'then', 'reason', 'wasted']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bc79849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bad_review = \"This was the most awful worst super bad movie ever!\"\n",
    "\n",
    "features = tfidf.transform([new_bad_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4dd2d596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_good_review = 'WHAT A WONDERFUL, FANTASTIC MOVIE!!!'\n",
    "\n",
    "features = tfidf.transform([new_good_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfebbf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type another review here\n",
    "my_review = 'I hated this movie, even though my friend loved it'\n",
    "my_features = tfidf.transform([my_review])\n",
    "model.predict(my_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643623ff",
   "metadata": {},
   "source": [
    "## Going further: Anchored topic modeling\n",
    "\n",
    "Check out Chapter 4.5 in this book for BERTopic modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d404d",
   "metadata": {},
   "source": [
    "## Quiz - 20 newsgroups dataset\n",
    "\n",
    "Go through the 20 newsgroups text dataset to get familiar with newspaper data: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "\"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9176674",
   "metadata": {},
   "source": [
    "## Appendix: *More on text preprocessing*\n",
    "\n",
    "While the exact steps you elect to use for text preprocessing will ultimately depend on applications, there are some more generalizable techniques that you can usually look to apply: \n",
    "\n",
    "* **Expand contractions** - contractions like \"don't\", \"they're\", and \"it's\" all count as unique tokens if punctuation is simply removed (converting them to \"dont\", \"theyre\", \"its\", respectively). Decompose contractions into their constituent words to get more accurate counts of tokens like \"is,\" \"they,\" etc. [pycontractions](https://pypi.org/project/pycontractions/) can be useful here! \n",
    "\n",
    "    * Let's see an example:   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58da4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b9fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pycontractions in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: gensim>=2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pycontractions) (4.1.2)\n",
      "Requirement already satisfied: language-check>=1.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/language_check-1.1-py3.8.egg (from pycontractions) (1.1)\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pycontractions) (0.5.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from gensim>=2.0->pycontractions) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from gensim>=2.0->pycontractions) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from gensim>=2.0->pycontractions) (1.21.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# required install: \n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1932163b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_models() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m contractions \u001b[38;5;241m=\u001b[39m ct(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# optional: load the model before the first .expand_texts call \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m example_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md like to know how you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre doing! You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre her best friend, but I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm your friend too, aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt I?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# let's see the text, de-contraction-afied!\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_models() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions as ct\n",
    "\n",
    "# load contractions from a vector model - many models accepted!\n",
    "contractions = ct('GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# optional: load the model before the first .expand_texts call \n",
    "ct.load_models() \n",
    "\n",
    "example_sentence = \"I'd like to know how you're doing! You're her best friend, but I'm your friend too, aren't I?\"\n",
    "\n",
    "# let's see the text, de-contraction-afied!\n",
    "print(list(ct.expand_texts([example_sentence])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e5f1f",
   "metadata": {},
   "source": [
    "* **Remove stopwords** - stopwords are words like \"a,\" \"from,\" and \"the\" which are typically filtered out from text before analysis as they do not meaningfully contribute to the content of a document. Leaving in stopwords can lead to irrelevant topics in topic modeling, dilute the strength of sentiment in sentiment analysis, etc. \n",
    "\n",
    "    * Here's a quick loop that can help filter out the stopwords from a string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71869c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example stopwords include: ['i', 'me', 'my', 'myself', 'we']\n",
      "['Hi', '!', 'This', 'is', 'a', 'needlessly', 'wordy', 'sentence', 'with', 'lots', 'of', 'stopwords', '.', 'My', 'favorite', 'words', 'are', ':', 'a', ',', 'the', ',', 'with', ',', 'which', '.', 'You', 'may', 'think', 'that', 'is', 'strange', '-', 'and', 'it', 'is', '!']\n",
      "['Hi', '!', 'needlessly', 'wordy', 'sentence', 'lots', 'stopwords', '.', 'favorite', 'words', ':', ',', ',', ',', '.', 'may', 'think', 'strange', '-', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence = \"Hi! This is a needlessly wordy sentence with lots of stopwords. My favorite words are: a, the, with, which. You may think that is strange - and it is!\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Example stopwords include: \" + str(stopwords.words('english')[:5])) # if you want to see what are considered English stopwords by the NLTK package\n",
    "\n",
    "word_tokens = word_tokenize(example_sentence)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "# let's see the difference!\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa8891",
   "metadata": {},
   "source": [
    "* Note that different packages have different lists which define stopwords, so make sure you pick a suitable one. Also, feel free to define your own custom stopwords lists!  \n",
    "\n",
    "* **Standardize phrases** - oftentimes text preprocessing is carried out as a precursor to a matching exercise (e.g. using company name to merge two databases). In such cases, we may want to standardize key phrases. For example, \"My Little Startup, LLC\" and \"my little startup\" clearly refer to the same entity, but will not match currently. \n",
    "\n",
    "    * In such cases, we may need to write a custom script to standardize key phrases, or there may be a packages out there that already do this for us. Let's take a look at one for our example, standardizing company names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8741bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "Collecting cleanco\n",
      "  Downloading cleanco-2.2-py3-none-any.whl (11 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: cleanco\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "Successfully installed cleanco-2.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# required install: \n",
    "!pip install cleanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff02725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from cleanco import basename\n",
    "\n",
    "business_name_one = \"My Little Startup, LLC\"\n",
    "cleaned_name_one  = basename(business_name_one) # feel free to print this out! just add: 'print(cleaned_name_one)' below. \n",
    "\n",
    "business_name_two = \"My Little Startup\"\n",
    "cleaned_name_two  = basename(business_name_two)\n",
    "\n",
    "# sanity check - are the cleaned company names identical?  \n",
    "print(cleaned_name_one == cleaned_name_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da737b4",
   "metadata": {},
   "source": [
    "* How and where you choose to standardize phrases in text will of course depend on your end goal, but there are plenty of resources/examples out there for you to model an approach after if a package doesn't already exist!\n",
    "\n",
    "* **Normalize text** - normalization refers to the process of transforming text into a canonical (standard) form. Sometimes, people take this to mean the entire text pre-processing pipeline, but here we're using it to refer to conversions like \"2mrrw\" to \"tomorrow\" and \"b4\" to \"before.\" \n",
    "\n",
    "    * This process is especially useful when using social media comments as your base text for analysis but often requires custom scripting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb80eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16c684165a00eba53f696e92e1de76bb4a10a33402bb31cdf5ab4f07210fc261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
